use CHIUAN;
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Computer method and apparatus providing interactive control and remote identity through in-world proxy','US08629866','International Business Machines Corporation','Betzler;Moody;Katz;Hamilton, II;O'Connell','Computer method and apparatus render views of a computer generated virtual world to a remote viewer. The invention method and apparatus establish an avatar of a resident user as a proxy on behalf of the remote viewer. The computer node of the resident user exports proxy avatar views of the virtual world to the remote viewer. The remote viewer computer renders the exported virtual world views without executing virtual world application programs.','1','1. A computer method of operating an individual avatar in a computer virtual environment, the method comprising:communicatively coupling a first computer node to a server supporting a virtual environment;sending a video stream of the virtual environment from the first computer node to a second computer node, the first computer node running a full client application of the virtual environment and the second computer node running no application program of the virtual environment and thus being a lightweight client, wherein said full client application renders the virtual environment to a user of the first computer node and handles functions to control an individual avatar in said virtual environment, the individual avatar being controlled by the user of the first computer node, and wherein said lightweight client, by receiving the video stream from the first computer node with the user of the first computer node controlling the individual avatar, (a) relieves the second computer node from being required to execute any application program of the virtual environment, and (b) enables a user of the second computer node to be a remote viewer of the virtual environment, and the first computer node running the full client application includes establishing one state of the individual avatar as corresponding to the remote viewer, wherein the user of the first computer node and the remote viewer of the second computer node are different from each other, the user of the first computer node, through the first computer node operating the individual avatar (i) in the one state on behalf of the remote viewer and (ii) in a different state as representative of himself, and the user of the first computer node operates the individual avatar in the one state to export views of the virtual environment in the video stream and to render virtual environment views through the video stream to the remote viewer on the second computer node;using the first computer node, receiving a request from the remote viewer through the second computer node to take over control of the individual avatar; andreceiving at the first computer node communications from the second computer node containing functions for control of the individual avatar in the one state, such that a same avatar is used by both the user of the first computer node and the user of the second computer node.2. A computer method as claimed in3. A computer method as claimed inthe step of establishing the one state of the individual avatar includes enabling the remote viewer to be a non-resident, guest participant in the virtual environment.4. A computer method as claimed inchanging identity of the individual avatar to represent the remote viewer.5. A computer method as claimed inenabling the remote viewer to interactively control the individual avatar in the virtual environment.6. A computer method as claimed in7. A computer method as claimed in8. A computer method as claimed in9. A computer method as claimed in10. Computer apparatus rendering virtual environment views comprising:a first computer node coupled to a server supporting a virtual environment and the first computer node running a full client application of the virtual environment, wherein the full client application renders the virtual environment to a user of the first computer node and handles functions to control an individual avatar in said virtual environment, the individual avatar being controlled by the user of the first computer node,a proxy system executed by the first computer node and enabling sending a video stream of the virtual environment from the first computer node to a second computer node, the second computer node running no client application program of the virtual environment and thus being a lightweight client, and wherein said lightweight client, by receiving the video stream from the first computer node with the user of the first computer node controlling the individual avatar, (a) relieves the second computer node from being required to execute any application program of the virtual environment, and (b) enables a user of the second computer node to be a remote viewer of the virtual environment, and the first computer node running the full client application includes enabling configuration of one state of the individual avatar as corresponding to the remote viewer, wherein the user of the first computer node and the remote viewer are different from each other, the user of the first computer node, through the first computer node operating the individual avatar (i) in the one state on behalf of the remote viewer and (ii) in a different state as representative of himself;a proxy view exporting subsystem coupled to the proxy system, the proxy view exporting subsystem exporting views of the virtual environment to the remote viewer from the first computer node providing the individual avatar in the one state, wherein the user of the first computer node operates the individual avatar in the one state to export views of the virtual environment in the video stream and to render virtual environment views through the video stream to the remote viewer on the second computer node,wherein the first computer node receives a request from the remote viewer through the second computer node to take over control of the individual avatar; andthe first computer node receives communications from the second computer node containing functions for control of the individual avatar in the one state, such that a same avatar is used by both the user of the first computer node and the user of the second computer node.11. Computer apparatus as claimed in12. Computer apparatus as claimed inthe user of the first computer node is a resident of the virtual environment; andthe proxy system enables the remote viewer to be a non-resident, guest participant in the virtual environment.13. Computer apparatus as claimed in14. Computer apparatus as claimed in15. Computer apparatus as claimed in16. Computer apparatus as claimed in17. Computer apparatus as claimed in18. Computer apparatus as claimed in19. Computer apparatus as claimed in20. Computer program product rendering views of a computer virtual world, the computer program product comprising:a non-transitory computer useable medium having computer useable program code embodied therewith, the computer useable program code comprising:computer useable program code configured to send a video stream of a virtual world from a first computer node to a second computer node, the virtual world being hosted by a server, and the first computer node coupled for communication to the server and running a full client application of the virtual world, and the second computer node running no client application of the virtual world such that the second computer node is a lightweight client, wherein said full client application renders the virtual world to a user of the first computer node and handles functions to control an individual avatar of the user in the virtual world, the individual avatar being controlled by the user of the first computer node, and wherein said lightweight client by receiving the video stream from the first computer node with the user of the first computer node controlling the individual avatar (a) relieves the second computer node from being required to execute any application program of the virtual world, and (b) enables a user of the second computer node to be a remote viewer of the virtual world, and the first computer node running the full client application includes forming one state of the individual avatar corresponding to the remote viewer, wherein the user of the first computer node and the remote viewer are different from each other, the user of the first computer node, through the first computer node operating the individual avatar (i) in the one state on behalf of the remote viewer and (ii) in a different state as representative of himself, and the user of the first computer node operates the individual avatar in the one state to export views of the virtual world in the video stream and to render virtual world views through the video stream to the remote viewer on the second computer node; andcomputer useable program code configured to receive at the first computer node: (i) a request from the remote viewer through the second computer node to take over control of the individual avatar, and (ii) communications from the second computer node containing functions for control of the individual avatar in the one state, such that a same avatar is used by both the user of the first computer node and the user of the second computer node.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Performing vector multiplication','US08629867','International Business Machines Corporation','Hickey;Muff;Tubbs;Wait','A method includes receiving packed data corresponding to pixel components to be processed at a graphics pipeline. The method includes unpacking the packed data to generate floating point numbers that correspond to the pixel components. The method also includes routing each of the floating point numbers to a separate lane of the graphics pipeline. Each of the floating point numbers are to be processed by multiplier units of the graphics pipeline.','1','1. A method, comprising:receiving packed data corresponding to pixel components to be processed at a graphics pipeline;unpacking the packed data to generate a first floating point number that corresponds to a first pixel component;routing the first floating point number to a corresponding first lane of the graphics pipeline, the first floating point number to be processed by a multiplier unit of the first lane;routing a second floating point number in parallel to the multiplier unit of the first lane and to an aligner unit of the first lane, the aligner unit of the first lane to align the second floating point number for a subtraction operation;multiplying the first floating point number with the second floating point number at the multiplier unit of the first lane to obtain a first multiplication result; androuting the first multiplication result and the aligned second floating point number to an adder unit of the first lane.2. The method of3. The method of4. The method ofunpacking the packed data to generate a third floating point number that corresponds to a second pixel component;routing the third floating point number to a corresponding second lane of the graphics pipeline, the third floating point number to be processed by a multiplier unit of the second lane;routing a fourth floating point number in parallel to the multiplier unit of the second lane and to an aligner unit of the second lane, the aligner unit to align the fourth floating point number for a second subtraction operation;multiplying the third floating point number with the fourth floating point number to obtain a second multiplication result; androuting the second multiplication result and the aligned second floating point number to an adder unit of the second lane, wherein the adder unit of the second lane is operable to subtract the aligned fourth floating point number from the second result to produce a second scaled floating point number.5. The method of6. The method of7. The method of8. The method of9. A method, comprising:receiving packed data corresponding to pixel components to be processed at a graphics pipeline;receiving a mapping instruction;in response to receiving the mapping instruction, unpacking the packed data to generate a first floating point number that corresponds to a pixel component;routing the first floating point number to a corresponding first lane of the graphics pipeline via a set of mux units, the first floating point number to be processed by a multiplier unit of the first lane;routing a second floating point number to the first lane of the graphics pipeline via the set of mux units, the second floating point number to be processed by an aligner unit of the first lane and the multiplier unit of the first lane, wherein the aligner unit of the first lane aligns the second floating point number to produce an aligned second floating point number; andmultiplying the first floating point number with the second floating point number at the multiplier unit of the first lane to obtain a first multiplication result;incrementing an exponent of the first result at the multiplier unit of the first lane;routing the first multiplication result to an adder unit of the first lane; androuting the aligned second floating point number to the adder unit of the first lane.10. The method ofsubtracting, at the adder unit of the first lane, the aligned second floating point number from the first multiplication result to produce a first scaled floating point number.11. The method of12. The method of13. A system, comprising:an unpack unit to receive packed data corresponding to pixel components and to unpack the packed data to generate a first floating point number, a second floating point number, and a third floating point number that each correspond to the pixel components, the pixel components including a red component, a green component, and a blue component;a first plurality of mux units to selectively route the first floating point number to a corresponding first lane of a graphics pipeline, the first floating point number to be processed by a multiplier unit of the first lane; anda second plurality of mux units to selectively route a fourth floating point number toan aligner unit of the first lane, wherein the aligner unit aligns the fourth floating point number to produce an aligned fourth floating point number and routes the aligned fourth floating point number to an adder unit of the first lane; andthe multiplier unit of the first lane, wherein the multiplier unit of the first lane multiplies the first floating point number with the fourth floating point number to produce a first multiplication result and increments an exponent of the first multiplication result, and wherein the first multiplication result is routed to the adder unit of the first lane for a subtraction operation with the aligned fourth floating point number.14. The system of15. The system of16. The system of17. The system of','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Systems and methods for simulating depth of field on a computer generated display','US08629868','Rockwell Collins, Inc.','Gardiner;Buckmiller','A system for simulating depth of field in a computer graphics display is provided according to an exemplary embodiment. The system comprises an image generation circuit configured to generate an image representing a plurality of objects for display on the computer graphics display. The image generation circuit is configured to generate the image using a single blur parameter for each of the plurality of objects that represents both a magnitude of blur and a relative distance with respect to a lens focal distance being simulated by the image generation circuit.','1','1. A system for simulating depth of field in a computer graphics display, comprising:an image generation circuit configured to generate an image representing a plurality of objects for display on the computer graphics display, wherein the image generation circuit is configured to generate the image using a single blur parameter for each of the plurality of objects that represents both a magnitude of blur and a relative distance with respect to a lens focal distance being simulated by the image generation circuit;wherein the relative distance represents whether the object is in a far-field, in-focus, or near-field position with respect to the focal distance;wherein the blur parameter is a base blur parameter value for in-focus objects, a higher value than the base blur parameter value for far-field objects, and a lower value than the base blur parameter value for near-field objects.2. The system of3. The system of4. The system of5. A system for simulating depth of field in a computer graphics display, comprising:an image generation circuit configured to generate an image representing a plurality of objects for display on the computer graphics display, wherein the image generation circuit is configured to generate the image using a single blur parameter for each of the plurality of objects that represents both a magnitude of blur and a relative distance with respect to a lens focal distance being simulated by the image generation circuit;wherein the relative distance represents whether the object is in a far-field, in-focus, or near-field position with respect to the focal distance;wherein the blur parameter is a base blur parameter value for in-focus objects, a lower value than the base blur parameter value for far-field objects, and a higher value than the base blur parameter value for near-field objects.6. The system of7. A method of anti-aliasing an image, the image having a plurality of pixels and a plurality of objects represented therein, comprising:using an image generation circuit to generate an image representing the plurality of objects for display on the computer graphics display;using properties of the objects to retrieve a blur value and a relative distance for each of a plurality of subpixels of each pixel, wherein the relative distance represents whether an object associated with the subpixel is in a far-field, in-focus, or near-field position with respect to a focal distance of the image;determining an anti-aliasing output for one or more pixels based on the blur values and relative distances of the subpixels of the pixel, wherein the anti-aliasing output for each pixel is determined using a non-averaging method such that the blur values of the subpixels are not averaged to determine the anti-aliasing output of the pixel; andproviding the determined anti-aliasing output for each pixel;wherein the anti-aliasing output is a base anti-aliasing output for pixels associated with in-focus objects, a lower value than the base anti-aliasing output for pixels associated with far-field objects, and a higher value than the base anti-aliasing value for pixels associated with near-field objects.8. The method ofdetermining an anti-aliasing color output for each pixel using an averaging method such that color values of the subpixels of the pixel are averaged to determine the anti-aliasing color output of the pixel.9. The method of10. The method of11. The method of12. The method of13. The method of14. The method of','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Volumetric data exploration using multi-point input controls','US08629869','Perceptive Pixel Inc.','Davidson','A three-dimensional data set is accessed. A two-dimensional plane is defined that intersects a space defined by the three-dimensional data set. The two-dimensional plane defines a two-dimensional data set within the three-dimensional data set and divides the three-dimensional data set into first and second subsets. A three-dimensional view based on the three-dimensional data set is rendered on such that at least a portion of the first subset of the three-dimensional data set is removed and at least a portion of the two-dimensional data set is displayed. A two-dimensional view of a first subset of the two-dimensional data set also is rendered. Controls are provided that enable visual navigation through the three-dimensional data set by engaging points on the multi-touch display device that correspond to either the three-dimensional view based on the three-dimensional data set and/or the two-dimensional view of the first subset of the two-dimensional data set.','1','1. A computer-implemented method of enabling visual navigation through a three-dimensional data set on a multi-touch display device that includes a touch surface, the method comprising:accessing a three-dimensional data set from a computer memory storage device;defining a two-dimensional planar bounded surface that intersects the three-dimensional data set, that defines a two-dimensional data set within the three-dimensional data set, and that divides the three-dimensional data set into first and second subsets of the three-dimensional data set, the two-dimensional bounded surface having a normal defining positive and negative directions relative to the two-dimensional bounded surface, the first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, the second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, and the two-dimensional data set including data from the three-dimensional data set that is intersected by the two-dimensional bounded surface;rendering, on a multi-touch display device, a three-dimensional view of the three-dimensional data set while also rendering the two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set;providing a first control that enables a user of the multi-touch display device to rotate the two-dimensional bounded surface in three dimensions about a point on the two-dimensional bounded surface, where the first control is displayed on the multi-touch display device as a handle coupled to an end of a vector displayed on the multi-touch surface, where the vector is displayed on the multi-touch surface as a vector normal to the two-dimensional bounded surface originating at the point on the two-dimensional bounded surface and extending away from the two-dimensional bounded surface to the end of the vector at the handle, the first control being configured to perform operations comprising:detect engagement by an input mechanism of a point on the multi-touch display device corresponding to the first control,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch display device,rotate the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the rotation of the two-dimensional bounded surface about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set;providing a second control based on contact at substantially any point along the vector displayed on the multi-touch display device that enables a user of the multi-touch display device to translate the two-dimensional bounded surface along the vector displayed on the multi-touch display device along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set, the normal being neither parallel nor perpendicular to a plane of the touch surface of the multi-touch display device, the second control being configured to perform operations comprising:detect engagement by an input mechanism of a point on the multi-touch display device corresponding to the second control, where the point on the multi-touch display device corresponding to the second control corresponds to the point on the two-dimensional bounded surface from which the vector displayed on the multi-touch surface originates,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch display device,translate the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the newly-positioned two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the new position of the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the new position of the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set;detecting concurrent engagement: (1) of a point on the multi-touch display device corresponding to the first control by a first input mechanism; and (2) of a point on the multi-touch display device corresponding to the second control by a second input mechanism;tracking movements of the first input mechanism and the second input mechanism while the first input mechanism and second input mechanism remain concurrently engaged with the multi-touch display device;rotating the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface from which the vector displayed on multi-touch surface originates as a function of the tracked movement of the first input mechanism;translating the two-dimensional bounded surface along the vector displayed on the multi-touch display device to a new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism;updating the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to concurrently reflect both the rotation of the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the first input mechanism and the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to the new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism.2. The method ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,translate the two-dimensional bounded surface on a plane containing and parallel to the two-dimensional bounded surface to a new position on the plane as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the translated two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the translated two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the new position of the translated two-dimensional bounded surface on the plane.3. The method ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,scale dimensions of the two-dimensional bounded surface on a plane containing and parallel to the two-dimensional bounded surface as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the scaled two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the scaled two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the scaled dimensions of the scaled two-dimensional bounded surface on the plane.4. The method ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,rotate the two-dimensional bounded surface on a plane that contains and is parallel to the two-dimensional bounded surface, the two-dimensional bounded surface being rotated in two dimensions around a point within the two-dimensional bounded surface as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the rotated two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the rotated two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the new position of the rotated two-dimensional bounded surface on the plane.5. The method of6. The method offurther comprising providing a rotation control that enables a user of the multi-touch display device to rotate the three-dimensional view of the three-dimensional data set, the rotation control being configured to:detect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the visual indications of the boundaries of the three-dimensional view of the three-dimensional data set,track movements of the one or more input mechanisms while the one or more input mechanisms remain engaged with the multi-touch display device,rotate the three-dimensional data set about an axis defined through the three-dimensional data set as a function of the tracked movement of the one or more input mechanisms to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the rotation of the three-dimensional data set about the axis defined through the three-dimensional data set as a function of the tracked movement of the one or more input mechanisms to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set.7. The method of8. The method of9. The method of10. The method of11. The method of12. The method of13. The method of14. The method of15. The method of16. The method of17. A tangible computer-readable storage device storing instructions that, when executed by a computing system, cause the computing system to perform operations comprising:access a three-dimensional data set from a computer memory storage device;define a two-dimensional planar bounded surface that intersects the three-dimensional data set, that defines a two-dimensional data set within the three-dimensional data set, and that divides the three-dimensional data set into first and second subsets of the three-dimensional data set, the two-dimensional bounded surface having a normal defining positive and negative directions relative to the two-dimensional bounded surface, the first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, the second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, and the two-dimensional data set including data from the three-dimensional data set that is intersected by the two-dimensional bounded surface;render, on a multi-touch display device having a touch surface, a three-dimensional view of the three-dimensional data set while also rendering the two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set;provide a first control that enables a user of the multi-touch display device to rotate the two-dimensional bounded surface in three dimensions about a point on the two-dimensional bounded surface, where the first control is displayed on the multi-touch display device as a handle coupled to an end of a vector displayed on the multi-touch surface, where the vector is displayed on the multi-touch surface as a vector normal to the two-dimensional bounded surface originating at the point on the two-dimensional bounded surface and extending away from the two-dimensional bounded surface to the end of the vector at the handle, the first control being configured to perform operations comprising:detect engagement by an input mechanism of a point on the multi-touch display device corresponding to the first control,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch display device,rotate the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the rotation of the two-dimensional bounded surface about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set;provide a second control based on contact at substantially any point alone the vector displayed on the multi-touch display device that enables a user of the multi-touch display device to translate the two-dimensional bounded surface along the vector displayed on the multi-touch display device along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set, the normal being neither parallel nor perpendicular to a plane of the touch surface of the multi-touch display device, the second control being configured to perform operations comprising:detect engagement by an input mechanism of a point on the multi-touch display device corresponding to the second control, where the point on the multi-touch display device corresponding to the second control corresponds to the point on the two-dimensional bounded surface from which the vector displayed on the multi-touch surface originates,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch display device,translate the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the newly-positioned two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the new position of the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the new position of the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set;detect concurrent engagement: (1) of a point on the multi-touch display device corresponding to the first control by a first input mechanism; and (2) of a point on the multi-touch display device corresponding to the second control by a second input mechanism;track movements of the first input mechanism and the second input mechanism while the first input mechanism and second input mechanism remain concurrently engaged with the multi-touch display device;rotate the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface from which the vector displayed on multi-touch surface originates as a function of the tracked movement of the first input mechanism;translate the two-dimensional bounded surface along the vector displayed on the multi-touch display device to a new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism;update the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to concurrently reflect both the rotation of the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the first input mechanism and the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to the new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism.18. The tangible computer-readable storage device ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,translate the two-dimensional bounded surface on a plane containing and parallel to the two-dimensional bounded surface to a new position on the plane as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the translated two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the translated two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the new position of the translated two-dimensional bounded surface on the plane.19. The tangible computer-readable storage device ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,scale dimensions of the two-dimensional bounded surface on a plane containing and parallel to the two-dimensional bounded surface as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the scaled two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the scaled two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the scaled dimensions of the scaled two-dimensional bounded surface on the plane.20. The tangible computer-readable storage device ofdetect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the display of the two-dimensional data set displayed within the three-dimensional view of the three-dimensional data set,track movement of the one or more input mechanisms while the one or more input mechanisms remain engaged with the one or more points on the multi-touch display device,rotate the two-dimensional bounded surface on a plane that contains and is parallel to the two-dimensional bounded surface, the two-dimensional bounded surface being rotated in two dimensions around a point within the two-dimensional bounded surface as a function of the tracked movement of the one or more input mechanisms to cause a new two-dimensional data set within the three-dimensional data set to be defined that corresponds to the rotated two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device of the three-dimensional view of the three-dimensional data set with the rotated two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set to reflect the new position of the rotated two-dimensional bounded surface on the plane.21. The tangible computer-readable storage device offurther storing instructions that, when executed by a computing system, cause the computing system to provide a rotation control that enables a user of the multi-touch display device to rotate the three-dimensional view of the three-dimensional data set, the rotation control being configured to:detect engagement by one or more input mechanisms of one or more points on the multi-touch display device corresponding to the visual indications of the boundaries of the three-dimensional view of the three-dimensional data set,track movements of the one or more input mechanisms while the one or more input mechanisms remain engaged with the multi-touch display device,rotate the three-dimensional data set about an axis defined through the three-dimensional data set as a function of the tracked movement of the one or more input mechanisms to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, andupdate the rendering, on the multi-touch display device, of the three-dimensional view of the three-dimensional data set to reflect the rotation of the three-dimensional data set about the axis defined through the three-dimensional data set as a function of the tracked movement of the one or more input mechanisms to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set.22. The tangible computer-readable storage device of23. The tangible computer-readable storage device of24. The tangible computer-readable storage device of25. The tangible computer-readable storage device of26. The tangible computer-readable storage device of27. The tangible computer-readable storage device of28. The tangible computer-readable storage device of29. A multi-touch display device comprising:a display having a touch surface;a multi-touch input sensor;a processor; anda computer memory device storing instructions that, when executed by the processor, cause the processor to perform operations comprising:access a three-dimensional data set;define a two-dimensional planar bounded surface that intersects the three-dimensional data set, that defines a two-dimensional data set within the three-dimensional data set, and that divides the three-dimensional data set into first and second subsets of the three-dimensional data set, the two-dimensional bounded surface having a normal defining positive and negative directions relative to the two-dimensional bounded surface, the first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, the second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, and the two-dimensional data set including data from the three-dimensional data set that is intersected by the two-dimensional bounded surface;render, on the device, a three-dimensional view of the three-dimensional data set while also rendering the two-dimensional bounded surface intersecting the three-dimensional data set, wherein at least a portion of the first subset of the three-dimensional data set is excluded from the three-dimensional view of the three-dimensional data set and at least a portion of the two-dimensional data set is displayed within the three-dimensional view of the three-dimensional data set;provide a first control that enables a user of the multi-touch display device to rotate the two-dimensional bounded surface in three dimensions about a point on the two-dimensional bounded surface, where the first control is displayed on the multi-touch display device as a handle coupled to an end of a vector displayed on the multi-touch surface, where the vector is displayed on the multi-touch surface as a vector normal to the two-dimensional bounded surface originating at the point on the two-dimensional bounded surface and extending away from the two-dimensional bounded surface to the end of the vector at the handle, the first control being configured to:detect engagement by an input mechanism of a point on the multi-touch input sensor corresponding to the first control,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch input sensor,rotate the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the positive direction relative to the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the rotated three-dimensional data set that is in the negative direction relative to the two-dimensional bounded surface, andupdate the rendering, on the display, of the three-dimensional view of the three-dimensional data set to reflect the rotation of the two-dimensional bounded surface about the point on the two-dimensional bounded surface as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set;provide a second control based on contact at substantially any point alone the vector displayed on the multi-touch display device that enables a user of the multi-touch display device to translate the two-dimensional bounded surface along the vector displayed on the multi-touch display device along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set, the normal being neither parallel nor perpendicular to a plane of the touch surface of the multi-touch display device, the second control being configured to perform operations comprising:detect engagement by an input mechanism of a point on the multi-touch input sensor corresponding to the second control, where the point on the multi-touch display device corresponding to the second control corresponds to the point on the two-dimensional bounded surface from which the vector displayed on the multi-touch surface originates,track movements of the input mechanism while the input mechanism remains engaged with the multi-touch input sensor,translate the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause the two-dimensional bounded surface to intersect a new two-dimensional data set within the three-dimensional data set and to divide the three-dimensional data set into new first and second subsets of the three-dimensional data set, the new first and second subsets of the three-dimensional data set being distinct and corresponding to points located on opposing sides of the newly-positioned two-dimensional bounded surface, the new first subset of the three-dimensional data set including data from the three-dimensional data set that is in the positive direction relative to the new position of the two-dimensional bounded surface, and the new second subset of the three-dimensional data set including data from the three-dimensional data set that is in the negative direction relative to the new position of the two-dimensional bounded surface, andupdate the rendering, on the display, of the three-dimensional view of the three-dimensional data set to reflect the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to a new position within the three-dimensional data set as a function of the tracked movement of the input mechanism to cause at least a portion of the new first subset of the three-dimensional data set to be excluded from the updated three-dimensional view of the three-dimensional data set and at least a portion of the new two-dimensional data set to be displayed within the three-dimensional view of the three-dimensional data set;detect concurrent engagement: (1) of a point on the multi-touch input sensor corresponding to the first control by a first input mechanism; and (2) of a point on the multi-touch input sensor corresponding to the second control by a second input mechanism;track movements of the first input mechanism and the second input mechanism while the first input mechanism and second input mechanism remain concurrently engaged with the multi-touch input sensor;rotate the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface from which the vector displayed on multi-touch surface originates as a function of the tracked movement of the first input mechanism;translate the two-dimensional bounded surface along the vector displayed on the multi-touch display device to a new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism;update the rendering, on the display, of the three-dimensional view of the three-dimensional data set to concurrently reflect both the rotation of the two-dimensional bounded surface in three dimensions about the point on the two-dimensional bounded surface as a function of the tracked movement of the first input mechanism and the translation of the two-dimensional bounded surface along the normal to the two-dimensional bounded surface to the new position within the three-dimensional data set as a function of the tracked movement of the second input mechanism.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Apparatus, method, and program for displaying stereoscopic images','US08629870','Fujifilm Corporation','Oota;Sutoh;Ueno;Mishiba;Wantanabe;Sawachi','A stereoscopic image display apparatus, includes an image obtaining unit, for obtaining a plurality of images, which are obtained by photography of a subject from different positions by a photography unit, a distance measuring unit, for measuring the distance to the subject from the photography unit, a stereoscopic image generating unit, for generating a stereoscopic image for three dimensional display from the plurality of images, a display region generating unit, for generating display regions that display the images in having amounts of parallax corresponding to the distance, when the display regions are displayed overlapped in the stereoscopic image, a display unit, for displaying the stereoscopic image, on which the display regions are overlapped, and a zoom unit for enlarging reducing the stereoscopic image, the display region generating unit changing sizes of the display regions according to the zoom magnification rate when the stereoscopic image is enlarged/reduced.','1','1. A stereoscopic image display apparatus, comprising:an image obtaining unit, for obtaining a plurality of images, which are obtained by photography of a subject, using visible light, by a plurality of photography units which are positioned at different positions;a distance measuring unit, for measuring a distance to the subject from the photography unit;a stereoscopic image generating unit, for generating a stereoscopic image for three dimensional display from the plurality of images;a display region generating unit, for generating display regions that display images of a menu screen, which are different from the plurality of images obtained by the photography, in having amounts of parallax based on the distance, when the display regions are displayed overlapped in the stereoscopic image;a display unit, for displaying the stereoscopic image, on which the display regions are overlapped; anda zoom unit for enlarging and reducing the stereoscopic image,wherein the display region generating unit comprises a unit that changes sizes of the display regions according to a zoom magnification rate when the stereoscopic image is enlarged and reduced,wherein the display regions are enlarged when the zoom magnification rate is higher and the display regions are smaller when the zoom magnification rate is lower, andwherein the relative location of the display regions with respect to the subject changes according to the zoom magnification rate.2. A stereoscopic image display apparatus as defined ina memory unit, for storing a table in which various types of distance sections are correlated with display parameters,wherein the display region generating unit refers to the table to obtain display parameters correlated with the distance sections corresponding to the distance, and generates the display regions that display the images in having amounts of parallax corresponding to the distance, based on the display parameters.3. A stereoscopic image display apparatus as defined in4. A stereoscopic image display apparatus as defined ina command input unit, for inputting commands to change the amounts of parallax with which images are displayed,wherein the display region generating unit changes the amounts of parallax with which the display regions are displayed, in response to the input commands.5. A stereoscopic image display apparatus as defined inwherein the display region generating unit generates the display region with decreased brightness in cases that the stereoscopic image is to be displayed, compared to the level of brightness in cases that the images are displayed two dimensionally.6. A stereoscopic image display apparatus as defined in7. A stereoscopic image display apparatus as defined inwherein the display region generating unit generates display regions as images displayed two dimensionally, in a case that the at least one image is to be displayed two dimensionally.8. A stereoscopic image display method, comprising:in a computer, obtaining a plurality of images, which are obtained by photography of a subject, using visible light, from different positions by a plurality of photography units which are positioned at different positions;measuring a distance to the subject from the photography unit;generating a stereoscopic image for three dimensional display from the plurality of images;generating display regions that display images of a menu screen, which are different from the plurality of images obtained by the photography, in having amounts of parallax based on the distance, when the display regions are displayed overlapped in the stereoscopic image;displaying the stereoscopic image, on which the display regions are overlapped; andchanging sizes of the display regions according to a zoom magnification rate when the stereoscopic image is enlarged and reduced,wherein the display regions are enlarged when the zoom magnification rate is higher and the display regions are smaller when the zoom magnification rate is lower, andwherein the relative location of the display regions with respect to the subject9. A computer readable non-transitory medium having a program recorded therein that causes a computer to execute the procedures of:obtaining a plurality of images, which are obtained by photography of a subject, using visible light, from different positions by a plurality of photography units which are positioned at different positions;measuring a distance to the subject from the photography unit;generating a stereoscopic image for three dimensional display from the plurality of images;generating display regions that display images of a menu screen, which are different from the plurality of images obtained by the photography, in having amounts of parallax based on the distance, when the display regions are displayed overlapped in the stereoscopic image;displaying the stereoscopic image, on which the display regions are overlapped; andchanging sizes of the display regions according to a zoom magnification rate when the stereoscopic image is enlarged and reduced,wherein the display regions are enlarged when the zoom magnification rate is higher and the display regions are smaller when the zoom magnification rate is lower, andwherein the relative location of the display regions with respect to the subject changes according to the zoom magnification rate.10. A stereoscopic image display apparatus as defined in11. A method as defined in12. A computer readable non-transitory medium as defined in','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Systems and methods for rendering three-dimensional objects','US08629871','Zynga Inc.','O'Brien;Ogles;Hyatt','In one embodiment, a three-dimensional object is rendered on a two-dimensional display screen by associating a three-dimensional mesh with an image of the object, generating a vector-based texture map that defines a surface area of the object, and rendering the vector-based texture map on the display screen.','1','1. A method comprising:associating a three-dimensional mesh with a two-dimensional image that depicts at least a portion of a three-dimensional object;generating a vector-based texture map that lists endpoints of a color boundary within the two-dimensional image that depicts at least the portion of the three-dimensional object,the generating of the vector-based texture map being performed by a processor;applying the vector-based texture map that lists the endpoints of the color boundary to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object; andrendering the vector-based texture map applied to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object,the rendering of the vector-based texture map being on a two-dimensional display screen to represent the three-dimensional object.2. A system comprising:an association module stored in a memory and configured to associate a three-dimensional mesh with a two-dimensional image that depicts at least a portion of a three-dimensional object;a processor configured by a mapping module to generate a vector-based texture map that lists endpoints of a color boundary within the two-dimensional image that depicts at least the portion of the three-dimensional object; anda renderer stored in the memory and configured toapply the vector-based texture map that lists the endpoints of the color boundary to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object, and torender the vector-based texture map applied to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object,the rendering of the vector-based texture map being on a two-dimensional display screen to represent the three-dimensional object.3. The method ofthe vector-based texture map includes a pair of vertices that define locations of a starting point of a curve that defines the color boundary and an ending point of the curve that defines the color boundary; andthe rendering of the vector-based texture map on the two-dimensional display screen renders the pair of vertices that define the locations of the starting and ending points of the curve that defines the color boundary.4. The method ofthe vector-based texture map lists a plurality of vertices that define further endpoints of a plurality of curves that include a curve that defines the color boundary.5. The method ofthe vector-based texture map lists a plurality of curves that join vertices and define further color boundaries.6. The method ofthe vector-based texture map lists a plurality of curves that include a line segment between two vertices.7. The method ofthe vector-based texture map lists a plurality of curves that include a quadratic bezier that starts at a first vertex, ends at a second vertex, and uses a third vertex as a control point.8. The method ofthe vector-based texture map lists a plurality of curves as including a cubic bezier.9. The method ofthe vector-based texture map lists a fill color defined for at least one of a right side of a curve that defines the color boundary or a left side of the curve that defines the color boundary.10. The method ofthe vector-based texture map lists multiple curves defining a single fill color of a particular area of the vector-based texture map.11. The method ofthe rendering of the vector-based texture map includes:cutting the vector-based texture map into multiple triangular pieces having three vertices each; andprojecting the three vertices of each of the multiple triangular pieces into a screen space.12. The method ofa triangular piece among the multiple triangular pieces includes a curve that defines the color boundary completely contained within the triangular piece.13. The method ofa triangular piece among the multiple triangular pieces includes a portion of the color boundary from the vector-based texture map; andthe cutting of the vector-based texture map includes determining the portion of the color boundary by splitting the color boundary at an intersection point defined by an edge of the triangular piece.14. The method ofthe cutting of the vector-based texture map includes creating three new curves that represent three edges of a triangular piece among the multiple triangular pieces.15. The system ofthe vector-based texture map includes a pair of vertices that define locations of a starting point of a curve that defines the color boundary and an ending point of the curve that defines the color boundary; andthe renderer is configured to render the pair of vertices that define the locations of the starting and ending points of the curve that defines the color boundary.16. The system ofthe renderer is configured to:cut the vector-based texture map into multiple triangular pieces having three vertices each; andproject the three vertices of each of the multiple triangular pieces into a screen space.17. The system ofa triangular piece among the multiple triangular pieces includes a portion of the color boundary from the vector-based texture map; andthe render is configured to determine the portion of the color boundary by splitting the color boundary at an intersection point defined by an edge of the triangular piece.18. The system ofthe render is configured to add a vector control point to a curve that defines the color boundary, the vector control point being added at a location where the curve intersects with at least one of a face of the three-dimensional mesh or a border of the three-dimensional mesh.19. A non-transitory computer-readable article of manufacture comprising instructions that, when executed by one or more processors of a machine, cause the machine to perform operations comprising:associating a three-dimensional mesh with two-dimensional image that depicts at least a portion of a three-dimensional object;generating a vector-based texture map that lists endpoints of a color boundary within the two-dimensional image that depicts at least the portion of the three-dimensional object,the generating of the vector-based texture map being performed by the one or more processors of the machine;applying the vector-based texture map that lists the endpoints of the color boundary to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object; andrendering the vector-based texture map applied to the three-dimensional mesh associated with the two-dimensional image that depicts at least the portion of the three-dimensional object,the rendering of the vector-based texture map being on a two-dimensional display screen to represent the three-dimensional object.20. The non-transitory computer-readable article of manufacture ofthe rendering of the vector-based texture map includes:computing barycentric coordinates of a vertex in a triangular piece of the vector-based texture map; anddetermining screen coordinates of the vertex based on the barycentric coordinates of the vertex.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('System and method for displaying and analyzing financial correlation data','US08629872','The Capital Group Companies, Inc.','Phoa','A method for displaying a matrix of correlations or other statistical measures of co-movement associated with a plurality of financial instruments, portfolios, indices, or asset classes is disclosed. The method includes: converting the matrix of correlations or other co-movement measures into a probability transition matrix; defining a corresponding abstract distance measurement between any two of the plurality of financial instruments, portfolios, indices, or asset classes based on the probability transition matrix; assigning coordinates in a Euclidean space to each of the plurality of financial instruments, portfolios, indices, or asset classes, wherein a Euclidean distance between any two financial instruments, portfolios, indices, or asset classes in the Euclidean space corresponds to the corresponding abstract distance measurement; and displaying on a display device the plurality of financial instruments, portfolios, indices, or asset classes based on more significant dimensions of the Euclidean space.','1','1. A method for displaying a matrix of correlations or other statistical measures of co-movement associated with a plurality of financial instruments, portfolios, indices, or asset classes, the method comprising:converting the matrix of correlations or other co-movement measures into a probability transition matrix;defining a corresponding abstract distance measurement between any two of the plurality of financial instruments, portfolios, indices, or asset classes based on the probability transition matrix;assigning coordinates in a Euclidean space to each of the plurality of financial instruments, portfolios, indices, or asset classes, wherein a Euclidean distance between any two financial instruments, portfolios, indices, or asset classes in the Euclidean space corresponds to the corresponding abstract distance measurement;displaying on a display device the plurality of financial instruments, portfolios, indices, or asset classes based on significant dimensions of the Euclidean space; andgenerating a measure of diversification of the financial instruments, portfolios, indices, or asset classes, whereinthe measure of diversification comprises a global concentration, andthe generating of the global concentration comprises:assigning a weight to each of the financial instruments, portfolios, indices, or asset classes; andweighting a contribution of each of the financial instruments, portfolios, indices, or asset classes by its respective said weight in the global concentration.2. The method of3. The method of4. The method of5. The method of6. The method of7. The method of8. The method of9. The method of10. The method of11. The method of12. The method of13. The method ofidentifying ones of the financial instruments, portfolios, indices, or asset classes;assigning second weights to respective said ones of the financial instruments, portfolios, indices, or asset classes; andgenerating the global concentration by only using the ones of the financial instruments, portfolios, indices, or asset classes in place of each of the financial instruments, portfolios, indices, or asset classes, and using the second weights in place of the weight of each of the financial instruments, portfolios, indices, or asset classes.14. The method of15. The method of16. The method of17. The method of18. The method of19. The method of20. The method ofin response to the displaying on the display device, receiving a user command to modify an attribute for a selected one of the plurality of financial instruments, portfolios, indices, or asset classes; andmodifying the attribute in response to the user command.21. The method of22. The method of23. A system for displaying a matrix of correlations or other statistical measures of co-movement associated with a plurality of financial instruments, portfolios, indices, or asset classes, comprising:a processor;a display device coupled to the processor; anda nonvolatile storage device coupled to the processor and storing instructions that, when executed by the processor, cause the processor to:convert the matrix of correlations or other co-movement measures into a probability transition matrix;define a corresponding abstract distance measurement between any two of the plurality of financial instruments, portfolios, indices, or asset classes based on the probability transition matrix;assign coordinates in a Euclidean space to each of the plurality of financial instruments, portfolios, indices, or asset classes, wherein a Euclidean distance between any two financial instruments, portfolios, indices, or asset classes in the Euclidean space corresponds to the corresponding abstract distance measurement;display on the display device the plurality of financial instruments, portfolios, indices, or asset classes based on significant dimensions of the Euclidean space; andgenerate a measure of diversification of the financial instruments, portfolios, indices, or asset classes, whereinthe measure of diversification comprises a global concentration, andthe instructions, when executed by the processor, further cause the processor to generate the global concentration by:assigning a weight to each of the financial instruments, portfolios, indices, or asset classes; andweighting a contribution of each of the financial instruments, portfolios, indices, or asset classes by its respective said weight in the global concentration.24. The system of25. The system of26. The system of','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Data analysis system','US08629873','Shimadzu Corporation','Kageyama','In a liquid chromatograph mass spectrometer (LC/MS), three kinds of graphic data (or graphs) composed of an LC chromatogram, an MS chromatogram, and an MS spectrum should be mutually compared to perform a specific analysis. Conventionally, a plurality of graphs are arranged for comparison on one monitor in accordance with a user's purpose. However, when an operation is once terminated and then the graphs are displayed again or in other processes, their display formats are not reproduced. Hence, the user needs to manually set the desired layout every time, which is cumbersome. The data analysis system according to the present invention has been developed to solve this problem. The data analysis system has a layout memory for storing layout information including a plurality of kinds of graphs to be displayed on the monitor and the display position of each graph, and refers to the layout information when displaying the graphs.','1','1. A data analysis system having a function of arranging a plurality of graphs on a monitor, the graphs being based on analysis data which are obtained as a result of analyses by different kinds of analyzing apparatuses, comprising:a layout memory, which is a nonvolatile memory, for memorizing layout information including at least kinds of the plurality of graphs to be displayed on the monitor and display position of each of the graphs;a graph display unit for displaying graphs of the analysis data, in response to an input of a graph display command or at a predetermined timing and based on the layout information;a layout change unit for changing the kinds of the graphs to be displayed on the monitor and/or the display position of each of the graphs, in response to an input of a layout change command; anda layout memorizing unit for storing at least the kinds of the graphs which are displayed on the monitor and display position of each of the graphs as the layout information in the layout memory, in response to an input of a layout memorizing command or at a predetermined timing; wherein:the layout information stored in the layout memory further includes relation information on a relationship between the kinds of the graphs, andthe data analyzing system further comprises a display format change interlock unit for changing, in response to an input of a display format change command with regard to one of the graphs which are displayed on the monitor, a display format of the graph and for referring to the layout information to make an interlocked change to a display format of another related graph which is displayed on the monitor.2. The data analysis system according to3. The data analysis system according to4. The data analysis system according tothe layout information stored in the layout memory further includes relevant information on a relationship between the kinds of the graphs, andthe data analyzing system further comprises a display format change interlock unit for changing, in response to an input of a display format change command with regard to one of the graphs which are displayed on the monitor, a display format of the graph and for referring to the layout information to make an interlocked change to a display format of another related graph which is displayed on the monitor.5. The data analysis system according toin a case where an interlock halt command is applied to one or plural graphs which are displayed on the monitor, the display format change interlock unit makes the interlocked change to the display format of the graph other than the graph or graphs to which the interlock halt command has been applied.6. The data analysis system according tothe layout information stored in the layout memory further includes relevant information on a relationship between the kinds of the graphs, andthe data analyzing system further comprises a display format change interlock unit for changing, in response to an input of a display format change command with regard to one of the graphs which are displayed on the monitor, a display format of the graph and for referring to the layout information to make an interlocked change to a display format of another related graph which is displayed on the monitor.7. The data analysis system according toin a case where an interlock halt command is applied to one or plural graphs which are displayed on the monitor, the display format change interlock unit makes the interlocked change to the display format of the graph other than the graph or graphs to which the interlock halt command has been applied.8. The data analysis system according toin a case where an interlock halt command is applied to one or plural graphs which are displayed on the monitor, the display format change interlock unit makes the interlocked change to the display format of the graph other than the graph or graphs to which the interlock halt command has been applied.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Mobile display device and method of controlling display of conversion candidates of input characters on the mobile display device','US08629874','Kyocera Corporation','Honda;Tsuruta;Urano;Takei','It is possible to provide a mobile display device which can improve usability even when the character size is increased for improving visibility. A method for controlling the mobile display device is also disclosed. A control unit (','1','1. A mobile display device, comprising:an input information acquisition part acquiring input information,a display part having a first display region and a second display region, anda control part which performs processing for input of a character in accordance with the input information acquired by the input information acquisition part, displays the input character in the first display region of the display part, and displays a plurality of conversion candidates of the input character in the second display region of the display part, whereinthe control part determines at least one conversion candidate to be displayed in the second display region among the plurality of conversion candidates in accordance with a display size of the conversion candidates displayed in the second display region, wherein the control part changes display priorities of the plurality of conversion candidates in the second display region in accordance with a display size of the conversion candidates displayed in the second display region and the control part sets degrees of priority of display of the conversion candidates depending upon the number of displayable characters in the second display region based on the display size of the conversion candidates and a number of characters of each of the plurality of conversion candidates, wherein setting the degrees of priority comprises an operation to rearrange the conversion candidates in a list comprising the conversion candidates where the rearranging is done at least according to their respective lengths.2. A mobile display device as set forth inthe control part sets a conversion candidate to be displayed second based on a difference between the number of displayable characters in the second display region and the number of characters of the conversion candidate displayed first.3. A mobile display device as set forth in4. A mobile display device as set forth in5. A mobile display device as set forth in6. A mobile display device as set forth in7. A mobile display device as set forth in8. A mobile display device as set forth in9. A mobile display device as set forth in10. A mobile display device as set forth inthe device has with a display mode setting part enabling setting of either of a first display mode in which the plurality of conversion candidates are displayed in a first display size or a second display mode in which the plurality of conversion candidates are displayed in a second display size larger than the first display size, andthe control part changes a display form of the conversion candidate selected from the plurality of conversion candidates being displayed in the second display region in accordance with selection information acquired at the input information acquisition part by selection of the conversion candidate to be selected from the plurality of conversion candidates by a different method in accordance with the display mode selected at the display mode setting part.11. A mobile display device as set forth in12. A mobile display device as set forth in13. A control method in a mobile display device comprising:acquiring input information,performing processing for input of character in accordance with the acquired input information,displaying the input character of the input processing in a first display region of a display part,displaying a plurality of conversion candidates of the input character of the input processing in a second display region of the display part,determining at least one conversion candidate to be displayed in the second display region among the plurality of conversion candidates in accordance with a display size of the conversion candidates displayed in the second display region,changing display priorities of the plurality of conversion candidates in the second display region in accordance with a display size of the conversion candidates displayed in the second display region, andsetting degrees of priority of display of the conversion candidates depending upon the number of displayable characters in the second display region based on the display size of the conversion candidates and a number of characters of each of the plurality of conversion candidates, wherein setting the degrees of priority comprises an operation to rearrange the conversion candidates in a list comprising the conversion candidates where the rearranging is done at least according to their respective lengths.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Constraint systems and methods for manipulating non-hierarchical objects','US08629875','QUALCOMM Incorporated','Guerrab;Smithers;Elmieh','Methods and apparatus for animating images using bidirectional constraints are described.','1','1. A computer-implemented method for animating images, the method comprising:specifying, by a processor, a first bidirectional constraint between a first object and a second object;assigning weights to the first bidirectional constraint and to at least one other constraint between the first and second objects, wherein the assigned weights correspond to a relative degree of effect on the first and second objects, and wherein a prioritization scheme determines which of the first bidirectional constraint and the at least one other constraint controls;detecting execution of a command;determining whether the command relates to the first object or to the second object;creating a first graph for a first scene based on the first bidirectional constraint in response to determining that the command relates to the first object, wherein the first graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the second object based on a first change to the first object caused by the command;creating a second graph for a second scene based on the first bidirectional constraint in response to determining that the command relates to the second object, wherein the second graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the first object based on a first change to the second object caused by the command; andrendering one or more images based on the first bidirectional constraint.2. The computer-implemented method ofrendering at least one of the one or more images based on the first graph in response to determining that the command relates to the first object.3. The computer-implemented method ofrendering at least one of the one or more images based on the second graph in response to determining that the command relates to the second object.4. The computer-implemented method of5. The computer-implemented method of6. The computer-implemented method ofrendering an image based on the first change to the first state of the first object and the first change to the first state of the second object, or based on the second change to the second state of the second object and the second change to the second state of the first object.7. The computer-implemented method ofrendering at least one of the one or more images, at a first instance in time, based on the first change to the first state of the first object and the first change to the first state of the second object; andrendering the at least one of the one or more images, at a second instance in time, based on the second change to the second state of the second object and the second change to the second state of the first object.8. The computer-implemented method ofcreating the first bidirectional constraint without changing an existing constraint between either the first object or the second object and a third object.9. The computer-implemented method ofspecifying a second bidirectional constraint between the first object and a third object;determining a blended effect on the first object from the first bidirectional constraint and the second bidirectional constraint; andrendering the one or more images based on the blended effect.10. A computer system for animating images, the system comprising:means for specifying a first bidirectional constraint between a first object and a second object;means for assigning weights to the first bidirectional constraint and to at least one other constraint between the first and second objects, wherein the assigned weights correspond to a relative degree of effect on the first and second objects, and wherein a prioritization scheme determines which of the first bidirectional constraint and the at least one other constraint controls;means for detecting execution of a command;means for determining whether the command relates to the first object or to the second object;means for creating a first graph for a first scene based on the first bidirectional constraint in response to determining that the command relates to the first object, wherein the first graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the second object based on a first change to the first object caused by the command;means for creating a second graph for a second scene based on the first bidirectional constraint in response to determining that the command relates to the second object, wherein the second graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the first object based on a first change to the second object caused by the command; andmeans for rendering one or more images based on the first bidirectional constraint.11. The computer system ofmeans for rendering at least one of the one or more images based on the first graph in response to determining that the command relates to the first object.12. The computer system ofmeans for rendering at least one of the one or more images based on the second graph in response to determining that the command relates to the second object.13. The computer system of14. The computer system of15. The computer system ofmeans for rendering an image based on the first change to the first state of the first object and the first change to the first state of the second object, or based on the second change to the second state of the second object and the second change to the second state of the first object.16. The computer system ofmeans for rendering at least one of the one or more images, at a first instance in time, based on the first change to the first state of the first object and the first change to the first state of the second object; andmeans for rendering the at least one of the one or more images, at a second instance in time, based on the second change to the second state of the second object and the second change to the second state of the first object.17. The computer system ofmeans for creating the first bidirectional constraint without changing an existing constraint between either the first object or the second object and a third object.18. The computer system ofmeans for specifying a second bidirectional constraint between the first object and a third object;means for determining a blended effect on the first object from the first bidirectional constraint and the second bidirectional constraint; andmeans for rendering the one or more images based on the blended effect.19. A system for animating images, the system comprising:a processor configured with processor-executable instructions to perform operations comprising:specifying a first bidirectional constraint between a first object and a second object;assigning weights to the first bidirectional constraint and to at least one other constraint between the first and second objects, wherein the assigned weights correspond to a relative degree of effect on the first and second objects, and wherein a prioritization scheme determines which of the first bidirectional constraint and the at least one other constraint controls;detecting execution of a command;determining whether the command relates to the first object or to the second object;creating a first graph for a first scene based on the first bidirectional constraint in response to determining that the command relates to the first object, wherein the first graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the second object based on a first change to the first object caused by the command;creating a second graph for a second scene based on the first bidirectional constraint in response to determining that the command relates to the second object, wherein the second graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the first object based on a first change to the second object caused by the command; andrendering one or more images based on the first bidirectional constraint.20. The system ofrendering at least one of the one or more images based on the first graph in response to determining that the command relates to the second object.21. The system ofrendering at least one of the one or more images based on the second graph in response to determining that the command relates to the second object.22. The system of23. The system of24. The system ofrendering an image based on the first change to the first state of the first object and the first change to the first state of the second object, or based on the second change to the second state of the second object and the second change to the second state of the first object.25. The system ofrendering at least one of the one or more images, at a first instance in time, based on the first change to the first state of the first object and the first change to the first state of the second object; andrendering the at least one of the one or more images, at a second instance in time, based on the second change to the second state of the second object and the second change to the second state of the first object.26. The system ofcreating the first bidirectional constraint without changing an existing constraint between either the first object or the second object and a third object.27. The system ofspecifying a second bidirectional constraint between the first object and a third object; determine a blended effect on the first object from the first bidirectional constraint and the second bidirectional constraint; andrendering the one or more images based on the blended effect.28. A non-transitory computer-readable medium having stored thereon processor-executable instructions configured to cause a computer processor to perform operations comprising:specifying a first bidirectional constraint between a first object and a second object;assigning weights to the first bidirectional constraint and to at least one other constraint between the first and second objects, wherein the assigned weights correspond to a relative degree of effect on the first and second objects, and wherein a prioritization scheme determines which of the first bidirectional constraint and the at least one other constraint controls;detecting execution of a command;determining whether the command relates to the first object or to the second object;creating a first graph for a first scene based on the first bidirectional constraint in response to determining that the command relates to the first object, wherein the first graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the second object based on a first change to the first object caused by the command;creating a second graph for a second scene based on the first bidirectional constraint in response to determining that the command relates to the second object, wherein the second graph depicts the first bidirectional constraint as a unidirectional constraint specifying a resulting change to the first object based on a first change to the second object caused by the command; andrendering one or more images based on the first bidirectional constraint.29. The non-transitory computer-readable medium ofrendering at least one of the one or more images based on the first graph in response to determining that the command relates to the first object.30. The non-transitory computer-readable medium ofrendering at least one of the one or more images based on the second graph in response to determining that the command relates to the second object.31. The non-transitory computer-readable medium of32. The non-transitory computer-readable medium of33. The non-transitory computer-readable medium ofrendering an image based on the first change to the first state of the first object and the first change to the first state of the second object, or based on the second change to the second state of the second object and the second change to the second state of the first object.34. The non-transitory computer-readable medium ofrendering at least one of the one or more images, at a first instance in time, based on the first change to the first state of the first object and the first change to the first state of the second object; andrendering the at least one of the one or more images, at a second instance in time, based on the second change to the second state of the second object and the second change to the second state of the first object.35. The non-transitory computer-readable medium ofcreating the first bidirectional constraint without changing an existing constraint between either the first object or the second object and a third object.36. The non-transitory computer-readable medium ofspecifying a second bidirectional constraint between the first object and a third object;determining a blended effect on the first object from the first bidirectional constraint and the second bidirectional constraint; andrendering the one or more images based on the blended effect.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Displayport control and data registers','US08629876','Apple Inc.','Whitby-Strevens;Kyriazis','Circuits, methods, and apparatus for registers to store information that may be used by devices in a display system. One example provides control and data registers in a display to store information pertaining to a display system that includes the display. The registers can store attributes of the display, a host device, and a branch device. The information may include an organizationally unique identifier, chip identification, major and minor chip revision information, and firmware major and minor revision information.','1','1. A display comprising:a first bus interface for receiving an auxiliary channel;a first plurality of registers coupled to the auxiliary channel to store information regarding the display; anda second plurality of registers coupled to the auxiliary channel to store information regarding a host device,wherein the information regarding a host device comprises:an organizationally unique identifier; anda chip identification identifying one or more integrated circuits of the host.2. The display ofa third plurality of registers coupled to the auxiliary channel to store information regarding a branch device.3. The display ofan organizationally unique identifier.4. The display ofa chip identification identifying one or more integrated circuits of the display.5. The display ofinformation regarding major and minor revisions of one or more integrated circuits of the display.6. The display ofinformation regarding major and minor revisions of firmware used by the display.7. The display ofinformation regarding revisions of one or more integrated circuits of the host.8. The display ofinformation regarding revisions of firmware used by the host.9. The display ofinformation regarding revisions of firmware used by the host.10. The display ofinformation regarding revisions of firmware used by the host.11. A display comprising:a first bus interface for receiving an auxiliary channel;a first plurality of registers coupled to the auxiliary channel to store information regarding the display, the information regarding the display comprising:an organizationally unique identifier; anda chip identification identifying one or more integrated circuits in the display; anda second plurality of registers coupled to the auxiliary channel to store information regarding the host, the information regarding the host comprising:an organizationally unique identifier; anda chip identification identifying one or more integrated circuits in the host.12. The display ofinformation regarding major and minor revisions of one or more integrated circuits of the display.13. The display ofinformation regarding major and minor revisions of firmware used by the display.14. The display ofa third plurality of registers coupled to the auxiliary channel to store information regarding an adapter.15. The display ofan organizationally unique identifier; anda chip identification, identifying one or more integrated circuits in the adapter.16. A method of operating a display comprising:reading information regarding a host from a register on the display;determining whether a workaround exists for a known problem with the host;implementing the workaround; andreceiving data for an image to be displayed on the display,wherein the information regarding the host comprises:an organizationally unique identifier;a chip identification identifying one or more integrated circuits of the host.17. The method ofdetermining whether the host has a capability that may be used; andusing the capability.18. The method ofusing the information regarding the host in an error report.19. The method ofusing the information regarding the host in a compliance test.20. The method ofinformation regarding major and minor revisions of one or more integrated circuits of the host; andinformation regarding major and minor revisions of firmware used by the host.21. A display comprising:a first bus interface for receiving an auxiliary channel;a first plurality of registers coupled to the auxiliary channel to store information regarding the display; anda second plurality of registers coupled to the auxiliary channel to store information regarding a host device,wherein the information regarding a host device comprises:information regarding revisions of firmware used by the host.22. The display ofa chip identification identifying one or more integrated circuits of the host.23. The display ofinformation regarding revisions of one or more integrated circuits of the host.24. A display comprising:a first bus interface for receiving an auxiliary channel;a first plurality of registers coupled to the auxiliary channel to store information regarding the display; anda second plurality of registers coupled to the auxiliary channel to store information regarding a host device,wherein the information regarding a host device comprises:a chip identification identifying one or more integrated circuits of the host; andinformation regarding revisions of one or more integrated circuits of the host.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Extension to a hypervisor that utilizes graphics hardware on a host','US08629878','Red Hat, Inc.','Jackson','Graphics rendering in a virtual machine system is accelerated by utilizing host graphics hardware. In one embodiment, the virtual machine system includes a server that hosts a plurality of virtual machines. The server includes one or more graphics processing units. Each graphics processing unit can be allocated to multiple virtual machines to render images. A hypervisor that runs on the server is extended to include a redirection module, which receives a rendering request from a virtual machine and redirects the rendering request to a graphics driver. The graphics driver can commands an allocated portion of a graphics processing unit to render an image on the server.','1','1. A method comprising:receiving a representation of drawing commands issued by a guest operating system (OS) of one of a plurality of virtual machines hosted by a server; andcausing, by a processing device, an allocated portion of a graphics processing unit on the server to render an image according to the representation of drawing commands, the graphics processing unit being allocated to the plurality of virtual machines.2. The method ofreceiving a rendering request from the one of the virtual machines; andredirecting the rendering request to a graphics driver, the graphics driver to command the graphics processing unit to render the image.3. The method oftranslating the representation of drawing commands into a display list by a virtual graphics device associated with the guest OS, the display list being an encoded list of commands describing the image to be viewed on a remote client.4. The method ofstoring the image rendered by the graphics processing unit on the server;continuing to receive new drawing commands from the guest OS; andsending the image and a representation of the new drawing commands to a remote client when the remote client connects to the virtual machine.5. The method ofcomparing, by a virtual graphics device associated with the guest OS, a space occupied by the representation of drawing commands with a space occupied by the image to be rendered; anddetermining, by the virtual graphics device, when to send a rendering request to render the image by the server.6. The method of7. A system comprising:a server to host a plurality of virtual machines, the server comprising a graphics processing unit,the graphics processing unit allocated to the plurality of virtual machines to render images for the virtual machines, wherein an allocated portion of the graphics processing unit renders an image according to a representation of drawing commands issued by a guest operating system (OS) of one of the plurality of virtual machines.8. The system ofa redirection module on the server, the redirection module to receive a rendering request from the one of the virtual machines and to redirect the rendering request to a graphics driver, which commands the graphics processing unit to render an image.9. The system of10. The system ofa virtual graphics device on each virtual machine, the virtual graphics device to translate drawing commands from a guest OS into a display list, wherein the display list is an encoded list of commands that describes an image to be viewed on a remote client.11. The system ofa virtual graphics device on each virtual machine, the virtual graphics device to determine when to send a rendering request to render an image by the server.12. The system ofa network interface coupled to the server, the network interface to connect the server to remote clients on which the rendered images are to be viewed.13. The system of14. A non-transitory computer readable storage medium including instructions that, when executed by a processing device, cause the processing device to perform operations comprising:receiving a representation of drawing commands issued by a guest operating system (OS) of one of a plurality of virtual machines hosted by a server; andcausing an allocated portion of a graphics processing unit on the server to render an image according to the representation of drawing commands, the graphics processing unit being allocated to the plurality of virtual machines.15. The non-transitory computer readable storage medium ofreceiving a rendering request from the one of the virtual machines; andredirecting the rendering request to a graphics driver, the graphics driver to command the graphics processing unit to render the image.16. The non-transitory computer readable storage medium oftranslating the representation of drawing commands into a display list by a virtual graphics device associated with the guest OS, the display list being an encoded list of commands describing the image to be viewed on a remote client.17. The non-transitory computer readable storage medium ofstoring the image rendered by the graphics processing unit on the server;continuing to receive new drawing commands from the guest OS; andsending the image and a representation of the new drawing commands to a remote client when the remote client connects to the virtual machine.18. The computer readable storage medium ofcomparing, by a virtual graphics device associated with the guest OS, the space occupied by the representation of drawing commands with a space occupied by the image to be rendered; anddetermining, by the virtual graphics device, when to send a rendering request to render the image by the server.19. The non-transitory computer readable storage medium of','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Layer combination in a surface composition system','US08629886','Microsoft Corporation','Michail;Liu','A system and method for processing and rendering multiple layers of a two-dimensional scene. A system provides a mechanism to determine a number of scene surfaces and a mapping between scene layers and scene surfaces. The mechanisms may include combining and aggregating areas of layers to create one opaque surface, aggregating non-overlapping semi-transparent opaque areas of layers, or creating surfaces from overlapping semi-transparent surfaces. Moving objects are accommodated, so that layers below a moving object may be rendered properly in frames where the moving object is above the layer and frames where the moving object is not above the layer, for each pixel.','1','1. A computer-based method of rendering a two-dimensional graphic scene, the method comprising:a) receiving a scene graph including a first static layer having a corresponding depth value, a second static layer having a corresponding depth value, and a moving layer, the moving layer having a depth value between respective depth values of the first static layer and the second static layers; each of the first static layer and the second layer containing a corresponding vector element;b) determining a first area of pixels in the first static layer and a second area of pixels in the second static layer, the pixels of the first area having different pixel addresses from the pixels of the second area;c) creating a first surface that includes the first area and the second area, the first surface comprising a depth value corresponding to each pixel of the first area and the second area;d) creating a second surface that includes a moving area of the moving layer; ande) providing the first surface and the second surface to a graphics processing unit for composition of the first surface and the second surface based on the depth values;f) rasterizing at least one vector element of the second static layer prior to providing the first surface and the second surface to the graphics processing unit for composition, and not rasterizing at least one other vector element of the second layer.2. The computer-based method of3. The computer-based method of4. The computer-based method of5. The computer-based method ofa) creating a moving surface derived from the moving layer, each pixel of the moving surface having a corresponding depth value;b) providing the moving surface to the graphics processing unit for composition with the first surface and the second surface;c) storing the first surface and the second surface in a surface cache; andd) for each frame of the two-dimensional graphic scene, providing the first surface and the second surface to the graphics processing unit from the cache.6. The computer-based method of7. The computer-based method of8. The computer-based method of9. The computer-based method of10. A computer-readable storage memory comprising computer program instructions for rendering a two-dimensional graphic scene, the program instructions executable by one or more processors to perform actions including:a) receiving at least one moving layer having a corresponding depth value;b) receiving a plurality of static layers, each static layer having a corresponding depth value, each static layer separated from other static layers by one or more of the at least one moving layers, each static layer containing at least one vector element;c) determining a set of topmost opaque pixels, including pixels from at least two of the static layers;d) creating an opaque surface including the set of topmost opaque pixels, the opaque surface having a depth buffer that indicates a z-order for each pixel, including at least two different z-order values;e) creating one or more moving surfaces derived from the at least one moving layer, the moving surface having a depth buffer that indicates a z-order for each pixel; andf) providing the opaque surface and the one or more moving surfaces to a compositor for composition of the opaque surface and the one or more moving surfaces;wherein creating the opaque surface comprises rasterizing at least one vector element from a static layer of the plurality of static layers and determining pixels from the static layer of the plurality of static layers to exclude from rasterization; and wherein rasterizing the at least one vector element is performed prior to providing the opaque surface and the one or more moving surfaces to the compositor.11. The computer-readable storage memory ofa) creating a semi-transparent surface by selectively selecting semi-transparent pixels from the plurality of static layers based on whether each pixel overlaps another semi-transparent pixel with a moving layer in between the pixel and the other semi-transparent pixel.12. The computer-readable storage memory of13. A computer-based system for rendering a two-dimensional graphic scene having a plurality of static layers and at least one moving layer, each of the plurality of static layers containing at least one vector element and separated from other static layers of the plurality of static layers by at least one moving layer, the system comprising:a) a rasterizer configured to perform actions including:i) receiving a scene graph representing the graphic scene;ii) determining at least two areas of respective static layers of the plurality of static layers to be aggregated into one static surface;iii) after determining the at least two areas, rasterizing the at least two areas to create the one static surface, each pixel of the one static surface having a corresponding depth level; andiv) creating at least one moving surface based on the at least one moving layer, each pixel of the at least one moving surface having a corresponding depth level; andb) a processor configured to compose a frame of the graphic scene by composing the one static surface and the moving surface by employing the depth level of each pixel of the at least one moving surface and the depth level of each pixel of the one static surface;wherein the rasterizer actions further include selectively rasterizing each pixel of a vector element of at least one static layer of the plurality of static layers prior to providing the static surface and the moving surface to the processor, based on whether the pixel is obscured by an opaque element of another static layer of the plurality of static layers.14. The computer-based system ofa) determining whether semi-transparent pixels of each static layer overlap semi-transparent pixels of other static layers; andb) determining the at least two areas based on the determination of overlapping semi-transparent pixels.15. The computer-based system ofa) determining whether semi-transparent pixels of each static layer overlap semi-transparent pixels of other static layers; andb) determining the at least two areas to exclude semi-transparent pixels of each static layer that overlap semi-transparent pixels of other static layers.16. The computer-based system ofa) determining whether semi-transparent pixels of each static layer overlap semi-transparent pixels of other static layers; andb) determining the at least two areas to include semi-transparent pixels of each static layer that overlap semi-transparent pixels of other static layers.17. The computer-based system of18. The computer-based system of','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Display apparatus, display method, and moving body','US08629887','Kabushiki Kaisha Toshiba','Sasaki;Okumura;Yoneyama;Kuwabara;Kawawada;Nagahara;Odate;Morimoto','According to an embodiment, a display apparatus includes an information acquisition unit, a control unit, and a display unit. The information acquisition unit is configured to acquire information relating to an energy efficiency of a moving body. The control unit is configured to generate image data to include a display object indicating the energy efficiency based on the information. The display unit is configured to present the image including the display object to a human viewer operating the moving body. The control unit is configured to generate the image data to include a first display object when the energy efficiency is in a first state and generate the image data to include a second display object when the energy efficiency is in a second state lower than the first state.','1','1. A display apparatus, comprising:an information acquisition unit configured to acquire information relating to an energy efficiency of a moving body;a control unit configured to generate image data to include a display object indicating the energy efficiency based on the information acquired by the information acquisition unit; anda display unit configured to present the image including the display object generated by the control unit to a human viewer operating the moving body,the control unit being configured to:generate the image data to include a first display object having a first configuration and moving along a first path inside the image when the energy efficiency is in a first state; andgenerate the image data to include a second display object having a second configuration and moving along a second path inside the image when the energy efficiency is in a second state lower than the first state,the control unit being configured to implement at least one selected from:causing a temporal change of the second path to be greater than a temporal change of the first path;causing recesses and protrusions of the second path to be greater than recesses and protrusions of the first path; andcausing recesses and protrusions of the second configuration to be greater than recesses and protrusions of the first configuration.2. The apparatus according to3. The apparatus according to4. The apparatus according to5. The apparatus according to6. The apparatus according to7. The apparatus according to8. The apparatus according to9. The apparatus according to10. The apparatus according to11. The apparatus according to12. The apparatus according to13. The apparatus according to14. The apparatus according to15. The apparatus according to16. The apparatus according tothe display unit is configured to project a light flux including the image to the human viewer, andthe display unit includes a divergence angle control unit configured to control a divergence angle of the light flux to project the light flux to only one eye of the human viewer.17. The apparatus according to18. The apparatus according to19. A display method including acquiring information relating to an energy efficiency of a moving body, generating image data to include a display object indicating the energy efficiency based on the acquired information, and presenting an image including the display object based on the generated image data to a human viewer operating the moving body, the method comprising:using a control unit to generate a first image from the image data to include a first display object having a first configuration and moving along a first path inside the image when the energy efficiency is in a first state;using a control unit to generate a second image from the image data to include a second display object having a second configuration and moving along a second path inside the image when the energy efficiency is in a second state lower than the first state; andimplementing at least one selected fromcausing a temporal change of the second path to be greater than a temporal change of the first path,causing recesses and protrusions of the second path to be greater than recesses and protrusions of the first path, andcausing recesses and protrusions of the second configuration to be greater than recesses and protrusions of the first configuration.20. A moving body, comprising:a display apparatus; anda reflecting unit configured to reflect a light flux emitted from the display apparatus toward a human viewer,the display apparatus including:an information acquisition unit configured to acquire information relating to an energy efficiency of the moving body;a control unit configured to generate image data to include a display object indicating the energy efficiency based on the information acquired by the information acquisition unit; anda display unit configured to present the image including the display object generated by the control unit to the human viewer operating the moving body,the control unit being configured to:generate the image data to include a first display object having a first configuration and moving along a first path inside the image when the energy efficiency is in a first state; andgenerate the image data to include a second display object having a second configuration and moving along a second path inside the image when the energy efficiency is in a second state lower than the first state,the control unit being configured to implement at least one selected from:causing a temporal change of the second path to be greater than a temporal change of the first path;causing recesses and protrusions of the second path to be greater than recesses and protrusions of the first path; andcausing recesses and protrusions of the second configuration to be greater than recesses and protrusions of the first configuration.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Image processing apparatus and control method thereof','US08630499','Canon Kabushiki Kaisha','Shikata;Kikkawa;Shibamiya;Urabe;Takayanagi;Masuda','When an image processing apparatus capable of connecting to a digital camera is to perform image-correction processing on irreversible-compression encoded image data acquired from the digital camera, it is determined whether the image-correction processing can be executed by the connected digital camera. When the processing can be executed, it is confirmed whether or not RAW data that corresponds to the irreversible-compression encoded image data is present in the digital camera. If the corresponding RAW data is present in the digital camera, the digital camera is requested to execute the image-correction processing based on the RAW data. This makes it possible to suppress degradation in the image quality more than when directly correcting an irreversible-compression encoded image.','1','1. An image processing apparatus, comprising:an interface that communicates with a camera;an acquisition unit that acquires compressed image data which is obtained by compressing image data photographed by the camera from the camera via the interface;a decompressing unit that decompresses the compressed image data acquired by the acquisition unit;an image correction unit for performing image-correction processing according to a parameter set by a user, on the image data decompressed by the decompressing unit; anda control unit that controls the acquisition unit and the image correction unit in accordance with a compression rate of the compressed image data acquired by the acquisition unit,wherein the control unit controls the acquisition unit to further acquire compressed corrected image data on which the image-correction processing has been performed by the camera from the camera in the case where:(a) the compression rate of the compressed image data acquired by the acquisition unit is larger than a threshold value and(b) corresponding image data, which is not compressed and corresponds to the compressed image data of which the compression rate is larger than the threshold value, exists in the camera, andwherein the control unit further controls the image correction unit to perform the image-correction processing on the decompressed image data decompressed by the decompressing unit in the case where the compression rate of the compressed image data acquired by the acquisition unit is smaller than the threshold value.2. The image processing apparatus according towherein the acquisition unit further acquires attribute information of the compressed image data; andthe control unit detects the compression rate of the compressed image data based on the attribute information.3. A control method for an image processing apparatus provided with an image correction unit for performing image-correction processing according to a parameter set by a user on decompressed image data and an interface for communicating with a camera, comprising the steps of:acquiring compressed image data which is obtained by compressing image data photographed by the camera from the camera via the interface;decompressing the compressed image data acquired in the acquiring step; andfurther acquiring compressed corrected image data on which the image-correction processing has been performed by the camera from the camera, in the case where:(a) the compression rate of the compressed image data acquired in the acquisition step is larger than a threshold value, and(b) corresponding image data, which is not compressed and corresponds to the compressed image data of which the compression rate is larger than the threshold value, exists in the camera; andcontrolling the image correction unit to perform the image-correction processing on the decompressed image data decompressed in the decompressing step in the case where the compression rate of the compressed image data acquired by the acquisition unit is smaller than the threshold value.4. An image processing apparatus comprising:an acquisition unit that acquires compressed image data obtained by compressing first image data;a decompressing unit that decompresses the compressed image data acquired by the acquisition unit;an image processing unit for performing image processing on the image data decompressed by the decompressing unit according to a set parameter; anda control unit that executes control in accordance with a compression rate of the compressed image data acquired by the acquisition unit,wherein if the compression rate of the compressed image data acquired by the acquisition unit is larger than a threshold value, the control unit controls the acquisition unit to acquire, from an external apparatus, compressed processed image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter.5. The image processing apparatus according toa communication unit that communicates with the external apparatus,wherein if the compression rate of the compressed image data acquired by the acquisition unit is larger than a threshold value, the control unit controls the communication unit to request the external apparatus for a transmission of the image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter.6. The image processing apparatus according to7. The image processing apparatus according to8. The image processing apparatus according toa display control unit that outputs at least one of the image data output from the image processing unit and the image data, which is acquired from the external apparatus and on which the image processing according to the set parameter has been performed by the external apparatus.9. The image processing apparatus according to10. An image processing apparatus comprising:an acquisition unit that acquires compressed image data obtained by compressing first image data;a decompressing unit that decompresses the compressed image data acquired by the acquisition unit;an image processing unit for performing image processing on the image data decompressed by the decompressing unit according to the a set parameter;a control unit; anda mode setting unit that sets one of a plurality of modes according to a compression rate of the compressed image data acquired by the acquisition unit,wherein the plurality of modes includes:(a) a first mode that is set when the compression rate of the compressed image data acquired by the acquisition unit is larger than a threshold value, and in which the control unit controls the acquisition unit to acquire, from an external apparatus, compressed processed image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter, and(b) a second mode that is set when the compression rate of the compressed image data acquired by the acquisition unit is smaller than the threshold value and in which the control unit controls the image processing unit to perform the image processing on the decompressed image data, decompressed by the decompressing unit, according to the set parameter.11. A control method for an image processing apparatus provided with an image processing unit for performing image processing on the image data decompressed according to a set parameter comprising the steps of:acquiring compressed image data obtained by compressing first image data;decompressing the compressed image data acquired in the acquisition step; andin a case where the compression rate of the compressed image data acquired in the acquiring step is larger than a threshold value, acquiring, from an external apparatus, compressed processed image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter.12. The control method according tocommunicating with the external apparatus, andin the case where the compression rate of the compressed image data acquired in the acquiring step is larger than a threshold value, the communicating step requests the external apparatus for a transmission of the image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter.13. The control method according to14. The control method according to15. The control method according toa display step of outputting at least one of the image data output from the image processing unit and the image data which is acquired from the external apparatus and on which the image processing according to the set parameter has been performed by the external apparatus.16. The control method according to17. A control method for an image processing apparatus provided with an image processing unit for performing image processing on the image data decompressed according to a set parameter comprising the steps of:acquiring compressed image data obtained by compressing first image data;decompressing the compressed image data acquired in the acquiring step; andmode setting that sets one of a plurality of modes according to a compression rate of the compressed image data acquired in the acquiring step,wherein the plurality of modes includes:(a) a first mode that is set when the compression rate of the compressed image data acquired in the acquiring step is larger than a threshold value and in which case the acquiring step acquires, from an external apparatus, compressed processed image data obtained by the external apparatus by performing the image processing on the first image data according to the set parameter, and(b) a second mode that is set when the compression rate of the compressed image data acquired in the acquiring step unit is smaller than the threshold value and in which the image processing unit performs the image processing on the image data decompressed in the decompressing step according to the set parameter.','20140114','G06T');
INSERT INTO USPTO (title, patNO, assignee, inventors, abstract, description, claim ,date, kind) VALUES ('Cut-line steering methods for forming a mosaic image of a geographical area','US08630510','Pictometry International Corporation','Giuffrida;Schultz;Gray;Bradacs','A method for automatically steering mosaic cut lines along preferred routes to form an output mosaic image includes creating an assignment map corresponding to the output mosaic image where each pixel has an initial designation of unassigned; marking each pixel of the assignment map that intersects the preferred routes as being a Preferred Cut Line pixel to divide the Assignment Map into one or more regions; searching for each region to locate one or more source images that cover that region; and using a Selection Heuristic or Pairing Heuristic to determine quality of coverage. The Preferred Cut Line pixels are redesignated to match the image assignments of their bounded regions, and the output mosaic image is formed by contributing pixel values from the source images based upon the designations set forth in the assignment map.','1','1. A method for automatically steering mosaic cut lines along preferred routes to form an output mosaic image, comprising the steps of:creating an assignment map corresponding to the output mosaic image where each pixel has an initial designation of unassigned;marking pixels of the assignment map that intersect the preferred routes as being Preferred Cut Line pixels to divide the Assignment Map into one or more regions that are bounded by Preferred Cut Line pixels or the edge of the Assignment Map;for each region, searching for one or more source images that completely cover that region, and responsive to multiple source images completely covering the region, using a Selection Heuristic to determine quality of coverage, and then designating each pixel in that region as being assigned to the source image determined to be best by the Selection Heuristic;for any remaining unassigned regions, searching for two or more source images whose combined footprint completely covers the region, and for each set of two or more combined images, using a Pairing Heuristic to determine quality of coverage, and then designating each pixel in the region as being assigned to the two or more combined images determined best by the Pairing Heuristic;redesignating the Preferred Cut Line pixels to match the image assignments of their bounded regions; andforming the output mosaic image by contributing pixel values from the source images based upon the designations set forth in the Assignment Map.2. The method of3. The method of4. The method of5. The method of6. The method of7. The method of8. The method of9. The method of10. The method of11. The method ofselecting a plurality of geo-referenced, digital source images captured from different vantage points and cooperating to cover the geographic area, the source images having overlapping portions depicting a portion of the geographic area;creating the ground confidence map of the geographic area, the ground confidence map having a plurality of pixels with each pixel corresponding to a particular geographic location;assigning the pixels in the ground confidence map with pixel values indicative of composite ground confidence scores by:calculating composite ground confidence scores for the pixel values of common geographic regions within the overlapping portions of the source images within a kernel corresponding to the particular geographic location of the pixels; andstoring pixel values indicative of the composite ground confidence score calculated for particular pixels, the pixel values indicative of a statistical probability that the geographical location represented by the particular pixels represent the ground.12. The method of13. One or more non-transitory computer readable medium storing logic that when executed on one or more computers performs the functions of:creating an assignment map corresponding to the output mosaic image where each pixel has an initial designation of unassigned;marking pixels of the assignment map that intersect the preferred routes as being Preferred Cut Line pixels to divide the Assignment Map into one or more regions that are bounded by Preferred Cut Line pixels or the edge of the Assignment Map;for each region, searching for one or more source images that completely cover that region, and responsive to multiple source images completely covering the region, using a Selection Heuristic to determine quality of coverage, and then designating each pixel in that region as being assigned to the source image determined to be best by the Selection Heuristic;for any remaining unassigned regions, searching for two or more source images whose combined footprint completely covers the region, and for each set of two or more combined images, using a Pairing Heuristic to determine quality of coverage, and then designating each pixel in the region as being assigned to the two or more combined images determined best by the Pairing Heuristic;redesignating the Preferred Cut Line pixels to match the image assignments of their bounded regions; andforming the output mosaic image by contributing pixel values from the source images based upon the designations set forth in the Assignment Map.14. The one or more non-transitory computer readable medium of15. The one or more non-transitory computer readable medium of16. The one or more non-transitory computer readable medium of17. The one or more non-transitory computer readable medium of18. The one or more non-transitory computer readable medium of19. The one or more non-transitory computer readable medium of20. The one or more non-transitory computer readable medium of21. The one or more non-transitory computer readable medium of22. The one or more non-transitory computer readable medium of23. The one or more non-transitory computer readable medium ofselecting a plurality of geo-referenced, digital source images captured from different vantage points and cooperating to cover the geographic area, the source images having overlapping portions depicting a portion of the geographic area;creating the ground confidence map of the geographic area, the ground confidence map having a plurality of pixels with each pixel corresponding to a particular geographic location;assigning the pixels in the ground confidence map with pixel values indicative of composite ground confidence scores by:calculating composite ground confidence scores for the pixel values of common geographic regions within the overlapping portions of the source images within a kernel corresponding to the particular geographic location of the pixels; andstoring pixel values indicative of the composite ground confidence score calculated for particular pixels, the pixel values indicative of a statistical probability that the geographical location represented by the particular pixels represent the ground.24. The one or more non-transitory computer readable medium of','20140114','G06T');
use CHIUAN;
use CHIUAN;
use CHIUAN;
use CHIUAN;
use CHIUAN;
use CHIUAN;
use CHIUAN;
