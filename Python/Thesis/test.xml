<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624888-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624888</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11801924</doc-number>
<date>20070511</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>867</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>17</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>17</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<combination-set>
<group-number>1</group-number>
<combination-rank>
<rank-number>1</rank-number>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</combination-rank>
<combination-rank>
<rank-number>2</rank-number>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>17</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</combination-rank>
<combination-rank>
<rank-number>3</rank-number>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</combination-rank>
<combination-rank>
<rank-number>4</rank-number>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>15</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</combination-rank>
</combination-set>
</further-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
<further-classification>345420</further-classification>
<further-classification>345421</further-classification>
<further-classification>345422</further-classification>
<further-classification>345423</further-classification>
<further-classification>345424</further-classification>
<further-classification>345426</further-classification>
<further-classification>345427</further-classification>
<further-classification>345581</further-classification>
<further-classification>345582</further-classification>
<further-classification>345629</further-classification>
<further-classification>345473</further-classification>
<further-classification>345474</further-classification>
</classification-national>
<invention-title id="d2e53">Screen space optimization techniques for use in a hair/fur pipeline</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4519037</doc-number>
<kind>A</kind>
<name>Brodeur et al.</name>
<date>19850500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4731743</doc-number>
<kind>A</kind>
<name>Blancato</name>
<date>19880300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4872056</doc-number>
<kind>A</kind>
<name>Hicks et al.</name>
<date>19891000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5319705</doc-number>
<kind>A</kind>
<name>Halter et al.</name>
<date>19940600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5404426</doc-number>
<kind>A</kind>
<name>Usami et al.</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5437600</doc-number>
<kind>A</kind>
<name>Liboff et al.</name>
<date>19950800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5758046</doc-number>
<kind>A</kind>
<name>Rouet et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5764233</doc-number>
<kind>A</kind>
<name>Brinsmead et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5777619</doc-number>
<kind>A</kind>
<name>Brinsmead</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6097396</doc-number>
<kind>A</kind>
<name>Rouet et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6300960</doc-number>
<kind>B1</kind>
<name>DeRose et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6333985</doc-number>
<kind>B1</kind>
<name>Ueda et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6348924</doc-number>
<kind>B1</kind>
<name>Brinsmead</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6559849</doc-number>
<kind>B1</kind>
<name>Anderson et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6720962</doc-number>
<kind>B1</kind>
<name>Alter</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345420</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6952218</doc-number>
<kind>B1</kind>
<name>Bruderlin</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7050062</doc-number>
<kind>B2</kind>
<name>Bruderlin</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7348973</doc-number>
<kind>B1</kind>
<name>Gibbs et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7609261</doc-number>
<kind>B2</kind>
<name>Gibbs et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2002/0021302</doc-number>
<kind>A1</kind>
<name>Lengyel</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345583</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2004/0227757</doc-number>
<kind>A1</kind>
<name>Petrovic et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2005/0253842</doc-number>
<kind>A1</kind>
<name>Petrovic et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>JP</country>
<doc-number>09-326042</doc-number>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>JP</country>
<doc-number>2001-297331</doc-number>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>WO</country>
<doc-number>01/11562</doc-number>
<kind>A2</kind>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>WO</country>
<doc-number>WO 01/11562</doc-number>
<kind>A2</kind>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>K. Ward, M. C. Lin, J. Lee, S. Fisher and D. Macri, &#x201c;Modeling Hair Using Level-of-Detail Representations&#x201d;, Proc. of the 16th International Conf. on Computer Animation and Social Agents, CASA '03.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Shave and a HairCut Version 4.5 manual, Feb. 2007, pp. 1-56.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>F. Bertails, T. Y. Kim, M. P. Cani and U. Neumann, &#x201c;Adaptive Wisp Tree&#x2014;a multiresolution control structure for simulating dynamic clustering in hair motion&#x201d;, Eurographics, SIGGRAPH Symposium on Computer Animation, 2003.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Office Action from U.S. Appl. No. 11/345,355 mailed Apr. 17, 2008, 18 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Office Action from U.S. Appl. No. 11/801,829, mailed Apr. 17, 2008, 13 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Magnenat-Thalmann, Nadia , et al., &#x201c;Virtual Clothes, Hair and Skin for Beautiful Models&#x201d;, <i>Proc. Computer Graphics International</i>, (Jun. 24, 1996), pp. 132-141.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Ward, Kelly , et al., &#x201c;A Survey on Hair Modeling: Styling, Simulation, and Rendering&#x201d;, <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 13, No. 2, (Mar./Apr. 2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Office Action for U.S. Appl. No. 11/880,588, 1st Names Inventor Armin Walter Bruderlin, Mailed Aug. 20, 2008, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Office Action for U.S. Appl. No. 11/880,594, filed Jul. 23, 2007, 1st Named Inventor: Armin Walter Bruderlin, Mailed Aug. 20, 2008, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Office Action for U.S. Appl. No. 11/801,913, filed May 11, 2007, 1st Named Inventor: Armin Walter Bruderlin, Mailed Aug. 19, 2008, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Office Action for U.S. Appl. No. 11/801,923, filed May 11, 2007, 1st Named Inventor: Armin Walter Bruderlin, mailed Aug. 27, 2008, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>&#x201c;Alias Systems Hair and Dynamic Curves&#x201d;, <i>Maya Version 6</i>, Toronto, Canada, (2004), pp. 1-100.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Alter, Joe , &#x201c;Shave and a Haircut, Real Hair, Real Fast&#x201d;, www.joealter.com/docs/shavedoc.html., (2000), pp. 1-37.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>Iones, A. , et al., &#x201c;Fur and Hair: Practical Modeling and Rendering Techniques&#x201d;, <i>Proc. IEEE International Conference on Information Visualization</i>, (Jul. 2000), pp. 145-151.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>United States Office Action U.S. Appl. No. 11/801,914 1st Named Inventor: Armin Walter Bruderlin, Mailed Sep. 18, 2008, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Office Action U.S. Appl. No. 11/345,355, 1st Named Inventor Armin Walter Bruderlin, Mailed Oct. 1, 2008, 22 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>U.S. Appl. No. 11/801,829 Office Action, mailed Nov. 6, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Bertails, F. , et al., &#x201c;Adaptive Wisp Tree&#x2014;a multiresolution control structure for simulating dynamic clustering in hair motion&#x201d;, <i>Eurographics, SIGGRAPH Symposium on Computer Animation</i>, (2000).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>United States Office Action U.S. Appl. No. 11/880,588, mailed Jan. 8, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>United States Office Action U.S. Appl. No. 11/880,594 mailed Jan. 26, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>U.S. Appl. No. 11/801,923 Final Office Action mailed Feb. 24, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>U.S. Appl. No. 11/345,355 Office Action mailed Feb. 24, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>U.S. Appl. No. 11/801,914 Office Action mailed Mar. 11, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>U.S. Appl. No. 11/801,913 Office Action mailed Mar. 19, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>U.S. Appl. No. 11/801,829 Office Action, mailed May 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>U.S. Appl. No. 11/801,913, Office Action mailed Jun. 30, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>U.S. Appl. No. 11/880,594, Office Action mailed Jun. 26, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>U.S. Appl. No. 11/880,588 Office Action mailed Jun. 4, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>U.S. Appl. No. 11/345,355 Final Office Action mailed Jul. 8, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00056">
<othercit>U.S. Appl. No. 11/801,913, mailed Jul. 31, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00057">
<othercit>Ward, Kelly , et al., &#x201c;Adaptive Grouping and Subdivision for Simulating Hair Dynamics&#x201d;, <i>Proc. Pacific Graphics Conf. Computer Graphics and Applications</i>,, (Oct. 2003), pp. 234-243.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>U.S. Appl. No. 11/801,914 Office Action mailed Oct. 29, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>U.S. Appl. No. 11/345,355 Non-Final Office Action mailed Jan. 11, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>U.S. Appl. No. 11/801,829, Final Office Action mailed Jan. 7, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>U.S. Appl. No. 11/880,588, Final Rejection mailed Feb. 5, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>U.S. Appl. No. 11/880,594, Final Rejection Action mailed Feb. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>U.S. Appl. No. 11/801,923 Final Rejection Action mailed Feb. 16, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>U.S. Appl. No. 11/801,925 Office Action mailed Mar. 19, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>U.S. Appl. No. 11/801,913 Office Action, mailed Mar. 30, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>Official Decision of Grant for Registration issued in related application JP Application No. 2009-521798, May 29, 2012.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>U.S. Appl. No. 11/801,914, Final Rejection mailed Jun. 25, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>U.S. Appl. No. 11/345,355, Final Office Action, mailed Jul. 7, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>EPO Extended Search Report, Application No. 12189025.5 dated Jan. 2, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>Ando, Makoto et al., &#x201c;Expression and Motion Control of Hair Using Fast Collision Detection Methods&#x201d;, <i>Faculty of Engineering</i>, Seikei University, Tokyo, Japan, p. 463-470, Dec. 1995.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>Anjyo, Ken-Ichi et al., &#x201c;A Simple Method for Extracting the Natural Beauty of Hair&#x201d;, <i>Computer Graphics</i>, vol. 26, No. 2, (Jul. 1992),pp. 111-120.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00072">
<othercit>Bruderlin, Armin W., &#x201c;A Basic Hair/Fur Pipeline&#x201d;, <i>Paper in course # 9: Photorealistic Hair Mode Animation, and Rendering. ACM SIGGRAPH Course</i>, (2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00073">
<othercit>Bruderlin, Armin W., &#x201c;A Method to Generate Wet and Broken-Up Animal Fur&#x201d;, <i>Computer Graphics and Applications 1999 Proc., 7th Pac. Conf</i>. Oct. 5-7, 1999,pp. 242-249.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00074">
<othercit>Bruderlin, Armin W., &#x201c;Production Hair/Fur Pipeline at Imageworks&#x201d;, <i>Presentation in Course # 9: Photorealistic Hair Modeling, Anmiation and Rendering. ACM SIGGRAPH Course</i>, (2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00075">
<othercit>Daldegan, Agnes et al., &#x201c;An Integrated System for Modeling, Animating, and Rendering Hair&#x201d;, <i>Eurographics 1993 </i>vol. 12, No. 3, (1993),pp. 211-221.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00076">
<othercit>Dave, J. et al., &#x201c;The Chronicles of Narnia: The Lioin, The Crowds and Rhythm and Hues&#x201d;, <i>ACM SIGGRAPH Course # 34</i>, (2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00077">
<othercit>Goldman, Dan B., &#x201c;Fake Fur Rendering&#x201d;, <i>Computer Graphics Proceedings, Annual Conference Series</i>, Aug. 3-8, 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00078">
<othercit>Kajiya, James T., et al., &#x201c;Rendering Fur with Three Dimensional Textures&#x201d;, <i>Computer Graphics</i>, vol. 23, No. 3, (Jul. 1989),pp. 271-281.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00079">
<othercit>Kong, Waiming et al., &#x201c;Visible Volume Buffer for Efficient Hair Expression and Shadow Generation&#x201d;, <i>IEEE</i>, (1999),p. 58-64.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00080">
<othercit>Magnenat-Thalmann, Nadia &#x201c;Photorealistic Hair Modeling, Animation, and Rendering&#x201d;, <i>SIGGRAPH 2004 Course # 9</i>, MIRALab&#x2014;University of Geneva,(2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00081">
<othercit>Magnenat-Thalmann, Nadia &#x201c;The Simulation of Ancient Hairstyle in Real-Time&#x201d;, <i>ACM SIGGRAPH 2004</i>.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00082">
<othercit>Marschner, Stephen R., et al., &#x201c;Light Scattering from Human Hair Fibers&#x201d;, <i>ACM Transactions on Graphics 2003 </i>Cornell University, (2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00083">
<othercit>Miller, Gavin S., &#x201c;From Wire-Frames to Furry Animals&#x201d;, <i>Graphics Interface</i>, (1988),pp. 138-145.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00084">
<othercit>Moon, J. T., et al., &#x201c;Simulating Multiple Scattering in Hair Using a Photon Mapping Approach&#x201d;, <i>ACM Transactions on Graphics</i>, 25, 3, 1067-1074, (2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00085">
<othercit>Paris, S. et al., &#x201c;Capture of Hair Geometry from Multiple Images&#x201d;, <i>ACM Transactions on Graphics</i>, 23, 3,, (2004),712-719.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00086">
<othercit>Preston, Martin et al., &#x201c;Grooming, Animating, &#x26; Rendering Fur for &#x201c;King Kong&#x201d;&#x201d;, <i>ACM SIGGRAPH Sketch</i>, (2006).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00087">
<othercit>Rankin, John et al., &#x201c;A Simple Naturalistic Hair Model&#x201d;, <i>Computer Graphics</i>, (Feb. 1996),p. 5-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00088">
<othercit>Van Gelder, Allen et al., &#x201c;An Interactive Fur Modeling Technique&#x201d; University of California, Santa Cruz, p. 1-6, May 1997.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00089">
<othercit>Watanabe, Yasuhiko et al., &#x201c;A Triagonal Prism-Based Method for Hair Image Generation&#x201d;, <i>IEEE</i>, (Jan. 1992),p. 47-53.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00090">
<othercit>Office Action for United States Application U.S. Appl. No. 11/880,588, 1st Named Inventor Armin Walter Bruderlin, Mailed Aug. 20, 2008, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00091">
<othercit>Office Action for United States Application U.S. Appl. No. 11/880,594, Filed Jul. 23, 2007, 1st Named Inventor: Armin Walter Bruderlin, Mailed Aug. 20, 2008, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>Office Action for United States Application U.S. Appl. No. 11/801,913 filed May 11, 2007, 1st Named Inventor: Armin Walter Bruderlin, Mailed Aug. 19, 2008, 11 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>Office Action for United States Patent Application U.S. Appl. No. 11/801,923, filed May 11, 2007, 1st Named Inventor: Armin Walter Bruderlin mailed Aug. 27, 2008 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00094">
<othercit>Alter, Joe , &#x201c;Shave and a Haircut, Real Hair, Real Fast&#x201d;, wwwinealter.com/docs/shavedoc.html,, (2000), pp. 1-37.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00095">
<othercit>Alia Systems, &#x201c;Hair and Dynamic Curves&#x201d;, Version 6, 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00096">
<othercit>U.S. Appl. No. 11/345,355, Non Final Office Action, mailed Jun. 12, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00097">
<othercit>U.S. Appl. No. 11/801,829, Non Final Office Action mailed May 6, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00098">
<othercit>Japanese Patent Application No. 2012-143262, Official Action dated Apr. 22, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345419-420</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345582-583</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345473-474</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-cpc-text>G06T 19/00</classification-cpc-text>
<classification-cpc-text>G06T 17/20</classification-cpc-text>
<classification-cpc-text>G06T 15/20</classification-cpc-text>
<classification-cpc-text>G06T 15/40</classification-cpc-text>
<classification-cpc-combination-text>G06T 19/00</classification-cpc-combination-text>
<classification-cpc-combination-text>G06T 17/20</classification-cpc-combination-text>
<classification-cpc-combination-text>G06T 15/20</classification-cpc-combination-text>
<classification-cpc-combination-text>G06T 15/40</classification-cpc-combination-text>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>45</number-of-drawing-sheets>
<number-of-figures>72</number-of-figures>
</figures>
<us-related-documents>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11345355</doc-number>
<date>20060201</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11801924</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10052068</doc-number>
<date>20020116</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7050062</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11345355</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09370104</doc-number>
<date>19990806</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6952218</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10052068</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60833133</doc-number>
<date>20060724</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60886286</doc-number>
<date>20070123</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070216701</doc-number>
<kind>A1</kind>
<date>20070920</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bruderlin</last-name>
<first-name>Armin Walter</first-name>
<address>
<city>Culver City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chardavoine</last-name>
<first-name>Francois</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chua</last-name>
<first-name>Clint</first-name>
<address>
<city>Alhambra</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Melich</last-name>
<first-name>Gustav</first-name>
<address>
<city>Playa del Rey</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bruderlin</last-name>
<first-name>Armin Walter</first-name>
<address>
<city>Culver City</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Chardavoine</last-name>
<first-name>Francois</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Chua</last-name>
<first-name>Clint</first-name>
<address>
<city>Alhambra</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Melich</last-name>
<first-name>Gustav</first-name>
<address>
<city>Playa del Rey</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Blakely, Sokoloff, Taylor &#x26; Zafman LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Sony Electronics Inc.</orgname>
<role>02</role>
<address>
<city>Park Ridge</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Wang</last-name>
<first-name>Jin-Cheng</first-name>
<department>2677</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A surface definition module of a hair/fur pipeline may be used to define a surface and an optimization module may be used to determine whether a hair is to be rendered upon the surface. In particular, the optimization module may be used to: determine a size metric for the hair; apply a first density curve to the size metric determined for the hair to generate a density multiplier value; and based upon the density multiplier value, determine whether the hair should be rendered.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="149.94mm" wi="229.28mm" file="US08624888-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="114.89mm" wi="108.20mm" file="US08624888-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="220.13mm" wi="72.47mm" orientation="landscape" file="US08624888-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="196.93mm" wi="154.86mm" file="US08624888-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="111.68mm" wi="123.53mm" file="US08624888-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="155.70mm" wi="76.12mm" file="US08624888-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="214.12mm" wi="128.27mm" file="US08624888-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="197.78mm" wi="146.64mm" file="US08624888-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="162.14mm" wi="98.21mm" file="US08624888-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="89.32mm" wi="105.07mm" file="US08624888-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="191.09mm" wi="104.22mm" file="US08624888-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="209.97mm" wi="141.56mm" file="US08624888-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="213.87mm" wi="134.37mm" file="US08624888-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="93.90mm" wi="128.86mm" file="US08624888-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="204.72mm" wi="107.78mm" file="US08624888-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="205.40mm" wi="109.90mm" file="US08624888-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="236.39mm" wi="128.52mm" file="US08624888-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="136.57mm" wi="88.22mm" file="US08624888-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="91.52mm" wi="131.91mm" file="US08624888-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="202.78mm" wi="114.72mm" file="US08624888-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="210.65mm" wi="124.54mm" file="US08624888-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="224.96mm" wi="89.83mm" file="US08624888-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="142.66mm" wi="100.84mm" file="US08624888-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="240.79mm" wi="119.46mm" orientation="landscape" file="US08624888-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="233.60mm" wi="129.71mm" file="US08624888-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="227.08mm" wi="163.32mm" file="US08624888-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="163.15mm" wi="154.86mm" file="US08624888-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="218.52mm" wi="133.43mm" file="US08624888-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="159.34mm" wi="138.01mm" file="US08624888-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="237.66mm" wi="155.53mm" file="US08624888-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="209.63mm" wi="140.72mm" file="US08624888-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="194.23mm" wi="121.07mm" file="US08624888-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="108.97mm" wi="154.18mm" file="US08624888-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="215.48mm" wi="143.59mm" orientation="landscape" file="US08624888-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="217.09mm" wi="156.72mm" file="US08624888-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="160.44mm" wi="159.77mm" file="US08624888-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="218.52mm" wi="166.71mm" file="US08624888-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="113.45mm" wi="164.00mm" file="US08624888-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="238.51mm" wi="144.02mm" orientation="landscape" file="US08624888-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="229.02mm" wi="164.08mm" orientation="landscape" file="US08624888-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="207.35mm" wi="156.46mm" file="US08624888-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="225.55mm" wi="148.25mm" file="US08624888-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="210.23mm" wi="158.83mm" file="US08624888-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="191.60mm" wi="121.58mm" file="US08624888-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00044" num="00044">
<img id="EMI-D00044" he="214.88mm" wi="147.83mm" file="US08624888-20140107-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00045" num="00045">
<img id="EMI-D00045" he="205.99mm" wi="157.73mm" file="US08624888-20140107-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation-in-part of application Ser. No. 11/345,355 filed on Feb. 1, 2006, which is a continuation of application Ser. No. 10/052,068 filed on Jan. 16, 2002, now issued as U.S. Pat. No. 7,050,062 which is a continuation of application Ser. No. 09/370,104 filed on Aug. 6, 1999, now issued as U.S. Pat. No. 6,952,218. This application also claims priority to provisional patent application 60/833,113 filed Jul. 24, 2006 and provisional patent application 60/886,286 filed Jan. 23, 2007.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The invention relates to the digital creation of fur. More particularly, the present invention relates to the digital creation of realistic close-up and distant looks of fur coats on animal models.</p>
<p id="p-0005" num="0004">2. Related Art</p>
<p id="p-0006" num="0005">One of the many challenges in modeling, animating and rendering believable mammals in computer graphics has been to produce realistic-looking fur. A real fur coat is made up of hundreds of thousands of individual, cylindrical hairs covering the skin, and fulfills vital functions such as protection against cold and predators. Between animals as well as across the body of individual animals, the look and structure of these hairs vary greatly with respect to length, thickness, shape, color, orientation and under/overcoat composition. In addition, fur is not static, but moves and breaks up as a result of the motion of the underlying skin and muscles, and also due to external influences, such as wind and water.</p>
<p id="p-0007" num="0006">Some prior computer graphics techniques used for fur creation have achieved convincing looks of smooth fur; however, these techniques do not take into account that real fur often breaks up at certain areas of the body, such as around the neck. In addition, the prior methods do not account for hairs of wet fur clump together resulting in a significantly different appearance compared to dry fur. Also, the process of simulating hair as it is getting increasingly wet when sprinkled on by water has not yet been addressed.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0008" num="0007">The system and method of the present invention provides a flexible technique for the digital representation and generation of realistic fur coats on geometric models of surfaces, such as animals. In one embodiment, an innovative technique for placement, adjustment and combing of fur on surfaces is provided. In one embodiment, the continuity of fur across surface patch boundaries is maintained. In addition, in one embodiment, an innovative method to simulate wet fur is provided. In this method static clumping and animated clumping may be applied to regions on the surfaces. In one embodiment, a method for the symmetric and one-sided breaking of hairs along fur-tracks on surfaces is provided. The above processes can be iteratively applied in order to generate layers of fur, such as an undercoat and an overcoat.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008">The objects, features and advantages of the present invention will be apparent from the following detailed description in which:</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. 1</figref><i>a </i>and <b>1</b><i>b </i>are simplified block diagrams of embodiments of systems that operate in accordance with the teachings of the present invention.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram of one embodiment of a process for the generation of fur in accordance with the teachings of the present invention.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 3</figref><i>a </i>is an illustration of a set of parametric surfaces defining the skin of a three-dimensional animal model.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref><i>b </i>is a simplified flow diagram illustrating one embodiment of static and animated combing processes.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. 3</figref><i>c </i>and <b>3</b><i>d </i>are examples that illustrate one embodiment of the combing processes.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart illustrating one embodiment of a process for adjusting control hairs for removing visual discontinuities at surface boundaries.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a flow chart illustrating one embodiment of a process for placing hairs.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> illustrates one example of subpatches defined on the surface.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref><i>a </i>illustrates an example of control vertices of one control hair.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 7</figref><i>b </i>illustrates an example for calculating control hair weights.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref><i>c </i>illustrates an example an interpolation process used to calculate orientations of final hairs in accordance with the teachings of one embodiment of the present invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref><i>d </i>is a simplified flow diagram of one embodiment for the calculation of the orientation of final hairs.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> is a flow chart illustrating one embodiment of a process to perform static clumping.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> illustrates examples of different clump-percent and clump-rate values.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref><i>a </i>shows a rendered frame of a combed fur coat and <figref idref="DRAWINGS">FIGS. 10</figref><i>b</i>, <b>10</b><i>c </i>and <b>10</b><i>d </i>show snapshots of one embodiment of an animated dry-to-wet fur sequence.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 11</figref> is a flow chart illustrating one embodiment of a process for animated area clumping.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 12</figref><i>a </i>is a flow chart illustrating one embodiment of a process for hair breaking.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 12</figref><i>b </i>illustrates examples of symmetric and one-sided breaking of hairs.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. 12</figref><i>c</i>, <b>12</b><i>d</i>, <b>12</b><i>e</i>, and <b>12</b><i>f </i>illustrate examples of breaking effects.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. 13</figref><i>a</i>, <b>13</b><i>b</i>, and <b>13</b><i>c </i>illustrate the visual effects of undercoat and overcoat.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 14</figref> is a flow chart illustrating one embodiment of a shading process.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram illustrating an embodiment of a hair/fur pipeline, similar to the pipeline of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>, but that includes additional and differing functionality.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. 16</figref><i>a </i>and <b>16</b><i>b </i>are diagrams that illustrate the clumping of control hairs with possible variations along the control hairs.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 17</figref> is a flow diagram illustrating a process <b>1700</b> to implement a fill-volume function, according to one embodiment of the invention.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 18</figref><i>a</i>-<i>c </i>are diagrams illustrating the generation of braid shapes defining surfaces and associated volumes (<figref idref="DRAWINGS">FIG. 18</figref><i>a</i>), the filling of these volumes with randomly placed control hairs (<figref idref="DRAWINGS">FIG. 18</figref><i>b</i>), and the interpolation of final hair strands from the control hairs (<figref idref="DRAWINGS">FIG. 18</figref><i>c</i>).</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. 19</figref><i>a</i>-<i>c </i>are diagrams showing side views of a deformed surface with identical control hairs that illustrate different types of interpolation techniques, respectively.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. 20</figref><i>a</i>-<b>20</b><i>c </i>are diagrams illustrating wave, weave, and wind effects.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 21</figref> is a flow diagram illustrating a process to implement geometric instancing, according to one embodiment of the invention.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. 22</figref><i>a </i>and <b>22</b><i>b </i>are diagrams illustrating an example of geometric instancing.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 23</figref> is a diagram illustrating a simple graph of a static node connected to an animation node and a finished control node to provide a hair motion compositing system, according to one embodiment of the invention.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 24</figref> is a diagram illustrating a process of utilizing a blend node.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. 25</figref><i>a </i>and <b>25</b><i>b </i>are diagrams illustrating rotational blending and positional blending, respectively.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 26</figref> is a diagram illustrating a blend ball.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 27</figref> illustrates a dynamic node graph including a dynamic solver node.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. 28</figref><i>a </i>and <b>28</b><i>b </i>are diagrams illustrating the use of volume node.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 29</figref> is a flow diagram illustrating a process super hair node processing is illustrated.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. 30</figref><i>a </i>and <b>30</b><i>b </i>are diagrams illustrating super hair operations in local-space and world space, respectively.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 31</figref> is a diagram illustrating a blend ball having both an inner sphere and an outer sphere.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 32</figref> is a diagram illustrating a cascading node graph to blend between various simulation caches.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 33</figref> is a diagram illustrating techniques to implement view-dependent screen-space optimization.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 34</figref> is a diagram illustrating a hair follicle root position and a hair that is transformed to a normalized device coordinate (NDC) system.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 35</figref> is a diagram illustrating the distance traveled by the root of a proxy hair from a first frame to a second frame in NDC space.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 36</figref> is a table illustrating a side-by-side comparison of non-optimized values versus optimized values in terms of hair count, time, and memory using the screen-space size metric.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 37</figref> is a table illustrating another comparison utilizing the screen-space speed method.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 38</figref> is a table illustrating non-optimized and optimized hair count, time, and memory values.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 39</figref> is a flow diagram illustrating a process to implement hair sub-patch optimization.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 40</figref> is a diagram illustrating a simplified example of the use of hair sub-patch optimization.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 41</figref> is a diagram showing a grassy landscape that was modeled, in which, the hairs were geometrically instanced with grass and/or trees, and that utilizes sub-patch optimization techniques.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 42</figref> is a diagram illustrating an example of a cache state file.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 43</figref> is a flow diagram illustrating a process to implement hair-caching.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 44</figref> is a table showing time-savings that can accomplished using hair-caching to render a fully-furred character.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0061" num="0060">The method and apparatus provides an innovative technique for the digital generation of fur on surfaces, such as on a computer generated animal. <figref idref="DRAWINGS">FIG. 1</figref><i>a </i>is a simplified block diagram of one embodiment that operates in accordance with the teachings of the present invention. Computer system <b>10</b> includes central processing unit (CPU) <b>15</b>, memory <b>25</b>, Input/Output <b>20</b>, which may be coupled to a storage device such as a disk drive or other device. The system may also include a keyboard. <b>40</b> or other user input device as well as a display <b>35</b> that may be used to display user interface and the final rendering of the fur in accordance with the teachings of the present invention.</p>
<p id="p-0062" num="0061">In one embodiment, memory <b>25</b> stores instructions which when executed by the CPU <b>15</b> performs the processes described herein. Alternately, the instructions may be received via storage <b>30</b> or other input such a user input <b>40</b>. The processes described herein may be executed via software by a system such as system <b>10</b> or hardware, or a combination of both hardware and software.</p>
<p id="p-0063" num="0062">An alternate embodiment is illustrated in <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>. Input is received by surface definition module <b>50</b> that defines a surface which, as will be explained below, defines surfaces and control hairs of the object to be rendered. Module <b>55</b> adjusts the control hairs to provide such functionality as combing and seamless hairs across surface boundaries. The interpolation module <b>60</b> interpolates across the surfaces using the control hairs. Hair clumping and breaking module <b>65</b> enhances the realistic visualization of the object by providing for clumping and breaking of hairs. Rendering module <b>70</b> renders the hairs and provides shading, black lighting and shadowing effects to the hairs, and module <b>75</b> displays the final output of the object with the hair surfaces</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 2</figref> is a flow diagram of the steps involved in generating fur coat in accordance with the teachings of the present invention. At step <b>200</b>, the geometry of the surfaces that contain the hair is defined. In one embodiment, a three-dimensional geometry may be used to model the skin, for example an animal skin, on which the fur coat is later generated. As illustrated in <figref idref="DRAWINGS">FIG. 3</figref><i>a</i>, the geometry is usually defined as a connected set of parametric surfaces often referred to as surface patches. The patches can be generated a number of ways known to one skilled in the art. In one embodiment, NURBS surface patches are used.</p>
<p id="p-0065" num="0064">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, at step <b>210</b>, control hairs are placed onto these surface patches, whereby each control hair is modeled as a parametric curve, e.g., a NURBS curve, defined by a user-specified number of control vertices. As will be discussed below, a global density value for the hair is given by the user to determine the number of actual hairs and their positions on the surface patches. Each hair also has a number of attributes such as length, width, waviness, opacity, and by default points in the direction of the surface normal at its position on the surface.</p>
<p id="p-0066" num="0065">In the present embodiment, a number of operations are performed on control hairs and the final hairs are generated based upon the control hairs and other information. However, it should be realized that these steps, such as combing and the like described herein may be performed on the final hairs instead of the control hairs.</p>
<p id="p-0067" num="0066">A number of different approaches may be taken to generate the control hairs. One simple algorithm places equally spaced x hairs in the u direction and y hairs in the v direction (where x and y are specified by the user) of each NURBS patch. Alternately, the x and y hairs are placed equally by arc-length. This will result in a more uniform distribution across the patch. However, it does not achieve a balanced distribution of control hairs across patches of different sizes; x and y hairs are placed on all selected patches, whether large or small. Therefore, in an alternate embodiment, the generation of control hairs takes into account the area of a NURBS patch to determine x and y for each patch. In one embodiment, the user specifies z hairs per unit area. In addition in one embodiment, control hairs can also be placed individually or along curves-on surfaces for finer control. For example, extra control hairs along the sharp rim of the ears of an animal can be generated to ensure proper alignment of the fur eventually generated.</p>
<p id="p-0068" num="0067">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, once the control hairs are generated, step <b>210</b>, the control hairs are adjusted at the surface boundaries. Since control hairs are placed in each surface patch, control hairs that lie on the boundary of a surface patch might not line up with control hairs on a neighboring surface patch; this can lead to visible discontinuities of the hairs along the surface boundaries. To address the potential problem, the control hairs on surface boundaries are adjusted. One embodiment of the process for adjusting the control hairs is illustrated by the flow chart of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0069" num="0068">At step <b>400</b>, seams are constructed between adjacent surfaces. Each seam identifies adjacent surfaces along a corresponding boundary (for example, an entire edge, T-junctions, or corners) of a surface patch. At step <b>405</b>, for each surface patch, the boundaries are traversed, step <b>410</b>. Each control hair is examined, step <b>412</b>. At step <b>415</b>, if a boundary hair is found, at step <b>420</b>, the neighboring patches, as identified by a corresponding seam, are checked to see if there is a corresponding hair on the neighboring patch. In one embodiment, a hair is corresponding if it is within a small predetermined distance from the boundary hair. The distance may be specified in parametric u, v, or absolute space. In one embodiment, the predetermined distance may be a relatively small distance such that the hairs visually appear co-located.</p>
<p id="p-0070" num="0069">If there is a corresponding control hair, the boundary hair and the corresponding hair are aligned, step <b>425</b>, by modifying the location and orientation of one or both of the control hairs to a common location and orientation, respectively. In one embodiment, the corresponding hair of an adjacent surface patch is snapped to the location of the boundary hair along the boundary. In one embodiment, if the neighboring surface patch does not have a corresponding hair, then one is inserted and aligned on the neighboring patch, step <b>445</b>. The process continues for each boundary hair along each boundary in each surface patch, steps <b>430</b>, <b>435</b> and <b>440</b> until all boundary hairs are aligned.</p>
<p id="p-0071" num="0070">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, after step <b>215</b>, in one embodiment, the control hairs have been placed on the surfaces defining an animal or other object model and the control hairs point along a surface normal at their positions on the surface.</p>
<p id="p-0072" num="0071">At step <b>220</b>, the hairs are combed to achieve desired, groomed, dry fur looks. A number of different combing processes may be used. In the present embodiment, however, static and animated combing processes are applied to the control hairs. The combination of static and animated combing provides a low computation cost but effective visual effect. In alternate embodiments, static or animated combing alone may be used and generate beneficial visual results. The combing process may be used on the same control hairs for different shots, to provide, for example, a groomed look versus a slightly messy look of the fur.</p>
<p id="p-0073" num="0072">One embodiment will be described with reference to <figref idref="DRAWINGS">FIG. 3</figref><i>b</i>. In one embodiment, static combing is applied if the hairs do not &#x201c;actively&#x201d; move during an animation of the object, e.g., the animal. It should be noted that since each hair is expressed in a local coordinate system defined by the surface normal, du and dv at the root of the hair, statically combed hairs will &#x201c;passively&#x201d; move as the underlying surface is deformed or animated. Combing is achieved by specification of combing direction curves, degree of bending and curvature of the hair, as well as with a fall-off for each combing direction curve.</p>
<p id="p-0074" num="0073">At step <b>325</b>, one or more combing direction curves are created. These curves indicate the direction applicable control hairs are to be combed. An example is illustrated by <figref idref="DRAWINGS">FIGS. 3</figref><i>c </i>and <b>3</b><i>d</i>. <figref idref="DRAWINGS">FIG. 3</figref><i>c </i>shows a number of uncombed control hairs. <figref idref="DRAWINGS">FIG. 3</figref><i>d </i>illustrates an exemplary combing direction curve <b>365</b> and its direction. The combed hairs as also illustrated in <figref idref="DRAWINGS">FIG. 3</figref><i>d. </i></p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 3</figref><i>d </i>illustrates one combing direction curve. However, it is common to implement a number of different curves, each curve corresponding to a different area of the surface. Thus, at step <b>330</b>, for each curve, one or more control hairs are assigned such that the assigned hairs are combed in accordance with the corresponding combing direction curve.</p>
<p id="p-0076" num="0075">In addition, at step <b>335</b>, for each curve, bend, curvature and fallout parameters are defined. The bend parameter defines how close the control hair is to the surface. The curvature parameter indicates the shape of the hair. For example, a curvature value of zero may indicate the hair is to be straight up and a maximum value (e.g., 1) may indicate the hair is bent on a tight arc from root to tip.</p>
<p id="p-0077" num="0076">The fallout value indicates a region beyond which the combing direction curve decreases its influence the further away the control hair is from the curve. In some embodiments, the fallout region is specified to cover relatively large areas so all control hairs are equally affected and fallout does not occur. In other embodiments it is desirable to decrease the combing effect, the further the distance between the control hair and the combing direction curve.</p>
<p id="p-0078" num="0077">At step <b>340</b>, each control hair is processed according to the combing direction curve it is assigned to, and the bend, curvature and fallout parameters. The result of the process is illustrated by <figref idref="DRAWINGS">FIG. 3</figref><i>d </i>in which the hairs are combed in the direction of the combing direction curve <b>365</b>. In addition, the hairs are curved according to the bend and curvature parameters defined. In the present example the fallout parameter defines the entire surface such all hairs are equally affected and fallout is not present.</p>
<p id="p-0079" num="0078">As noted above, animated combining may also be applied, step <b>345</b>. Key framing, known in the art, is used to interpolate between combing changes specified at certain frames to proving smooth transitions between changes. Thus for example, bend curvature and fallout parameters may be specified to change at certain frames. The key framing process execution then transitions during the frames between the specified frame changes. This technique can be used to simulate a variety of conditions which affect the look of the hair, such as wind. Thus, the hairs can be animated by key framing the parameters and executing the combing calculations at each frame during playback.</p>
<p id="p-0080" num="0079">The combing process may also include a simple hair/surface collision model process in which hairs that intersect with the underlying surface due to the combing process are pushed back up to be above the surface. Hairs may be rotated to intersect the underlying surface due to, for example, by setting the bend parameter to a large value.</p>
<p id="p-0081" num="0080">The process includes an iterative algorithm that determines hair/surface intersections. For example, the process performs a line segment intersection check of successive control vertices of a curve (e.g., the NURBS curve) defining a control hair with the surface. If a control vertex c goes below the surface, the hair is rotated back towards the surface normal from the previous non-intersecting vertex just enough for c to clear the surface. The amount of rotation is large enough to cause the hair to rotate back up above the surface by a small amount specified by the application. Thus the vertices of the vector affected by the combing are rotated back towards the surface normal so that the vector is above the surface.</p>
<p id="p-0082" num="0081">In an alternate embodiment, the combing may be animated by turning each control vertex of a control hair into a particle, and applying dynamic effects like gravity and external forces. Software, such as Maya, available by Alias|Wavefront, a division of Silicon Graphics, Inc., Toronto Canada, may be used to perform this function.</p>
<p id="p-0083" num="0082">Once the control hairs are identified and processed (e.g., adjusted, combed), the final hairs for each patch are generated from the control hairs, step <b>223</b>, <figref idref="DRAWINGS">FIG. 2</figref>. As noted above, in one embodiment the control hairs are first adjusted along surface boundaries. In an alternate embodiment, combing may be applied to control hairs alone or in conjunction with application of the surface boundary adjustment process.</p>
<p id="p-0084" num="0083">One exemplary process for the placement of hairs on patches is illustrated by the flow chart of <figref idref="DRAWINGS">FIG. 5</figref>. In this embodiment, final hairs are generated from control hairs in two set steps. First, the static hair features are calculated, e.g., the placement (the u, v position) of the final hairs. This step may be performed once. The second set of steps may be performed for each frame in an animation and provide frame dependent hair features.</p>
<p id="p-0085" num="0084">At step <b>505</b>, subpatches are identified on a surface of the object on which the hairs are to be generated. <figref idref="DRAWINGS">FIG. 6</figref> illustrates one embodiment for placing the final hairs on the surfaces defining the underlying skin. In the present embodiment, the root positions for each final hair are determined in terms of</p>
<p id="p-0086" num="0085">(u, v) parametric values of the surface. They are computed from an overall (global to all surfaces) density input value dmax (number of hairs per square unit area), and a set of normalized local density values (values range from 0 to 1; default value may be 1) per surface patch, arranged at a variable resolution grid of equally spaced points on the surface (for example, 128&#xd7;128 points).</p>
<p id="p-0087" num="0086">In one embodiment, the process attempts to make the number of hairs independent of the resolution of this grid and independent of the surface patch size to provide seamless density across surfaces of different scale. For purposes of discussion, it is assumed that the specified input density value (dmax) is 10 hairs/unit square area, and the local density values are arranged at equally spaced points on the surface as shown in <figref idref="DRAWINGS">FIG. 6</figref> (e.g., 0.4, 0.5, 0.6, 0.6 hairs, respectively). These points define the subpatches of the surface to be processed, step <b>505</b>, <figref idref="DRAWINGS">FIG. 5</figref>. As these equally spaced points are traversed, step <b>510</b>, the area in (u, v) space between the neighboring points is approximated by the area defined by two polygons, more particularly, triangles (a<b>1</b> and a<b>2</b>), and the number of hairs/square unit area for each triangle hair unit is averaged from the values at its vertices step <b>520</b>. In one embodiment, this is determined according to the following: HairUnit=dmax*Vavg, where dmax represents the specified input density value and Vavg represents the average local density value for each triangle determined from values at its vertices. For the example defined, this results in 10*(0.4+0.5+0.6)/3=5 and 10*(0.4+0.6+0.6)/3=5.333 hairs/square unit area for the top left and bottom right triangle, respectively.</p>
<p id="p-0088" num="0087">At step <b>525</b>, the total number of hairs to place on the current subpatch is determined from the actual approximated area of the subpatch (a<b>1</b> and a<b>2</b>) and the number of hairs per unit area. In one embodiment, the total number of hairs per unit area is determined according to the following: HairTotal=A*HairUnit, where A represents the actual approximated area of the subpatch. For example, if a value of 0.4 applies to area a<b>1</b> and 0.3 applies to area a<b>2</b>, as assumed for purposes of discussion, 0.4*5+0.3*5.333=3.5999 is the total number of hairs to place in subpatch defined by (ui, vi), (ui,vi+1), (ui+1,vi+1), (ui+1,vi).</p>
<p id="p-0089" num="0088">At step <b>530</b>, the final hairs are placed. Since it is preferable not to place fractional hairs, either 3 or 4 hairs are placed depending on whether a uniformly generated random number in [0,1] is bigger or smaller than the fraction part (0.5999). The 3 or 4 control hairs are randomly placed in u [ui, ui+1] and randomly in v [vi, vi+1]. The process then proceeds back to step <b>515</b> to the subpatch defined by the next four equally spaced points.</p>
<p id="p-0090" num="0089">Each final hair contains a number of control vertices. The root position (first control vertex) of each control hair is specified in terms of a (u,v) value of the underlying surface. The remaining control vertices of each hair are defined in a known local coordinate system with origins specified at the hair root position, and axes in the direction of the surface normal, du, dv. In one embodiment, each hair is oriented along the surface normal and the coordinates of the control vertices are generated by subdividing the length of each hair into n&#x2212;1 equal parts, where n is the number of control vertices/hair. One example is illustrated in <figref idref="DRAWINGS">FIG. 7</figref><i>a</i>, where a hair <b>725</b> is defined on surface <b>730</b> with n=4. The root is vertex <b>720</b> and the remaining vertices are <b>705</b>, <b>710</b> and <b>715</b>.</p>
<p id="p-0091" num="0090">Once the root position is calculated the enclosing control hairs (in one embodiment three) for each final hair are determined. In one embodiment, a 2-dimensional Delaunay triangulation (know in the art and therefore not further discussed herein) is constructed of the (u,v) positions of the control hairs for each surface patch. This triangulation was chosen because it creates &#x201c;well-proportioned&#x201d; triangles, by minimizing the circumcircle and maximizing the minimal angles of the triangles. Once the Delaunay triangulation is constructed, it is determined which triangle each final hair falls into. The indices of the three control hairs which form the particular triangle are assigned to the hair that falls into that triangle.</p>
<p id="p-0092" num="0091">The weights (w<b>1</b>, w<b>2</b>, w<b>3</b>) which each of the three control hairs (c<b>1</b>, c<b>2</b>, c<b>3</b>) has on the final hair (h) are then calculated. This may be done using barycentric coordinates (know in the art and not further discussed here), and is illustrated in <figref idref="DRAWINGS">FIG. 7</figref><i>b</i>, where &#x201c;A&#x201d; represents the area of triangle <b>726</b> (c<b>1</b>, c<b>2</b>, c<b>3</b>). These weights are used for interpolating the final hairs from the control hairs as explained below.</p>
<p id="p-0093" num="0092">The above information of each final hair (i.e., the (u, v) position, the 3 enclosing control hairs, and the weights of each control hair) may be generated only once for an object in animation. This information is referred to herein as the static information. In contrast, the calculation of the orientation of each final hair may be done at each frame of an animation. This orientation is determined from the orientation of the control hairs and their corresponding weights by an interpolation process as explained with reference to <figref idref="DRAWINGS">FIGS. 7</figref><i>c </i>and <b>7</b><i>d. </i></p>
<p id="p-0094" num="0093">For each final hair (h) at step <b>756</b>, the corresponding three control hairs (c<b>1</b>, c<b>2</b>, c<b>3</b>) are converted into a surface patch space (in one embodiment, the patch coordinate system) utilized. At step <b>758</b>, control hair vectors, (e.g., v<b>11</b>, v<b>12</b>, v<b>13</b>), between control vertices are calculated (e.g., <b>782</b>, <b>783</b>). A variety of techniques may be used to calculate the control hair vectors; in one embodiment, the vectors are equally distributed between control vertices. The control hair vectors are then normalized (e.g., nv<b>11</b>, nv<b>12</b>, nv<b>13</b>). At step <b>760</b> the corresponding control hair vectors of the three control hairs are interpolated and multiplied by the three determined weights that were calculated for the final hair. In one embodiment, for example, one control hair vector is determined according to the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>iv</i>1<i>=nv</i>11<i>*w</i>1<i>+nv</i>21<i>*w</i>2<i>+nv</i>31<i>*w</i>3;<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0095" num="0094">wherein iv<b>1</b> represents the interpolated, weighted control hair vector representation of the final hair, nv<b>11</b>, nv<b>21</b> and nv<b>31</b> represent normalized control vectors and w<b>1</b>, w<b>2</b> and w<b>3</b> represent the corresponding weights for the normalized control vectors. At step <b>762</b>, the resulting vectors are scaled to a final hair length (siv<b>1</b>, siv<b>2</b>, siv<b>3</b>). Once the scaled vectors are determined, at step <b>764</b> the control vertices (<b>786</b>, <b>788</b>, <b>790</b>, <b>792</b>) of the final hair are calculated from the scaled vectors.</p>
<p id="p-0096" num="0095">As illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, steps <b>225</b>, <b>230</b>, <b>235</b> and <b>245</b> may be optional and may not be necessary to produce a groomed, dry fur coat. Steps <b>225</b>, and <b>230</b> are applied for the generation of wet fur. Clumping of hairs can occur when the fur gets wet due to the surface tension or cohesion of water. The effect is that the tips of neighboring hairs (a bunch of hairs) tend to gravitate towards the same point, creating a kind of cone-shaped &#x201c;super-hair&#x201d;, or circular clump. As will be described below, step <b>225</b> is executed for static area clumping that generates hair clumps in fixed predefined areas. Step <b>230</b> is executed for animated area clumping, that is, when clumping areas move on the model, for example, to simulate spouts of water or raindrops hitting the fur and making it increasingly wet. In both cases, parameters which can be animated are provided to achieve various degrees of dry-to-wet fur looks. Step <b>235</b> is applied to generate dry fur clumping or breaking.</p>
<p id="p-0097" num="0096">According to a particular application all or some of the steps <b>225</b>, <b>230</b> and <b>235</b> may be executed. In addition, the steps <b>225</b>, <b>230</b> and <b>235</b> may be prioritized according to an application such that a hair adjusted in a higher priority step is not adjusted in the other steps. Alternately, the effects may be cumulative or selectively cumulative depending upon a particular application.</p>
<p id="p-0098" num="0097">At step <b>225</b> static area clumping is performed. One embodiment of this process is described with reference to <figref idref="DRAWINGS">FIG. 8</figref>. For purposes of discussion, the center hair of each clump is referred to as the clump-center hair, and all the other member hairs of that clump, which are attracted to this clump center hair, are referred to herein as clump hairs.</p>
<p id="p-0099" num="0098">In one embodiment there are four clumping input parameters: clump-density, clump-size, clump-percent and clump-rate. Similar to the hair-density parameter, clump-density specifies how many clumps should be generated per square area. The process described herein translates the clump density into an actual number of clumps defined by clump-center hairs, the number of clump center hairs depending on the size of each surface patch. As a result, some of the existing hairs are turned into clump-center hairs.</p>
<p id="p-0100" num="0099">Clump-size defines the area of a clump. In one embodiment, the clump size is defined in world space, a space that a user typically references with respect to a size of an object. In one embodiment, clump density takes priority over clump size, such that if there are many clumps and most of them overlap, the clump size cannot be maintained, since a clump hair can be a member of only one clump. If both clump density and size are small, many hairs between the clumps will not be clumped.</p>
<p id="p-0101" num="0100">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, to determine clump membership of each final hair (i.e., what clump each hair belongs to, if any), the clump of the specified clump-size is converted into u-radius and v-radius components in parametric surface space at each clump-center hair location, step <b>800</b>. Each hair is evaluated at steps <b>805</b>, <b>810</b> to determine whether it falls within the u, v radius components of a corresponding clump-center hair. If the hair is not within the u, v radius components, the hair is not a clump hair, step <b>815</b> and the process continues, step <b>830</b>, with the next hair. If the hair is within the u, v radius components, at step <b>820</b> the clump-center hair's index is referenced with the hair. In addition, a clump rate and clump percent is assigned, step <b>825</b>.</p>
<p id="p-0102" num="0101">A number of variations are contemplated. A clump-size noise parameter may be introduced to produce random variations in the size of the clumps. Feature (texture) maps for a clump-size can be created and specified by the user, one per surface patch, to provide local control of the radii used at steps <b>805</b>, <b>810</b>. In this embodiment, the global clump-size input parameter is multiplied for a particular clump (clump center hair) at (u,v) on a surface patch with the corresponding normalized (s,t) value in the clump-size feature map for that surface. Also, a static clump-area feature map can be provided to limit clumping to specified areas on surface patches rather than the whole model.</p>
<p id="p-0103" num="0102">In one embodiment a clump-percent and clump-rate value is assigned to each clump hair (step <b>825</b>). In one embodiment, the values for both range between [0,1], and are used subsequently to reorient clump hairs, step <b>835</b>, as described below.</p>
<p id="p-0104" num="0103">Clump-percent specifies the degree of clumping for a clump hair. For example, a value of zero indicates that the hair is not clumped at all, i.e., it is like a &#x201c;dry&#x201d; hair. A value of one indicates that the hair is fully attracted to its clump-center hair, i.e., the tip of the hair (its distant control vertex) is in the same location as the tip of the clump-center hair.</p>
<p id="p-0105" num="0104">Clump-rate defines how tightly a clump hair clumps with its corresponding clump-center hair. For example, a value of zero indicates that the clump hair is linearly increasingly attracted to its clump-center hair, from the root to the tip. A clump-rate value closer to one indicates that the hair's control vertices closer to the root are proportionally more attracted to corresponding clump-center hair vertices than those closer to the tip, which results in tighter clumps. Examples of different values for clump-percent and clump-rate are given in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0106" num="0105">At step <b>835</b>, the control vertices (except the root vertex) of each clump hair are reoriented towards corresponding clump-center hair vertices from the clump hair's dry, combed position determined at steps <b>200</b>, <b>210</b>, <b>215</b>, <b>220</b>, and <b>223</b>.</p>
<p id="p-0107" num="0106">In one embodiment, this process is performed at each frame. In one embodiment, the default value for number of control vertices (CVs) is 3 (4 minus the root vertex), and the index for the current control vertex i ranges from 1-3. In one embodiment, the reorientation is determined as follows: clumpHairCV[i]=clumpHairCV[i]+delta*(clumpCenterHairCV[i]&#x2212;clumpHairCV[i]) delta=clumpPercent*(fract+clumpRate*(1&#x2212;fract)); where fract=i/numberOfCVs; clumpHairCV[i] represents a clump hair vertex; clumpCenterHairCV[i] represents a corresponding clump center hair vertex; i represents an index to a current control vertex; numberofCVs represents the number of control vertices of a clump hair; clumpPercent represents clump-percent; and clumpRate represents the clump-rate.</p>
<p id="p-0108" num="0107">Both clump-percent and clump-rate parameters can be locally controlled via feature maps similar to the feature maps described above with respect to clump-size. Both values can also be animated or changed over time to provide continuous control for dry-to-wet-to-dry fur looks. This is illustrated by <figref idref="DRAWINGS">FIGS. 10</figref><i>a</i>, <b>10</b><i>b</i>, <b>10</b><i>c </i>and <b>10</b><i>d </i>which illustrate four frames from an animated clump-percent and clump-rate sequence. In the image of <figref idref="DRAWINGS">FIG. 10</figref><i>a </i>the clump-percent and clump-rate are both zero and may represent dry, combed hair. In the image of <figref idref="DRAWINGS">FIG. 10</figref><i>b</i>, clump-percent is 0.7 and clump-rate is 0, which results in a slightly wet look. In the image of <figref idref="DRAWINGS">FIG. 10</figref><i>c</i>, clump-percent is 1.0 and clump-rate is 0.3, which results in a wet look. In the image of <figref idref="DRAWINGS">FIG. 10</figref><i>d</i>, clump-percent and clump-rate are both 1.0, which produces a very wet look.</p>
<p id="p-0109" num="0108">Animated area clumping is desirable to simulate spouts of water or raindrops hitting the fur and making it increasingly wet. At step <b>230</b>, <figref idref="DRAWINGS">FIG. 2</figref>, animated clumping is performed. In one embodiment, the animated clumping areas are defined in an animation system.</p>
<p id="p-0110" num="0109">One embodiment of the process is described with reference to <figref idref="DRAWINGS">FIG. 11</figref>. In one embodiment, clumping areas are defined by particles hitting surface patches. Other embodiments may use alternate techniques for generating animated clumping areas. At step <b>1100</b> a global static area clumping process is performed on all hairs. This step identifies clumping regions and corresponding clump center hairs and clump hairs. As explained below this information is used in the animated clumping process. In one embodiment, the global static area clumping used is that described above for static area clumping.</p>
<p id="p-0111" num="0110">At step <b>1102</b>, one or more emitters that generate the particles are defined. The use of emitters to generate particles is known in the art and will not be discussed in detail herein. In one embodiment, the emitters define the rate generated and spread of the particles across a surface.</p>
<p id="p-0112" num="0111">At step <b>1105</b>, at each frame for each particle generated that hits the surface, the surface patch the particle hits is identified, step <b>1110</b>. In one embodiment, particles generated in prior frames are carried through subsequent frames such that the particles are cumulative.</p>
<p id="p-0113" num="0112">For each particle that hits a surface patch, including those particles generated in prior frames, a circular animated clumping area is created, step <b>1115</b>, on the patch at that (u,v) location, with clump-percent, clump-rate, and animated clumping area radius determined by a creation expression executed at the frame where the particle hits the surface so that when a particle hits the surface at that time (i.e., at the frame), the clump-percent may be set to zero and the radius may be defined to a specified value perhaps adjusted by a random noise value. Thus, the expression may be defined to provide the desired &#x201c;wetness&#x201d; effect.</p>
<p id="p-0114" num="0113">The radius of the circular clumping area defined is converted into a corresponding u-radius and v-radius similar to the clump size discussed above. Runtime expressions executed at each frame define clump-percent and clump-rate, thus determining how quickly and how much the fur &#x201c;gets&#x201d; wet. For example, one runtime expression may be: MIN(FrameNumber*0.1, 1) such that as the frame number increases, the hair appears increasingly wet.</p>
<p id="p-0115" num="0114">Each clump center hair of a clump (determined at step <b>1100</b>) is then evaluated to determine if it falls within the animated clumping area, step <b>1120</b>. To determine whether a clump falls within an animated clumping area, at each frame it is checked as to whether the (u,v) distance between the clump-center hair of the clump and the center of the animated clumping area is within the (u,v) radius parameters of the animated clumping area. For clumps that are located in overlapping animated clumping areas, the values for clump-percent and clump-rate are added resulting in the generation of wetter fur.</p>
<p id="p-0116" num="0115">If the clump center hair is within the animated clumping area, step <b>1125</b>, the corresponding clump is flagged with an animated clumping flag such that the clump hairs are subsequently reoriented to reflect the animated clumping effect. Alternately, each clump hair of the clump may have an animated clumping flag which is set if the corresponding clump center hair is determined to be within an animated clumping area. In addition, an animated clump-rate value and an animated clump-percent value are assigned to the clump hairs that are identified to be within an animated clumping area in accordance with a runtime expression. In one embodiment, the values for clump-percent and clump-rate for each clump within an animated clumping area are replaced with the corresponding values for the animated clumping area at each frame. As animated clumping areas may be much bigger than a clump, an animated clumping area may contain several individual clumps. Each clump is evaluated, step <b>1140</b>, for each particle, step <b>1145</b>.</p>
<p id="p-0117" num="0116">It should be noted that animated clumping areas could straddle surface patch boundaries. For example, the center of an animated clumping area may be located on one surface patch, but the area may be located on one or more other patches. Since the animated clumping areas are typically defined and therefore associated with the surface which contains the center of the animated clumping area, i.e., the position where the particle hit, portions of a clumping area straddling neighboring patches may be overlooked. This could lead to discontinuities in clumping of the final fur.</p>
<p id="p-0118" num="0117">In one embodiment, this potential problem is addressed. Whenever a new particle hits a surface and the (u,v) radii exceed the boundaries of that surface; an additional (u,v) center and (u,v) radii is generated for the animated clumping areas affecting neighboring patches. Thus, for example, if the clumping area covers portions of two neighboring patches, a corresponding (u,v) center and radii are generated for each neighboring patch to provide additional animated clumping areas for evaluation at steps <b>1120</b>-<b>1140</b></p>
<p id="p-0119" num="0118">At step <b>1150</b>, for each frame, the clump hairs of clumps that are within the animated clumping areas are reoriented. Thus, clump hairs are selectively adjusted if they are within an animated clumping area. In one embodiment, clumping is restricted to the animated clumping areas at each frame, so that final hairs of clumps outside the animated clumping areas are normally produced as &#x201c;dry&#x201d; hairs.</p>
<p id="p-0120" num="0119">At step <b>1155</b>, if more frames are to be processed, the process continues again at step <b>1105</b>. Thus the animated clumping process is performed across multiple frames to provide animated effects.</p>
<p id="p-0121" num="0120">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, step <b>235</b> may be applied to generate the effect of hair breaking or dry fur clumping by breaking up the groomed fur coat along certain lines (fur tracks or break line) on the underlying skin (surfaces). As described below, this process may include two kinds of hair breaking: symmetric and one-sided. In symmetric breaking, hairs on both sides of a fur-track &#x201c;break&#x201d; towards that track, whereas in one-sided breaking, hairs on one side of the track break away from the track.</p>
<p id="p-0122" num="0121">In one embodiment, fur tracks are specified as curves on surfaces in an animation system. Each track has a radius, break-percent and break-rate for symmetric and one-sided breaking, and an additional break-vector for one sided breaking. The final information generated is output into breaking files that are subsequently accessed to reorient the affected hairs.</p>
<p id="p-0123" num="0122">One embodiment of the hair breaking technique is illustrated by <figref idref="DRAWINGS">FIG. 12</figref><i>a</i>. At step <b>1200</b> the fur tracks are defined. The fur tracks may be defined similar to clumps by defining a (u,v) break radii. At step <b>1205</b> the break line hairs (hairs which lie on or are very close to the fur-track curve defined by the curve defined for the fur track) are computed. Using the break line hairs and break radii, at steps <b>1215</b>, <b>1220</b>, each hair is evaluated to determine whether the hair lies within the (u,v) break radii on both sides of the break line hairs in case of symmetric breaking, or to one side specified by the break vector (the break vector side) in case of one-sided breaking. For each hair within the space specified by the radii, referred to herein as a break hair, the corresponding break line hair (hair on the fur track) is then determined as the one closest to it. The hairs are labeled as break line hairs, break hairs with indices to their corresponding break line hairs, or normal hairs that do not reside within the areas specified by the break.</p>
<p id="p-0124" num="0123">It should be noted that for instances of one-sided breaking, each break hair is now reoriented &#x201c;away&#x201d; from its corresponding break line hair in the direction of the break-vector, rather than &#x201c;towards&#x201d; the break line hair. Examples of symmetric and one-sided breaking are shown in <figref idref="DRAWINGS">FIG. 12</figref><i>b. </i></p>
<p id="p-0125" num="0124">The break hairs are reoriented with respect to their corresponding break line hairs, step <b>237</b>. For symmetric breaking, this process is analogous to the process performed for clump hairs discussed earlier. However, for break hairs, the break-percent and break-rate values are used in place of the clump-percent and clump-rate used for clump hairs. For one-sided breaking, break hairs are repelled, as opposed to attracted to the break-line hairs according to the break-percent and break-rate parameters.</p>
<p id="p-0126" num="0125">The breaking effect is illustrated by <figref idref="DRAWINGS">FIGS. 12</figref><i>c</i>, <b>12</b><i>d </i><b>12</b><i>e </i>and <b>12</b><i>f</i>. <figref idref="DRAWINGS">FIG. 12</figref><i>c </i>illustrates an object <b>1250</b> having break line hairs <b>1252</b>, <b>1254</b>. <figref idref="DRAWINGS">FIG. 12</figref><i>d </i>shows the resultant effect on the object for symmetric breaking. <figref idref="DRAWINGS">FIGS. 12</figref><i>e </i>and <b>12</b><i>f </i>illustrate one-sided breaking along break line hairs <b>1256</b>-<b>1270</b>.</p>
<p id="p-0127" num="0126">At step <b>245</b>, <figref idref="DRAWINGS">FIG. 2</figref>, a decision is made as to whether multiple passes of the process are to be performed. The coat of most furred animals is composed of a fuzzier, thinner shorter layer of hairs called undercoat, plus an overcoat of longer and thicker hairs. Step <b>245</b> illustrates the capability to perform a two-(or multiple)-pass process, whereby steps <b>210</b>, <b>215</b>, <b>220</b> and <b>223</b> (and optionally <b>225</b>, <b>230</b>, and <b>235</b>) are executed more than once, producing a different set of hairs at each pass. These sets or layers are then processed and combined at render-time (step <b>250</b>). The effects can be seen by reference to <figref idref="DRAWINGS">FIGS. 13</figref><i>a</i>, <b>13</b><i>b </i>and <b>13</b><i>c</i>. <figref idref="DRAWINGS">FIG. 13</figref><i>a </i>is a representation of an undercoat generated in accordance with the teaching of the present invention. <figref idref="DRAWINGS">FIG. 13</figref><i>b </i>represents a representation of the overcoat and <figref idref="DRAWINGS">FIG. 13</figref><i>c </i>represents the combined image consisting of the undercoat and overcoat.</p>
<p id="p-0128" num="0127">As illustrated by step <b>250</b>, the clumped hairs represented by their control vertices are rendered into a series of two-dimensional images to create lifelike dry and wet hair looks. In one embodiment, the process functions to project a three-dimensional hair geometry onto a two-dimensional image plane from the perspective of a particular point of view.</p>
<p id="p-0129" num="0128">In order to render large amounts of hair quickly and efficiently, the geometric model of each hair may be kept simple. As explained above, a hair is represented by a parametric curve having a determined number of control vertices (in one embodiment, the default is four).</p>
<p id="p-0130" num="0129">In one embodiment, the process employs known rendering technology to produce the hairs described by the corresponding control vertices. In an alternate embodiment customized modules are added to realistically &#x201c;shade&#x201d; the hairs. This may be accomplished by assigning an intensity of color at each point along or on a hair, wherein points along a hair may be defined as the pixels which compose the hair.</p>
<p id="p-0131" num="0130">During the rendering of the hairs, a width is added for each hair to transform it into a narrow ribbon that is always oriented towards the camera or view point. The shading process properly shades these ribbon primitives to more realistically present them as thin hairs.</p>
<p id="p-0132" num="0131">One embodiment of the shading process is set forth in the flow chart of <figref idref="DRAWINGS">FIG. 14</figref>. At step <b>1400</b>, each hair is processed. At step <b>1405</b>, for each hair, the surface normal at the base of the hair is mixed with the normal vector at the current point on the hair in order to obtain a shading normal at the current point on the hair. In one embodiment, the hairs are rendered as a series of points or pixels on a display. Thus, the current point is one of the pixels representing a hair.</p>
<p id="p-0133" num="0132">The shading process may be applied at multiple points along the hair. In one embodiment, the amount with which each of these vectors contributes to the mix is based on the angle between the tangent vector at the current point on the hair, and the surface normal vector at the base of the hair. The smaller this angle, the more the surface normal contributes to the shading normal.</p>
<p id="p-0134" num="0133">At step <b>1410</b> the intensity of the hair at the current point on the hair is determined using the shading normal at that point. In one embodiment, a Lambertian model is used to calculate these intensities. Using this approach provides the benefit of allowing the user to light the underlying skin surface and receive predictable results when fur is added. This approach also accounts for shading differences between individual hairs, and differences in shading along the length of each hair.</p>
<p id="p-0135" num="0134">In order to obtain realistic shadows on the fur coat, shadow maps are used. The use of shadow maps is known in the art and will not be discussed further herein. However, incorporating the hair into the shadow maps may generate several unwanted side effects. One problem is that of dark streaking on brightly lit fur because of the fur self-shadowing. Dark streaks look wrong on brightly lit fur because normally light bounces off the skin and hair to prevent dark shadows on brightly lit fur.</p>
<p id="p-0136" num="0135">In order to minimize the dark streaking effects, in one embodiment, the hairs for the shadow maps are shortened based on certain criteria, step <b>1415</b>. For example, the length and density of the hair may dictate the percentage to shorten the hair. By selectively shortening hairs for the shadow maps, the hair self-shadowing effect is minimized while still producing a broken up shadow on the terminator lines for lights falling on the fur.</p>
<p id="p-0137" num="0136">Back lighting is achieved in a similar fashion using a shadow map for each light located behind the furred object, and again shortening the hair on the basis of density and length in the shadow map render process. In one embodiment, a lighting model for hairs also allows each light to control its diffuse fall-off angles. Thus, lights directly behind the furred object can wrap around the object. Using these lighting controls together with shadow maps reasonable back lighting effects are achieved.</p>
<p id="p-0138" num="0137">In one embodiment, the shading for clumped hairs is modified. In one embodiment, two aspects of the hair shading may be modified. First, the amount of specular on the fur is increased. Second, clumping is accounted for in the shading model. Geometrically, as explained earlier, fur is modeled in clumps to simulate what actually happens when fur gets wet. In the shading model, for each hair and for each light, the side of the clump the hair is on with respect to the light's position is determined, and the hair is either darkened or brightened based on the side the hair is on. Thus, hairs on the side of a clump facing the light are brighter than hairs on a clump facing away from the light.</p>
<p id="p-0139" num="0138">Additional Embodiments</p>
<p id="p-0140" num="0139">Further embodiments of the invention relate to additional features added to the animal fur and human hair pipeline of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>. In particular, these additional features to the pipeline are directed to producing a wide variety of stylized and photorealistic fur and hair looks for digital characters.</p>
<p id="p-0141" num="0140">Turning now to <figref idref="DRAWINGS">FIG. 15</figref>, <figref idref="DRAWINGS">FIG. 15</figref> is a block diagram illustrating a hair/fur pipeline <b>1500</b>, similar to the previously-described pipeline of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>, but that includes additional and differing functionality.</p>
<p id="p-0142" num="0141">As before, input, is received by surface definition module <b>1550</b>. Surface definition module <b>1550</b>, as previously described, defines surfaces and control hairs of the object to be rendered. Further, as previously described, control hair adjustment module <b>1555</b> adjusts control hairs to provide such functionality as combing and seamless hairs across surface boundaries.</p>
<p id="p-0143" num="0142">In addition to the previously-described pipeline of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>, a hair motion compositor module <b>1557</b> has been added to hair/fur pipeline <b>1500</b> to provide for editing and combining different hair animations, as will be described in detail hereinafter. Interpolation module <b>1560</b>, as previously described, may be utilized to interpolate hairs across surfaces using control hairs.</p>
<p id="p-0144" num="0143">Additionally, an effects module <b>1565</b> may be utilized as part of the hair/fur pipeline <b>1500</b> to provide various effects to hair and fur such as clumping, breaking, wave effects, weave effects, etc., as will be described in more detail hereinafter. Further, an optimization module <b>1567</b> may be utilized with hair/fur pipeline <b>1500</b> to provide methodologies to improve render times, as will be described in more detail hereinafter.</p>
<p id="p-0145" num="0144">Shading, backlighting, and shadowing module <b>1570</b> may be utilized in hair/fur pipeline <b>1500</b> to provide shading, backlighting, and shadowing effects to hair/fur and display module <b>1575</b> may be utilized to render and display the final output of the object with the hair/fur surface, as previously described.</p>
<p id="p-0146" num="0145">Moreover, as previously described with reference to <figref idref="DRAWINGS">FIG. 1</figref><i>a</i>, it should be appreciated that the hair/fur pipeline <b>1500</b> may be implemented with a computer system having a central processing unit (CPU), memory, input/output (I/O), etc., which may be coupled to a storage device such as a disk drive or other device. Further, the computer system may include a keyboard or other user input devices as well as a display that may be used to display user interfaces and the rendering of hair/fur in accordance with the embodiments of the present invention.</p>
<p id="p-0147" num="0146">In particular, embodiments of the invention are directed to techniques implemented by the previously-described pipeline <b>1500</b> of <figref idref="DRAWINGS">FIG. 15</figref> related to such items as: special combing tools, different final hair interpolation algorithms, and a general application program interface (API) to perform customizable per-hair calculations in addition to rendering each final hair. Further techniques relate to: a method and system for editing and combining different hair animations referred to herein as a Hair Motion Compositor (HMC) and hair optimization strategies to improve render times. In one embodiment, the HMC may be implemented by Hair Motion Compositor module <b>1557</b> of hair/fur pipeline <b>1500</b> and the optimization strategies may be implemented by optimization module <b>1567</b> of hair/fur pipeline <b>1500</b>.</p>
<p id="p-0148" num="0147">Digital animals, humans, and imaginary creatures are increasingly being incorporated into movies, both live-action and computer-animated. In order to make them believable, many of these characters need persuasive hair or fur. In a production environment, where quality is paramount, a pipeline to generate hair must not only work, but also needs to be practical, robust, flexible, efficient, and powerful.</p>
<p id="p-0149" num="0148">Described herein are tools and techniques which facilitate the creation of specific hair and fur looks to satisfy the appearance and expression that a director might choose for a particular show and its characters. Solutions are described which make it possible to generate convincing animal fur, produce believable human hair, and closely match the hair of real actors.</p>
<p id="p-0150" num="0149">Described herein are improvements to the hair/fur rendering pipeline of <figref idref="DRAWINGS">FIG. 1</figref><i>b</i>, which address and improve three of the most complex and time-consuming areas in the digital hair creation process: look development (combing), hair animation and shots, and rendering large numbers of hairs.</p>
<p id="p-0151" num="0150">It should be noted that when modeling hair geometrically, problems arise with respect to human hair that are slightly different from those with animal fur. Specially, longer human hair requires much more sophisticated combing and animation tools. For animal fur, the rendering stage needs to be optimized as there are millions of individual hair strands compared to around 100,000 to 150,000 for humans.</p>
<p id="p-0152" num="0151">As will be described herein, the techniques employed by hair/fur pipeline <b>1500</b> relate to producing a wide variety of hairstyles, from short animal fur to long human hair. In particular, hair/fur pipeline <b>1500</b> includes features such as: tools for modeling, animating, and rendering hair; allowing a small, arbitrary number of control hairs to define basic comb and animation features; and techniques for interpolating final rendered hair strands from these control hairs.</p>
<p id="p-0153" num="0152">It should be noted that conventional 3D animation software, such as MAYA available by AUTODESK, may be utilized to provide conventional viewing functionality. Additionally, the RENDERMAN software package available by PIXAR may be utilized to aid in rendering final hair. These software packages are well known to those in the computer graphics arts.</p>
<p id="p-0154" num="0153">Additional combing and control hair editing tools implemented by the hair/fur pipeline are described hereinafter. For example, in one embodiment, a basic guide-chain attached to a control hair may be used in either a forward or inverse kinematic mode to define a control hair's shape while approximately maintaining its length. It can also apply the same deformation to other selected control hairs either in world or local space. Further, an intuitive cutting tool may be utilized in which a user can sketch a curve in an orthographic view, which is then used to calculate intersections with selected control hairs and cuts them at those points.</p>
<p id="p-0155" num="0154">In one particular embodiment, a combing tool is disclosed that allows for selected control hairs to be clumped together in a controlled manner.</p>
<p id="p-0156" num="0155">With reference to <figref idref="DRAWINGS">FIGS. 16</figref><i>a </i>and <b>16</b><i>b</i>, <figref idref="DRAWINGS">FIGS. 16</figref><i>a </i>and <b>16</b><i>b </i>are diagrams that illustrate the clumping of control hairs with possible variations along the control hairs. As shown in <figref idref="DRAWINGS">FIG. 16</figref><i>a</i>, a first set of control hairs <b>1602</b> and a second set of control hairs <b>1604</b> are illustrated as extending normally from the head of a creature <b>1606</b>. A clump profile window <b>1610</b> includes a user-definable clump profile curve <b>1611</b> implemented with control hair adjustment module <b>1555</b> of hair/fur pipeline <b>1500</b> that may be applied to the first set of control hairs <b>1602</b> so that the control hairs are deformed in such a fashion, as shown by the deformed control hairs <b>1615</b>, such that they approximate user-definable clump profile <b>1611</b> as can be seen in <figref idref="DRAWINGS">FIG. 16</figref><i>b. </i></p>
<p id="p-0157" num="0156">Fill-Volume Techniques</p>
<p id="p-0158" num="0157">In one embodiment, surface definition module <b>1550</b> of the hair/fur pipeline <b>1500</b> may be used to generate a shape defining a surface and an associated volume. Control hair module <b>1555</b> may be used to fill the volume with control hairs and interpolation module <b>1560</b> may be used to interpolate final hair strands from the control hairs. These techniques will be described in more detail below.</p>
<p id="p-0159" num="0158">In particular, a fill-volume tool may be used to quickly fill an enclosed surface with randomly placed control hairs. This may be useful in generating hair to fill &#x201c;hair volumes&#x201d; such as volumes defining a ponytail often provided by modelers to describe the rough hair look of a character.</p>
<p id="p-0160" num="0159">Turning to <figref idref="DRAWINGS">FIG. 17</figref>, <figref idref="DRAWINGS">FIG. 17</figref> is a flow diagram illustrating a process <b>1700</b> to implement a fill-volume function. As shown in <figref idref="DRAWINGS">FIG. 17</figref>, at block <b>1710</b>, a shape is generated that defines a surface and an associated volume. For example, this may be accomplished utilizing surface definition module <b>1550</b> of hair/fur pipeline <b>1500</b>. Next, the volume is filled with randomly placed control hairs (block <b>1720</b>). For example, this may be accomplished utilizing control hair adjustment module <b>1555</b> of hair/fur pipeline <b>1500</b>. Lastly, final hair strands are interpolated from the control hair strands (block <b>1730</b>). For example, this may be accomplished utilizing interpolation module <b>1560</b> of hair/fur pipeline <b>1500</b>.</p>
<p id="p-0161" num="0160">Further, an example of this may be seen in <figref idref="DRAWINGS">FIGS. 18</figref><i>a</i>-<i>c</i>, where braids are simply represented as surfaces by a modeler, and then filled with hair. In particular, <figref idref="DRAWINGS">FIG. 18</figref><i>a</i>-<i>c </i>show the generation of braid shapes defining surfaces and associated volumes (<figref idref="DRAWINGS">FIG. 18</figref><i>a</i>), the filling of these volumes with randomly placed control hairs (<figref idref="DRAWINGS">FIG. 18</figref><i>b</i>), and the interpolation of final hair strands from the control hairs (<figref idref="DRAWINGS">FIG. 18</figref><i>c</i>).</p>
<p id="p-0162" num="0161">As shown in <figref idref="DRAWINGS">FIG. 18</figref><i>a</i>, at first, three braided cylinders <b>1802</b>, <b>1804</b>, and <b>1806</b> are generated each of which defines a shape and an associated volume. Next, as shown in <figref idref="DRAWINGS">FIG. 18</figref><i>b</i>, the fill-volume function is utilized to generate control hairs <b>1810</b>, <b>1812</b>, and <b>1816</b>, respectively. Lastly, as can be seen in <figref idref="DRAWINGS">FIG. 18</figref><i>c</i>, the final braids of hair <b>1820</b>, <b>1822</b>, and <b>1826</b> are interpolated from the control hairs and are rendered.</p>
<p id="p-0163" num="0162">Additional control hairs may be automatically interpolated from existing ones and final hair strands may also be automatically interpolated from existing ones utilizing interpolation algorithms similar to those described below. Additionally, the shape of newly inserted control hairs may be blended between selected existing control hairs.</p>
<p id="p-0164" num="0163">In one embodiment, the hair/fur pipeline <b>1500</b> may utilize different algorithms to interpolate final hair strands from control hairs as well as other control hairs from existing control hairs (e.g. utilizing interpolation module <b>1560</b>) based on different coordinate schemes.</p>
<p id="p-0165" num="0164">An example of this interpolation methodology may be seen with reference to <figref idref="DRAWINGS">FIGS. 19</figref><i>a</i>-<i>c</i>. As can be seen in <figref idref="DRAWINGS">FIGS. 19</figref><i>a</i>-<i>c</i>, side views of a deformed surface <b>1900</b> are shown with identical control hairs <b>1902</b>. By utilizing this interpolation methodology, the number of control hairs may be kept small across different applications.</p>
<p id="p-0166" num="0165">For example, <figref idref="DRAWINGS">FIG. 19</figref><i>a </i>shows a first interpolation method for the interpolation of final hair strands <b>1904</b> from control hairs <b>1902</b> in a &#x201c;world space&#x201d; coordinate frame, in which the final hair strands <b>1904</b> do not automatically follow surface deformations. This is useful for plants or long hairstyles that should not follow surface (skin) deformations.</p>
<p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. 19</figref><i>b </i>shows a second interpolation method for the interpolation of final hair strands <b>1904</b> from control hairs <b>1902</b> in a &#x201c;local space&#x201d; coordinate frame. In the &#x201c;local space&#x201d; scheme, the final hair strands <b>1904</b> follow surface deformations automatically. This is more natural for shorter hair or fur.</p>
<p id="p-0168" num="0167">Lastly, <figref idref="DRAWINGS">FIG. 19</figref><i>c </i>shows a third interpolation method for the interpolation of final hair strands <b>1904</b> from control hairs <b>1902</b> in which the final hair strands <b>1904</b> remain within the convex hull of influencing control hairs <b>1902</b>. This scheme may be useful, for instance, for long, un-clumped human hair dropping from the top and sides of the curved scalp. It should be noted that in the local and world space modes, interpolated final hair strands <b>1904</b> may appear longer than control hairs <b>1902</b> whereas in the convex hull mode they do not.</p>
<p id="p-0169" num="0168">Further, hierarchical clumping capabilities of final hair strands may be implemented utilizing the hair/fur pipeline <b>1500</b> (e.g. utilizing effects module <b>1565</b>). Final hairs may belong to manual clumps directly placed by a user or to procedurally generated auto and minor clumps. Auto-clump hairs may occur either inside or outside of manual clumps whereas minor clump hairs may only reside in manual or auto clumps. These techniques may be applied for wet hair looks and for customizing the look of tufts of dry hair.</p>
<p id="p-0170" num="0169">A further characteristic of the hair/fur pipeline <b>1500</b> is that a multitude of parameters or effects such as wave, weave, (rotations around the root of the hair), shorten-tip and wind may be implemented, for example, by effects module <b>1565</b> of hair/fur pipeline <b>1500</b>. These effects may be applied directly to final hairs after the interpolation between control hairs.</p>
<p id="p-0171" num="0170">Examples of these wave, weave, and wind effects are shown in <figref idref="DRAWINGS">FIGS. 20</figref><i>a</i>-<i>c</i>. In each of the cases of <figref idref="DRAWINGS">FIGS. 20</figref><i>a</i>-<i>c</i>, control hairs <b>2002</b> are shown as pointing straight up. For example, <figref idref="DRAWINGS">FIG. 20</figref><i>a </i>illustrates no effect on the final hairs <b>2004</b>. <figref idref="DRAWINGS">FIG. 20</figref><i>b </i>indicates some wave and weave effect being applied to the final hairs <b>2004</b>. Lastly, <figref idref="DRAWINGS">FIG. 20</figref><i>c </i>illustrates a wind effect being applied to the final hairs <b>2004</b>.</p>
<p id="p-0172" num="0171">Also, effects may be applied selectively to different hair &#x201c;types&#x201d; such as manual clump member hairs or un-clumped hairs, before or after clumping. In addition, independent hair parameters or effects can be used in combination with multiple final hair layers with possibly different sets of control hairs. For animal fur, undercoats and overcoats may be generated this way and complex human hair may be broken up into several distinct layers such as base, stray, and fuzz.</p>
<p id="p-0173" num="0172">Geometric Instancing</p>
<p id="p-0174" num="0173">Another embodiment of the invention relates to arbitrary geometry instancing. Arbitrary geometry instancing provides a feature that leverages off the hair/fur pipeline's <b>1500</b> ability to instantiate, animate, and control the style or look of final rendered hair primitive, as previously described. In this regard, a general application program interface (API) may be provided in conjunction with the hair rendering functions such that an application developer can obtain per hair follicle information from the hair system, and replace the rendering of hair with other operations.</p>
<p id="p-0175" num="0174">For example, in one embodiment, hair pipeline <b>1500</b> may be used to generate a user-selected geometry based upon a hair location for at least one hair with respect to a surface. Surface definition module <b>1550</b> may be used to define the surface. Display module <b>1575</b> may be used to: process a user-selected geometry and render the user-selected geometry at the hair location on the surface in place of the at least one hair.</p>
<p id="p-0176" num="0175">In particular, instead of outputting hair follicles of the final rendered primitive, a hook into the hair/fur pipeline <b>1500</b>, e.g., at display and rendering module <b>1575</b>, is provided to allow the user to override the rendered geometry and instead of rendering a hair, rendering another geometric shape. Thus, instead of populating, for example, a human head with hair, a human head with thorns may be generated. Or, instead of a field full of hairs, a field full of flowers, grass, or trees may be populated.</p>
<p id="p-0177" num="0176">Further, it should be noted that instantiating the exact same object (e.g., flowers or trees) does not work well when attempting to design a specific landscape because all the flowers or trees would be identical. Therefore, different kinds of primitives may be modeled and instantiated at render time to replace each final hair.</p>
<p id="p-0178" num="0177">Instead of completely discarding the final shape of the hair, the hair follicle can be used to represent the axis of deformation around which the instanced geometry may be deformed according to the shape of the hair. This way, all of the advantages of all the various effects (e.g., clumping, breaking, wave, weave, wind, etc.) previously described as to rendering hair may be utilized on rendering the other selected geometry (e.g., flowers, trees, etc.).</p>
<p id="p-0179" num="0178">Turning now to <figref idref="DRAWINGS">FIG. 21</figref>, <figref idref="DRAWINGS">FIG. 21</figref> is a flow diagram illustrating a process <b>2100</b> to implement geometric instancing, according to one embodiment of the invention. At block <b>2105</b> it is determined whether or not the hair/fur pipeline <b>1500</b> has entered a render stage. If not, the process is ended (block <b>2110</b>). However, if the render stage has been entered, at block <b>2115</b>, it is determined whether or not a user has decided to override the hair rendering operation to instead utilize the geometric instancing function. If not, the process is ended at block <b>2120</b>.</p>
<p id="p-0180" num="0179">On the other hand, if the user has decided to override the hair rendering process and to utilize the geometric instancing function instead, then at block <b>2125</b> the hair follicle information in obtained. Next, at block <b>2130</b>, based upon the user selected geometry, the hair follicles upon the surface are instead populated with the user-selected geometry <b>2130</b> (i.e., instead of hair/fur). Then, at block <b>2135</b>, the hair shape previously determined for the hairs are applied to deform the user-selected geometry.</p>
<p id="p-0181" num="0180">An example of this can be seen in <figref idref="DRAWINGS">FIGS. 22</figref><i>a </i>and <b>22</b><i>b</i>. As shown in <figref idref="DRAWINGS">FIG. 22</figref><i>a</i>, a plurality of hairs <b>2210</b> are shown as being rendered on a surface <b>2212</b>. Each of the hairs <b>2210</b> has hair follicle information (e.g., its relative position on the surface <b>2212</b>) and associated therewith an axial deformation framework (e.g., defining the shape of the hair) and related control information as to shape features such as curve, bend, rotation, twisting, etc. (as previously-described).</p>
<p id="p-0182" num="0181">However, with embodiments of the invention, as shown in <figref idref="DRAWINGS">FIG. 22</figref><i>b</i>, a user may decide to render another geometric object (e.g., a flower) instead of a hair. For example, a flower may be randomly selected from a set of three modeled flowers and may be deformed by the shape of the corresponding final hair.</p>
<p id="p-0183" num="0182">As shown in <figref idref="DRAWINGS">FIG. 22</figref><i>b</i>, a plurality of flowers <b>2220</b>, <b>2222</b>, and <b>2224</b>, are shown as replacing the hairs utilizing the same hair follicle information and the same shape information. It should be appreciated that flowers are only given as an example and that virtually any geometric shape may be used in lieu of a hair shape. Thus, new geometries may be instanced and deformed utilizing the hair/fur pipeline <b>1500</b> thereby providing a powerful and a unique way of rendering any type of geometric shape.</p>
<p id="p-0184" num="0183">Hair Motion Compositor System</p>
<p id="p-0185" num="0184">When dealing with hair animations and dynamic simulations in a production context, the need to combine different motion results arises quite often. If part of the hair is perfect in one simulation, but the rest of the hair looks better in another, it is easier to pick and choose which parts of each simulation are desirable to be kept than it is to figure out the right settings to get all the hair to move in a desired fashion.</p>
<p id="p-0186" num="0185">As will be described hereinafter, in one embodiment, a hair motion compositor provides a system and method that allows a user to combine and modify various control hair animations by building a network of nodes and operations. For example, in one embodiment, hair motion compositor module <b>1557</b> of hair/fur pipeline <b>1500</b> may be utilized to implement the hair motion compositor.</p>
<p id="p-0187" num="0186">For example, in one embodiment, hair pipeline <b>1500</b> utilizes surface definition module <b>1550</b> to define a surface and a control hair and hair motion compositor module <b>1557</b> combines different control hair curve shapes associated with the control hair and the surface. In particular, the hair motion compositor module <b>1557</b> generates a static node defining a static control hair curve shape; generates an animation node defining an animation control hair curve shape; and combines the static control hair curve shape of the static node with the animation control hair curve hair shape of the animation node to produce a resultant control hair curve shape for the control hair.</p>
<p id="p-0188" num="0187">Nodegraph Basics</p>
<p id="p-0189" num="0188">In one embodiment, the hair motion compositor (HMC) is a directed acyclic graph of nodes, wherein nodes can represent either animations or operations applied to animation. The connection between the nodes represents the data flow of animated control hair curve shapes.</p>
<p id="p-0190" num="0189">Simple Nodes</p>
<p id="p-0191" num="0190">Creating an HMC setup for a character or object typically assumes that there is an existing static comb of control hairs that is used as a basis. A HMC setup typically requires two nodes: a static node (containing the initial, non-animated shape of the control hair curves) and a control node (representing the final result&#x2014;which is the output of the node graph). Additional nodes representing control hair curve shapes may be inserted into the graph. These additional nodes may be termed animation nodes.</p>
<p id="p-0192" num="0191">Turning to <figref idref="DRAWINGS">FIG. 23</figref>, <figref idref="DRAWINGS">FIG. 23</figref> is a diagram illustrating a simple graph of a static node <b>2302</b> connected to an animation node <b>2304</b> and a finished control node <b>2306</b>. As can be seen in <figref idref="DRAWINGS">FIG. 23</figref>, the input to the animation node <b>2304</b> is the static combed shape from static node <b>2302</b>. The user can apply tweaks and key frames through an animation set of control hair curves provided by animation control node <b>2304</b> to offset the static shape to produce the final combed hair result of control node <b>2306</b>.</p>
<p id="p-0193" num="0192">Blend Nodes</p>
<p id="p-0194" num="0193">Blend nodes may be utilized for compositing features. A blend node may be defined as an operation node that takes two inputs and combines them together to form a single output. For example, control hair curves from each input may be blended together rotationally (to maintain curve length) or positionally (for linear blending).</p>
<p id="p-0195" num="0194">Additionally, a blend factor parameter for the blend node may be used to control how much each of the inputs should be used. For example, a value of 0 designates the total use of input node A and a value of 1 designates the total use of input node B; wherein the blend node provides smooth interpolation of the control hair curve shapes of input nodes A and B for all of the values in between.</p>
<p id="p-0196" num="0195">Turning now to <figref idref="DRAWINGS">FIG. 24</figref>, <figref idref="DRAWINGS">FIG. 24</figref> is a diagram illustrating a process of utilizing a blend node <b>2403</b>. Concurrent reference may also be made <figref idref="DRAWINGS">FIGS. 25</figref><i>a </i>and <b>25</b><i>b</i>, which illustrate the difference between rotational and positional blending, respectively. In particular, <figref idref="DRAWINGS">FIG. 24</figref> shows a blend node <b>2403</b> blending inputs from static node <b>2402</b> and animation node <b>2404</b> to obtain a resultant output shape of control node <b>2406</b>. In this example, blend node has a blend factor of 0.5.</p>
<p id="p-0197" num="0196">More particularly, as can be seen in <figref idref="DRAWINGS">FIG. 25</figref><i>a</i>, when rotational blending is selected by blend node <b>2403</b> with a blend factor of 0.5, resultant control hairs <b>2510</b> are obtained based upon input static control hairs <b>2520</b> and animation control hairs <b>2530</b>. The advantage of rotational blending is that it preserves the lengths of the resulting control hairs.</p>
<p id="p-0198" num="0197">As can be seen in <figref idref="DRAWINGS">FIG. 25</figref><i>b</i>, when positional blending is selected by blend node <b>2403</b> with a blend factor of 0.5, resultant control hairs <b>2555</b> are obtained based upon input static control hairs <b>2560</b> and animation control hairs <b>2570</b>. The advantage of positional blending is that the resulting control hair shapes are more predictable than with rotational blending.</p>
<p id="p-0199" num="0198">By default, blend node <b>2403</b> may apply the same blend value to all control vertices (CVs) of every input control hair curve. For per-CV control, a user may build a function curve specifying what blend factor value to use at every CV. For example, this may be used to cause the base CV motion to come from a first input, and the tip CV motion to come from a second input. Typically the same blend factor is applied to every control hair curve of the inputs, even when using per-CV function curve.</p>
<p id="p-0200" num="0199">To specify per-hair blend values, a user may utilize a blend ball that identifies regions in three-dimensional space. A blend ball may be made of two concentric spheres with an inner and outer blend factor value, respectively. If the inner value is 0 and the outer value is 1, all control hair curves within the inner sphere will get their animation from the first input, and all control hair curve outside the outer sphere will get their values from the second input, with a smooth interpolation in between.</p>
<p id="p-0201" num="0200">An example of this type of blend ball <b>2600</b> is illustrated in <figref idref="DRAWINGS">FIG. 26</figref>. In essence, blend ball <b>2600</b> is utilized to localize the effect of the blend node. As can be seen in <figref idref="DRAWINGS">FIG. 26</figref>, blend ball <b>2600</b> includes an inner sphere <b>2602</b> and an outer sphere <b>2604</b>. Further, <figref idref="DRAWINGS">FIG. 26</figref> shows the effect of blend ball <b>2600</b>, via inner sphere <b>2602</b> and outer sphere <b>2606</b>, upon blending animation control hairs <b>2607</b> with input static control hairs <b>2608</b>, and further shows the resultant output control hairs <b>2610</b>.</p>
<p id="p-0202" num="0201">Dynamic Solver</p>
<p id="p-0203" num="0202">It should be appreciated that other than specifying typical simulation settings (stiffness, damping, etc.), that a lot of initial effort goes into specifying a good goal shape that is to be fed as an input to a solver (such as key poses the user may want the control hair curves to &#x201c;hit&#x201d;). However, eventually most of the time is spent dealing with the results of one or more simulations.</p>
<p id="p-0204" num="0203">Since the dynamic solving process itself is often time consuming and outside of direct user control, it is important to minimize the number of simulations that need to be run to achieve the desired result.</p>
<p id="p-0205" num="0204">For example, in one embodiment, the MAYA hair dynamic solver may be used for hair simulations. However, it should be appreciated that the HMC system previously described is dynamic solver-agnostic. In essence, the solver appears as a single node in the node graph.</p>
<p id="p-0206" num="0205">With reference to <figref idref="DRAWINGS">FIG. 27</figref>, <figref idref="DRAWINGS">FIG. 27</figref> illustrates a dynamic setup including a dynamic solver node (represented by solver node <b>2706</b> and dynamic node <b>2708</b>). The initial static comb node <b>2702</b> is connected to the solver <b>2706</b> through animation node <b>2704</b> to be used as a goal, but the final result is blended (by blend node <b>2712</b>) between the output of the dynamic simulation node <b>2708</b> and the goal. Control node <b>2710</b> is the resultant output.</p>
<p id="p-0207" num="0206">By default, the blend is set to 0 to be 100% dynamic. However, to quickly make the hair stiffer (a common artistic request), the blend can be gradually increased to blend back to the static goal without running new simulations.</p>
<p id="p-0208" num="0207">Depending on the settings used and the complexity of the scene, dynamic simulations can take a long time, so that they often may be run on a RENDERFARM in order to free up a user's computer for other work. In order to accommodate this, a cache-out node <b>2720</b> connected to control node <b>2710</b> may be utilized to write to a storage device whenever a node is connected to it. The cache-out file can then subsequently be read back in with a cache-in node, as will be described. It should be appreciated that a cache-out node may be applied to any node of the system. Further, it should be appreciated that the storage device may be a hard disk drive or any other sort of storage device.</p>
<p id="p-0209" num="0208">Volume Nodes</p>
<p id="p-0210" num="0209">Very often, especially for computer graphic feature productions, proxy surfaces representing hair are modeled and key framed at certain key-poses by the animation department in order to represent how the hair should move in a given shot.</p>
<p id="p-0211" num="0210">For example, a ponytail of a character may be approximated by a bulged-out tube volume. In order to accommodate this functionality, a volume node may be utilized that in effect binds control hair curves so that they follow the animation of one or more volumes. Another use of volume nodes may be to offset the result of a dynamic simulation to produce a desired result. In this case, it may be impractical to use a hand-animated volume since it does not automatically follow the dynamic control hairs. Instead, a volume node may be used that provides its own volume which may be built-in on-the-fly as the convex hull of the control hair curves to be deformed. A user may then modify the volume to offset the animation. This is much easier than modifying the control vertices of individual hairs.</p>
<p id="p-0212" num="0211"><figref idref="DRAWINGS">FIG. 28</figref><i>a </i>is a diagram that demonstrates an automatically-generated cylindrical volume node <b>2804</b> that surrounds control hair curves <b>2802</b>. <figref idref="DRAWINGS">FIG. 28</figref><i>b </i>is a diagram showing control hair curves <b>2810</b> that have been offset after the user has edited the control hair curves with the volume node <b>2804</b>. As can be seen in <figref idref="DRAWINGS">FIGS. 28</figref><i>a </i>and <b>28</b><i>b</i>, the control hairs <b>2802</b> may be easily modified by the volume node <b>2804</b> as shown by modified control hair curves <b>2810</b>.</p>
<p id="p-0213" num="0212">Super Hair Nodes</p>
<p id="p-0214" num="0213">Super hair nodes provide a mechanism similar to the volume nodes by shaping or offsetting hair animation through a simplified proxy geometry. However, the proxy geometry in this case is a curve attached to the surface like all of the other hairs. A super hair node may be utilized to control a plurality of control hairs both at the combing stage to shape static hairs and in a shot to alter animated hairs.</p>
<p id="p-0215" num="0214">In one embodiment, a super hair node may have two modes of operation: absolute and relative. In absolute mode, a single controller curve is used by the super hair node to dictate the exact shape and animation of the control hairs. A user may choose to only partially apply the effect with a weight parameter whose value varies between 0 and 1. The super hair node also has the option to match the shape of the controller curve in world-space or to have the deformations applied in the local space of each control hair curve.</p>
<p id="p-0216" num="0215">Turning to <figref idref="DRAWINGS">FIG. 29</figref>, <figref idref="DRAWINGS">FIG. 29</figref> is a flow diagram illustrating a process <b>2900</b> of super hair node processing. At block <b>2910</b> a user selects a super hair node. At block <b>2915</b>, a user selects whether or not to utilize either an absolute or a relative mode. If absolute mode is selected, at block <b>2920</b>, a weight parameter may be selected and at block <b>2925</b> world or local space may be selected. At block <b>2930</b> the controller curve is applied to the control hairs.</p>
<p id="p-0217" num="0216">On the other hand, if during process <b>2900</b>, relative mode is selected, a user likewise selects a weight parameter (block <b>2940</b>) and selects world or local space (blocks <b>2945</b>). In the relative mode, however, both the controller curve and the base curve are applied (block <b>2950</b>) to the control hairs.</p>
<p id="p-0218" num="0217">In the relative mode, both a controller curve and a base curve are used, and only the difference between the two is applied to the control hair curves. This method is typically used when modifying control hairs that already have some incoming animation. It should be noted that the super hair node has no effect if the controller and base curve match exactly. When creating a super hair node, the controller and base curves are created straight up by default. For offsetting animated hairs, both curves can optionally take on the average shape of the control hairs. Tweaking the controller curve from there is then much more intuitive.</p>
<p id="p-0219" num="0218">Turning briefly to <figref idref="DRAWINGS">FIGS. 30</figref><i>a </i>and <b>30</b><i>b</i>, super hair operations in local-space and world space, respectively, are illustrated. As can be seen in <figref idref="DRAWINGS">FIGS. 30</figref><i>a </i>and <b>30</b><i>b</i>, the initial control curve shapes <b>3000</b> extend straight along the normal from the surface, and the resulting control hairs <b>3010</b> are shaped in order to match the controller curve <b>3020</b>.</p>
<p id="p-0220" num="0219">In both modes, absolute and relative, the effect of the super hair is applied uniformly to all the driven hairs. For localized control, a region of influence can be defined around the controller curve.</p>
<p id="p-0221" num="0220">Blend balls including inner and outer spheres (as previously-described) may be used to define where and by how much a super hair has an effect, as shown in <figref idref="DRAWINGS">FIG. 31</figref>. In particular, <figref idref="DRAWINGS">FIG. 31</figref> is a diagram illustrating a blend ball <b>3110</b> having both an inner sphere <b>3112</b>, and an outer sphere <b>3114</b> and the blend balls <b>3110</b> effect upon the controller curve <b>3120</b> and control hairs <b>3130</b>.</p>
<p id="p-0222" num="0221">Both an inner and outer weight are associated with each corresponding sphere <b>3112</b> and <b>3114</b> and the effect of the super hair <b>3120</b> and control hairs <b>3130</b> is interpolated in-between. As a practical application of the region of influence, the super hair <b>3120</b> can be used to fake collision such as a hand rubbing over a part of fur by having the controller curve slide on the surface to follow the hand motion and squash the hairs down as it moves.</p>
<p id="p-0223" num="0222">Exemplary Uses of Hair Motion Compositing</p>
<p id="p-0224" num="0223">Discussed below are some examples of the power and versatility of the hair motion compositor (HMC).</p>
<p id="p-0225" num="0224">Improving a Single Simulation Result</p>
<p id="p-0226" num="0225">Visual effects productions tend to be heavily art-directed for both live action and computer graphic features. One of the most frequent components from an initial hair shot review is to make a dynamic simulation match the artistically approved look more closely. This may entail either making the simulation stiffer, to remove extra or erratic motion, or making it match the key framed goal animation in a better fashion.</p>
<p id="p-0227" num="0226">Oftentimes, the simulation may produce physically-realistic results, but not necessarily the desired look. Rather than relying on a physically accurate solver by trying to fine-tune dynamic parameters and launching new simulations over and over again, it is often easier to &#x201c;composite-in&#x201d; some of the static or goal animation using the blend node functionality previously described with reference to <figref idref="DRAWINGS">FIGS. 24</figref>, <b>25</b>, <b>26</b>, <b>27</b>, and <b>31</b>. As has been discussed, the blend factor can be set or animated to preserve as much of the dynamic simulation that is found to be visually pleasing.</p>
<p id="p-0228" num="0227">Blending Between Multiple Simulation Results</p>
<p id="p-0229" num="0228">As previously described, simulation results rarely provide the perfect desired look &#x201c;out of the box&#x201d;. With varying sets of input parameters, one simulation might give a good starting motion, but another might provide a nicer end look, and a third might be more expressive at certain key moments. Finding a unified set of parameters that would provide a combination of all three is often impossible.</p>
<p id="p-0230" num="0229">However, with the features of the hair motion compositor (HMC), as previously described, a cascading graph may be easily set up to blend between various simulation caches as illustrated in <figref idref="DRAWINGS">FIG. 32</figref>.</p>
<p id="p-0231" num="0230">As can be seen in <figref idref="DRAWINGS">FIG. 32</figref>, three hair caches <b>3200</b>, <b>3202</b>, and <b>3204</b>, are each respectively processed through animation nodes <b>3210</b>, <b>3212</b>, and <b>3214</b>. Hair caches <b>3200</b> and <b>3202</b> are further combined through blend node <b>3220</b> and further through animation node <b>3222</b> and further blended with hair cache <b>3204</b> at blend node <b>3230</b>.</p>
<p id="p-0232" num="0231">This blended output is then processed though animation node <b>3240</b> and blended at blend node <b>3245</b> with a static node <b>3248</b> and a final process output is rendered at control node <b>3250</b>. It should be appreciated that the blend values can be key-framed to pick up various parts of each cache that are most pleasing.</p>
<p id="p-0233" num="0232">Blending can also be used for non-dynamic cache files. For characters with short fur, it may be practical to have two static combs: one &#x201c;normal&#x201d; and one &#x201c;collided&#x201d; with the hairs closely pushed down towards the surface. Then, instead of simulating all the interactions with the fur in a shot such as hands or props rubbing against it, the hair can simply be blended to the pre-collided comb in those areas, using blend balls that follow the collision objects.</p>
<p id="p-0234" num="0233">In addition to its blending power, the hair motion compositor (HMC) system may be utilized to correct problems in particular shots. For example, this often occurs in one of two situations that often cause bad results: animation issues which make static combed goal hairs penetrate the characters skin, and minor simulation errors such as missed collisions. In these cases, the general motion of the hair is perfectly acceptable, and all that is required to correct the problems may be accomplished by the use of volume offset nodes or super hair nodes, previously discussed. This is much faster than re-running a simulation until it is satisfactory.</p>
<p id="p-0235" num="0234">Optimization Techniques</p>
<p id="p-0236" num="0235">As previously discussed with reference to <figref idref="DRAWINGS">FIG. 15</figref>, embodiments of the invention further relate to optimization techniques that may be implemented by optimization module <b>1567</b> of hair/fur pipeline <b>1500</b>. In particular, three particular optimization techniques that improve the usability and rendering speed of hair/fur pipeline <b>1500</b> are disclosed herein.</p>
<p id="p-0237" num="0236">The first optimization technique relates to fine-grain control over the culling of final hair follicles dependent upon screen-space metrics. Referred to herein as view-dependent screen-space optimization techniques. A second optimization technique disclosed herein relates to the hair/fur pipeline's <b>1500</b> ability to selectively generate and render visible hair follicles based upon hair sub-patches. Referred to herein as hair sub-patch optimization techniques. Further, a third optimization technique implemented by the hair/fur pipeline <b>1500</b> relates to hair caching in order to decrease render times and improve turnaround time for lighting work. Referred to herein as hair caching techniques.</p>
<p id="p-0238" num="0237">View-Dependent Screen-Space Optimization</p>
<p id="p-0239" num="0238">As is well known, rendering a fully-furred creature at maximum hair density often takes a long time, utilizes a great deal of computing power, and utilizes a significant amount of memory. Although it is possible to manually adjust hair density in order to optimize hair density on a per-shot basis, this is a tedious and error-prone process.</p>
<p id="p-0240" num="0239">According to one embodiment of the invention, view-dependent screen-space optimization techniques implemented by the optimization module <b>1567</b> of the hair/fur pipeline <b>1500</b> may be implemented by utilizing continuous metrics measured in screen-space that give both fine-grain control over the setting of hair parameters&#x2014;such as hair density and hair width parameters at render time. Further, the behavior of optimization parameters may be customized by user-defined function curves.</p>
<p id="p-0241" num="0240">In one embodiment, surface definition module <b>1550</b> may be used to define a surface and optimization module <b>1567</b> may be used to determine whether a hair is to be rendered upon the surface. In particular optimization module <b>1567</b> may be used to: determine a size metric for the hair; apply a first density curve to the size metric determined for the hair to generate a density multiplier value; and based upon the density multiplier value, determine whether the hair should be rendered.</p>
<p id="p-0242" num="0241">The continuous metrics and customizability of optimization parameters enables a great deal of flexibility in this method and does not suffer from constraints associated with discrete selection of pre-determined values. In one embodiment of the present invention, the optimization parameters, as will be discussed hereinafter, relate to hair density and hair width parameters. However, it should be appreciated that other types of hair-related parameters may also be optimized.</p>
<p id="p-0243" num="0242">In this embodiment, hair screen-space optimization deals with determining the amount to reduce the number of rendered hairs based upon the size of each individual hair on the screen (screen-space size metric) and on the speed at which the hair travels onscreen (screen-space speed metric).</p>
<p id="p-0244" num="0243">Both metrics are calculated per hair and are then passed back to a user specified function curve that specifies whether or not a hair should be rendered. By using a custom function curve, a character's hair can be customized on a per shot basis and thus a great deal of flexibility is given to the artist to determine how much hair to cull from a character.</p>
<p id="p-0245" num="0244">With reference to <figref idref="DRAWINGS">FIG. 33</figref>, <figref idref="DRAWINGS">FIG. 33</figref> is a diagram illustrating techniques to implement view-dependent screen-space optimization. As can be seen in <figref idref="DRAWINGS">FIG. 33</figref>, view-dependent screen-space optimization <b>3300</b> is split into two stages. The first stage is a metric generation stage <b>3310</b> and the second stage is a metric-to-parameter mapping stage <b>3320</b>. In particular, each hair follicle <b>3308</b> is fed into the metric generation stage <b>3310</b>.</p>
<p id="p-0246" num="0245">In particular, each hair follicle <b>3308</b> is fed into the metric generation stage <b>3310</b>, and with additional reference to <figref idref="DRAWINGS">FIG. 34</figref>, each hair follicle root position <b>3402</b> for each hair <b>3410</b> is transformed into a normalized device coordinate (NDC) frame in the metric generation stage <b>3310</b> to generate a proxy hair <b>3405</b> that replaces the original hair.</p>
<p id="p-0247" num="0246">More particularly, in the metric generation stage <b>3310</b>, the NDC root position <b>3402</b> of each hair is used to measure different metrics that provide additional information to decide how to control per-hair parameters at render time.</p>
<p id="p-0248" num="0247">In this embodiment, two different metrics are utilized in the metric generation stage <b>3310</b>&#x2014;a size metric <b>3312</b> and a speed metric <b>3314</b>. However, it should be appreciated that a wide variety of other different metrics may also be utilized. The screen-space size metric <b>3312</b> is utilized to calculate the length of a proxy hair <b>3405</b> in NDC space. A proxy hair <b>3405</b> is a unit length hair that is grown straight up from the hair follicle root position <b>3402</b>. A proxy hair <b>3405</b> is utilized instead of an original final hair <b>3410</b> because it is easier to work with computationally than a final hair. This is because hair interpolation and effect operations associated with final hairs are very computationally intensive to perform especially if the hair is eventually culled. Another reason is that all hairs that are equal distance from the camera, but differing in length, should be treated identically. By doing so, looks with large hair-length variations are preserved, whereas ignoring proxy hairs in such a case may lead to bald spots or a different look once optimization is applied.</p>
<p id="p-0249" num="0248">Screen-space speed metric <b>3314</b> calculates the distance traveled by the root of a hair follicle from the current frame to the next frame in NDC space. Referring briefly to <figref idref="DRAWINGS">FIG. 35</figref>, <figref idref="DRAWINGS">FIG. 35</figref> is a diagram illustrating the distance traveled by the root <b>3502</b> of a proxy hair <b>3506</b> from a first frame <b>3510</b> at time t to a second frame <b>3520</b> at time t+1 in NDC space. This metric has been chosen because heavily motion-blurred objects often do not require full hair density to achieve the same look.</p>
<p id="p-0250" num="0249">With reference back to <figref idref="DRAWINGS">FIG. 33</figref>, after the size metric <b>3312</b> and the speed metric <b>3314</b> are determined at the metric generation stage <b>3310</b>, these values are passed to the metric-to-parameter mapping stage <b>3320</b>. For each parameter, an operator applies the corresponding function curves to each metric value. In this example, density curves <b>3324</b> and <b>3326</b> are applied to the size metric <b>3312</b> and speed metric <b>3314</b> (e.g., density vs. size and speed), respectively, and width curves <b>3330</b> and <b>3332</b> are applied to size metric <b>3312</b> and speed metric <b>3314</b> (e.g., width vs. size and speed), respectively. Each of the respective results are then multiplied to derive the final parameter multiplier value. In this example, a density multiplier value <b>3340</b> and a width multiplier value <b>3350</b> are determined that can be passed onto rendering functionality <b>3360</b>.</p>
<p id="p-0251" num="0250">It should be noted that in the case of the density multiplier, that the user-defined function curves <b>3324</b> and <b>3326</b> are constrained to map metric values in the range from [0,1]. The final result, the density multiplier value <b>3340</b>, which also correspondingly varies between [0,1], is then used to determine whether or not a final hair is rendered at render block <b>3360</b>. In one example, this may be done by generating a random number in the range [0,1]; if this number is lower than the final result, then the hair is drawn. Otherwise, it is culled.</p>
<p id="p-0252" num="0251">For the width multiplier, the user-defined function curves <b>3330</b> and <b>3332</b> are constrained to any non-negative real number and thus the final result is also a non-negative real number. This final result, width multiplier <b>3350</b>, is then multiplied with the current hair's width parameter and passed on to render block <b>3360</b>.</p>
<p id="p-0253" num="0252">It should be noted that because the hair count is no longer static, as an object (e.g., a character with hair) moves from the foreground to the background, that popping may occur. In order to alleviate this behavior, empirical testing has been done that shows that fading-out hair follicles in conjunction with these optimization techniques minimizes this effect. In view of this, each hair follicle may be forced to first pass through a visibility determination and if it is determined to be non-visible, the hair's opacity may be reduced by linearly reducing the hair's opacity value.</p>
<p id="p-0254" num="0253">It should be appreciated that by utilizing the above-defined techniques that the number of hairs generated or rendered may be reduced by the density multiplier value and the width of the remaining hairs may be increased by the width multiplier value. Therefore, by utilizing the speed and size metrics and the four function curves, these optimization techniques determine whether to cull a hair and by how much to scale its width.</p>
<p id="p-0255" num="0254">In particular, utilizing these techniques for a fully furred creature can result in significant processing and memory savings. This is because if a creature is not close up on the screen or is moving quickly across the screen, these techniques allow for the rendering of less than all of the hair that the creature had originally been designed with, without much of a visual difference, but render times and memory requirements are improved significantly.</p>
<p id="p-0256" num="0255">With reference to <figref idref="DRAWINGS">FIG. 36</figref>, <figref idref="DRAWINGS">FIG. 36</figref> shows a table <b>3600</b> that presents a side-by-side comparison of non-optimized values in terms of hair count, time, and memory versus optimized values in terms of hair count, time, and memory using the screen-space size metric at various frames. Render time (&#x201c;time&#x201d;) is listed in minutes and seconds and memory usage (&#x201c;mem&#x201d;) is given in Megabytes. In this test, illustrated in table <b>3600</b>, a furry character was moved from a close-up position at frame <b>10</b> to far away from a camera at frame <b>100</b>. The function curves used in this example were aggressive because the actual hair count goes from 78% of the original hair count at frame <b>10</b> to 1% at frame <b>100</b>. As can be seen in table <b>3600</b>, both render times and memory usage improve significantly.</p>
<p id="p-0257" num="0256">It should be noted that the view-dependent screen-space optimization techniques are flexible in the sense that the function curves can easily be adjusted to fine tune the optimization so that the character looks good at any distance and the memory and render time savings are as high as possible.</p>
<p id="p-0258" num="0257">With reference to <figref idref="DRAWINGS">FIG. 37</figref>, <figref idref="DRAWINGS">FIG. 37</figref> is a table <b>3700</b> (similar to table <b>3600</b>) illustrating another comparison utilizing the screen-space speed method. In this example, the character is shot moving across the screen very quickly and is rendered with motion blur. Frames were chosen where screen-space velocity is high. It should be noted that the hair count for the character is reduced to 19%. This technique shows dramatic savings as can be seen in table <b>3700</b> in both render time and memory for fast moving shots.</p>
<p id="p-0259" num="0258">Further, tests have been performed with both size and speed metrics being utilized in conjunction with one another. In particular, with reference to <figref idref="DRAWINGS">FIG. 38</figref>, <figref idref="DRAWINGS">FIG. 38</figref> is a table <b>3800</b> (similar to the other tables <b>3600</b> and <b>3700</b>) that illustrates non-optimized and optimized hair count, time, and memory values. In this example, a character was rendered with motion blur moving from far back at frame <b>101</b> to a much closer frame at frame <b>270</b>. As can be seen in the values of table <b>3800</b> both metrics work well with each other and save on rendering time and memory and further have been shown not to sacrifice visual quality.</p>
<p id="p-0260" num="0259">Hair Sub-Patch Optimization</p>
<p id="p-0261" num="0260">Another optimization technique implemented by the optimization module <b>1567</b> of hair/fur pipeline <b>1500</b> relates to hair sub-patch optimization. In general, this optimization is useful in rendering large amounts of hair based upon the fact that not all of the hair is visible to the camera at all times. Instead of generating all of the hairs and allowing display module <b>1575</b> to perform visibility culling during rendering, hair pipeline <b>1500</b> utilizing optimization module <b>1567</b> can save rendering time by not generating the non-visible primitives to begin with. In particular, aspects of these techniques may be implemented by the optimization module <b>1567</b> in conjunction with display module <b>1575</b>.</p>
<p id="p-0262" num="0261">In one embodiment, surface definition module <b>1550</b> may be used to define a surface and optimization module <b>1567</b> may be used to: create a bounding box for a set of hairs; determine whether the bounding box is visible; and if the bounding box is visible, hair associated with the visible bounding box is rendered upon the surface.</p>
<p id="p-0263" num="0262">In general a bounding box may be created for the initial set of hairs to be rendered and may then be tested for visibility before rendering. If the bounding box is visible, the hair primitives are spatially divided into four sub-groups and a new bounding box is created for each sub-group and each sub-group is again tested for visibility. If a bounding box is determined to be not visible, then no further subdivision is performed on that hair group, nor is it rendered. After recursively sub-dividing all the visible hair groups, a predetermined number of times, the hair primitives of each group are then sent to display module <b>1575</b> for rendering at that point.</p>
<p id="p-0264" num="0263">With reference to <figref idref="DRAWINGS">FIG. 39</figref>, <figref idref="DRAWINGS">FIG. 39</figref> is a flow diagram illustrating a process <b>3900</b> to implement hair sub-patch optimization. As can be seen in <figref idref="DRAWINGS">FIG. 39</figref>, at block <b>3910</b> a bounding box is created for an initial set of hairs. Next, at block <b>3915</b>, it is determined whether the bounding box for the initial set of hairs is visible. If not, process <b>3900</b> ends (block <b>3920</b>). For example, the sub-groups may be spatially divided in approximately equal relation. However, if the bounding box or a portion of the bounding box is visible for the initial set of hairs, then the bounding box is spatially divided into sub-groups (block <b>3925</b>). For example, the sub-groups may be spatially divided in approximately equal relation. Further, a bounding box is created for each sub-group (block <b>3930</b>).</p>
<p id="p-0265" num="0264">Reference can also be made to <figref idref="DRAWINGS">FIG. 40</figref> which is a diagram illustrating a simplified example of hair sub-patch optimization. As can be seen in <figref idref="DRAWINGS">FIG. 40</figref>, a first frame <b>4002</b> is shown with a bounding box <b>4010</b> for an initial set of hairs to the right side of the frame. As can be seen, the bounding box <b>4010</b> for the initial set of hairs is visible such that it is visible and, as shown in frame <b>4004</b>, bounding box <b>4010</b> is then spatially divided into sub-groups or sub-group bounding boxes <b>4012</b>.</p>
<p id="p-0266" num="0265">Returning to process <b>3900</b> of <figref idref="DRAWINGS">FIG. 39</figref>, at block <b>3935</b> it is determined whether these sub-group bounding boxes are visible. If not, at block <b>3940</b> the process ends. However, if these sub-group bounding boxes are visible then it is determined whether a predetermined number of subdivisions have been met (block <b>3945</b>). If not, the sub-group is further divided and is recursively subdivided a predetermined number of times. Assuming that the predetermined number has been met, then at block <b>3950</b>, the hair primitives of the sub-group bounding boxes are rendered.</p>
<p id="p-0267" num="0266">Particularly, with reference to <figref idref="DRAWINGS">FIG. 40</figref>, as can be seen at frame <b>3</b> <b>4006</b>, the two leftmost sub-group bounding boxes <b>4014</b> are determined to be visible (and are identified with hatch-marks) whereas the two rightmost boxes are not visible. Accordingly, the hair associated with the two leftmost visible sub-group bounding boxes <b>4014</b> are sent on for rendering whereas the hair associated with the two rightmost non-visible sub-group bounding boxes are discarded. It should be appreciated that this can be recursively implemented a pre-determined number of times.</p>
<p id="p-0268" num="0267">Further, because of the flexibility of the hair/fur pipeline <b>1500</b> as previously discussed with reference to the geometric instancing embodiment, environmental modelers may utilize aspects of the hair sub-patch optimization to extend it to other computer graphic uses such as to render large grassy landscapes. By utilizing embodiments of the hair sub-patch optimization embodiment, from shot to shot, only a small fraction of the entire landscape may need to be rendered resulting in substantial processing and memory savings. However, it should be appreciated that by utilizing the previously-described geometric instancing embodiments that any user-selected or randomly-generated geometric object may be rendered instead of hair with or without associated hair parameters (bend, rotation, wind, etc.) and the axial deformation parameters associated with the hair.</p>
<p id="p-0269" num="0268">For example, to define a landscape, the process may be initiated with a single bounding box that encompasses a patch defining the entire landscape. This patch then may be subdivided into four disjoint quadrants with smaller bounding boxes, as previously described. For bounding boxes that get culled, no further processing is done, but for bounding boxes that continue to be processed, further subdivision will occur until a user-defined stopping criterion is reached.</p>
<p id="p-0270" num="0269">Additional stopping criterion may be utilized as well, such as subdivision depth and parametric limits relating to the parametric length of the sub-patches dimensions and parametric texture space.</p>
<p id="p-0271" num="0270">Utilizing the previously described hair sub-patch optimization techniques discussed with reference to <figref idref="DRAWINGS">FIGS. 39 and 40</figref> as applied to a landscape, a single large grassy landscape was modeled with 25 million hairs in which the hairs were geometrically instanced with grass and/or trees. But as shown in the frame of <figref idref="DRAWINGS">FIG. 41</figref>, only a few hundred blades of grass were actually generated and rendered utilizing the hair sub-patch optimization techniques. Without this optimization, to render all the grass would have resulted in a far greater amount of processing and memory usage being required.</p>
<p id="p-0272" num="0271">Hair Caching Optimization</p>
<p id="p-0273" num="0272">During the lighting phase, a lighting expert typically works on a shot that has been approved by both the animation and layout departments. What is typically left to do is to determine the number and placement of lights and to create the type of lighting conditions needed to achieve the desired look.</p>
<p id="p-0274" num="0273">In particular, during the lighting phase, all the elements of the scene have been finalized including all of the hair parameters. Because of this, it is possible to cache out and re-use hair geometry and hair parameters in order to obtain significant processing and memory savings. In one embodiment, optimization module <b>1567</b> in conjunction with shading and backlighting module <b>1570</b> may be utilized to implement techniques according to the hair caching embodiment as described herein.</p>
<p id="p-0275" num="0274">In order to accomplish this, a cache hair state file may be created and/or validated and the state of the hair parameters, as they are to be rendered, can be saved. For example, these hair parameters may be saved as an unordered list of hair parameters within the cache hair state file.</p>
<p id="p-0276" num="0275">In one embodiment, lighting module <b>1570</b> may be used to produce lighting effects in a lighting phase for a shot and optimization module <b>1567</b> may be used to: determine if a cache hair state file including hair parameters exists; and determine if the cache hair state file includes matching hair parameters to be used in the shot, and if so, the hair parameter values from the cache hair state file are used in the lighting phase.</p>
<p id="p-0277" num="0276">Turning to <figref idref="DRAWINGS">FIG. 42</figref>, an example of a cache hair state file <b>4200</b> that includes a list of hair state parameters <b>4202</b>, <b>4204</b>, <b>4206</b>, etc., is illustrated. If the cache hair state file <b>4200</b> does not exist or contains different parameter values than the state of the current render, then the new set of values may be saved into the cache hair state file <b>4200</b> (i.e., as hair state parameters <b>4202</b>, <b>4204</b>, <b>4206</b>, etc.). This new or updated cache hair state file <b>4200</b> can then be utilized. However, if the cache hair state file <b>4200</b> already matches the state of current render, then the original cache hair state file <b>4200</b> may be used.</p>
<p id="p-0278" num="0277">Turning to <figref idref="DRAWINGS">FIG. 43</figref>, <figref idref="DRAWINGS">FIG. 43</figref> is a flow diagram illustrating a process <b>4300</b> to implement hair caching. At block <b>4302</b> it is determined whether a cache hair state file <b>4200</b> exists. If not, a new cache hair state file <b>4200</b> is created (block <b>4304</b>). Then, at block <b>4306</b> the new cache hair state file <b>4200</b> is saved.</p>
<p id="p-0279" num="0278">However, if a cache state file does exist, then at block <b>4310</b> it is determined whether or not the hair cache state file <b>4200</b> includes the same parameters to be utilized for rendering. If so, the process moves to block <b>4320</b> as will be described. Otherwise, at block <b>4306</b>, the different hair state parameters are saved to the cache hair state file <b>4200</b>.</p>
<p id="p-0280" num="0279">With either a new cache hair state file, a cache hair state file with the same parameters, or a cache hair state file with different parameters, the process moves on to block <b>4320</b>, where it is determined whether or not a proper file key is present. Each render and cache validation process is locked with an exclusive file lock prior to cache validation, cache generation and cache rendering. Accordingly, a proper key needs to be present for the cache hair state file to be used in the lighting phase. This is because multiple processes are often used to generate a final rendered frame while a lighter is working on a shot.</p>
<p id="p-0281" num="0280">If the proper file key is not present, then the process ends at block <b>4340</b> and the cache hair state file <b>4200</b> is deleted. However, if the cache hair state file <b>4200</b> was properly unlocked, then the cache file may be utilized in the lighting phase at block <b>4350</b>.</p>
<p id="p-0282" num="0281">In one embodiment, the physical cache hair state file may be represented as a RENDERMAN file. This may be done without a loss of generality since it simply encodes data points and parameters needed to generate hair geometry. In addition, compressed RENDERMAN files may be used as a space saving technique to represent cache files.</p>
<p id="p-0283" num="0282">Thus, the previously described process <b>4300</b> determines whether or not the cache hair state file <b>4200</b> is valid or invalid. The cache hair state file is generally determined to be invalid if it does not exist or the primitives found inside the cache file do not match the hair primitives that are to be rendered. In particular, to determine the latter condition, each cache hair state file may be required to contain the hair render settings that were used to generate it in the first place.</p>
<p id="p-0284" num="0283">Thus, a cache hair state file is deemed valid if the render settings match. On the other hand, if the cache hair state file is found to be invalid, then the correct hair parameters are fed and stored to cache hair state file, as previously described.</p>
<p id="p-0285" num="0284">Typically, in the first pass, the cache is invalid and thus needs to be generated (cache generation and use). Savings can be found here due to serialization that allows for immediate the use a cache once another processor has generated it. This alone cuts down rendering times by approximately 69%. Further savings may be realized when different processes are utilizing the same cache file. It should be noted that lighters typically keep re-rendering many times and the actual time savings are often multiplied.</p>
<p id="p-0286" num="0285">Turning now to <figref idref="DRAWINGS">FIG. 44</figref>, <figref idref="DRAWINGS">FIG. 44</figref> shows a table in which the time-savings a lighter can accomplish by using the previously-described techniques to render a fully-furred character is illustrated. As can be seen in <figref idref="DRAWINGS">FIG. 44</figref>, time to render without cache, time to generate and use cache for rendering, and time to render with the existing cache are shown. As can be seen significant time-savings are realized.</p>
<p id="p-0287" num="0286">In this description, numerous specific details are set forth. However, it is understood that embodiments of the invention may be practiced without these specific details. In other instances, well-known circuits, structures, software processes, and techniques have not been shown in order not to obscure the understanding of this description.</p>
<p id="p-0288" num="0287">Components of the various embodiments of the invention may be implemented in hardware, software, firmware, microcode, or any combination thereof. When implemented in software, firmware, or microcode, the elements of the embodiment of the present invention are the program code or code segments to perform the necessary tasks. A code segment may represent a procedure, a function, a subprogram, a program, a routine, a subroutine, a module, a software package, a class, or any combination of instructions, data structures, or program statements. A code segment may be coupled to another code segment or a hardware circuit by passing and/or receiving information, data, arguments, parameters, or memory contents. Information, arguments, parameters, data, etc. may be passed, forwarded, or transmitted via any suitable means including memory sharing, message passing, token passing, network transmission, etc.</p>
<p id="p-0289" num="0288">The program or code segments may be stored in a processor readable medium or transmitted by a computer data signal embodied in a carrier wave, or a signal modulated by a carrier, over a transmission medium. The &#x201c;processor readable or accessible medium&#x201d; or &#x201c;machine readable or accessible medium&#x201d; may include any medium that can store, transmit, or transfer information. Examples of the machine accessible medium include an electronic circuit, a semiconductor memory device, a read only memory (ROM), a flash memory, an erasable ROM (EROM), a floppy diskette, a compact disk (CD-ROM), an optical disk, a hard disk, a fiber optic medium, a radio frequency (RF) link, etc. The computer data signal may include any signal that can propagate over a transmission medium such as electronic network channels, optical fibers, air, electromagnetic, RF links, etc. The code segments may be downloaded via computer networks such as the Internet, Intranet, etc. The machine accessible medium may be embodied in an article of manufacture. The machine accessible medium may include data that, when accessed by a machine, cause the machine to perform the operation described in the following. The term &#x201c;data&#x201d; here refers to any type of information that is encoded for machine-readable purposes. Therefore, it may include program, code, data, file, etc.</p>
<p id="p-0290" num="0289">More particularly, all or part of the embodiments of the invention may be implemented by software. The software may have several modules coupled to one another. A software module is coupled to another module to receive variables, parameters, arguments, pointers, etc. and/or to generate or pass results, updated variables, pointers, etc. A software module may also be a software driver or interface to interact with the operating system running on the platform. A software module may also be a hardware driver to configure, set up, initialize, send and receive data to and from a hardware device.</p>
<p id="p-0291" num="0290">While the invention has been described in terms of several embodiments, those of ordinary skill in the art will recognize that the invention is not limited to the embodiments described, but can be practiced with modification and alteration within the spirit and scope of the appended claims. The description is thus to be regarded as illustrative instead of limiting.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer for generating hair comprising:
<claim-text>a processor;</claim-text>
<claim-text>a memory including a hair pipeline comprising:</claim-text>
<claim-text>a surface definition module to define a surface;</claim-text>
<claim-text>an optimization module to determine whether a hair is to be rendered upon the surface, the optimization module to:
<claim-text>determine a size metric for the hair, wherein determining the size metric for the hair includes calculating a length of the hair;</claim-text>
<claim-text>apply a first user-defined functional density curve to the size metric determined for the hair to generate a density multiplier value;</claim-text>
<claim-text>apply a first user-defined functional width curve to the size metric for the hair to adjust the width of the hair;</claim-text>
<claim-text>based upon the density multiplier value, determine whether to render the hair; and</claim-text>
</claim-text>
<claim-text>a display device module to display the rendered hair on a display device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The hair pipeline of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the density multiplier value is further compared to a random number to determine whether to render the hair.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The hair pipeline of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the size metric for the hair including calculating the length of the hair is calculated in a normalized device coordinate (NDC) space.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The hair pipeline of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optimization module further determines a speed space metric based upon a distance traveled by a hair root position of the hair from a first frame to a second frame.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The hair pipeline of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein a second density curve is applied to the speed space metric for the hair to generate the density multiplier value.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The hair pipeline of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein a first width curve is applied to the size metric for the hair and a second width curve is applied to the speed space metric for the hair to generate a width multiplier value to adjust the width of the hair.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method implemented by a computer system having a memory and processor to determine whether a hair is to be rendered comprising:
<claim-text>determining a size metric for a hair, wherein determining the size metric for the hair includes calculating a length of the hair;</claim-text>
<claim-text>applying a first user-defined functional density curve to the size metric determined for the hair to generate a density multiplier value;</claim-text>
<claim-text>applying a first user-defined functional width curve to the size metric for the hair to adjust the width of the hair;</claim-text>
<claim-text>determining whether to render the hair based upon the density multiplier value; and</claim-text>
<claim-text>displaying the rendered hair on a display device, wherein the steps of determining, applying, determining, and displaying are performed by the computer system.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising comparing the density multiplier value to a random number to determine whether to render the hair.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein determining the size metric for the hair including calculating the length of the hair is calculated in a normalized device coordinate (NDC) space.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising determining a speed space metric based upon a distance traveled by a hair root position of the hair from a first frame to a second frame.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising applying a second density curve to the speed space metric for the hair to generate the density multiplier value.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising applying a first width curve to the size metric for the hair and applying a second width curve to the speed space metric for the hair to generate a width multiplier value that is used to adjust the width of the hair.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A non-transitory computer-readable storage medium containing executable instructions tangibly stored thereon which, when executed in a computer processing system, cause the computer processing system to perform a method for determining whether a hair is to be rendered comprising:
<claim-text>determining a size metric for a hair, wherein determining the size metric for the hair includes calculating a length of the hair;</claim-text>
<claim-text>applying a first user-defined functional density curve to the size metric determined for the hair to generate a density multiplier value;</claim-text>
<claim-text>applying a first user-defined functional width curve to the size metric for the hair to adjust the width of the hair;</claim-text>
<claim-text>determining whether to render the hair based upon the density multiplier value; and</claim-text>
<claim-text>displaying the rendered hair on a display device.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer-readable storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising comparing the density multiplier value to a random number to determine whether to render the hair.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer-readable storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein determining the size metric for the hair including calculating the length of the hair is calculated in a normalized device coordinate (NDC) space.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer-readable storage medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising determining a speed space metric based upon a distance traveled by a hair root position of the hair from a first frame to a second frame.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising applying a second density curve to the speed space metric for the hair to generate the density multiplier value.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising applying a first width curve to the size metric for the hair and applying a second width curve to the speed space metric for the hair to generate a width multiplier value that is used to adjust the width of the hair.</claim-text>
</claim>
</claims>
</us-patent-grant>